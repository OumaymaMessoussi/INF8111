{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF8111 - Data Mining\n",
    "\n",
    "## TP1 SUMMER 2020 - recommendation system\n",
    "\n",
    "##### Team Members:\n",
    "\n",
    "    - Kacem Khaled\n",
    "    - Oumayma Messoussi\n",
    "    - Semah Aissaoui\n",
    "\n",
    "\n",
    "## 1 - Overview\n",
    "\n",
    "Stack Exchange is a network of question-and-answer (Q&A) websites on topics in diverse fields, each site covering a specific topic. On Stack Exchange website, a thread is composed of a question and their answers and comments. In this assignment, *we will implement a recommendation system that returns threads (question + answers) that are related to a specific question*. Before submitting questions, the  website will use this engine to show the most similar threads to users in order to reduce the number of duplicate questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the code below to install the packages needed for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\oumay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\oumay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\oumay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\oumay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want, you can use anaconda and install after nltk library\n",
    "# pip install --user numpy\n",
    "# pip install --user sklearn\n",
    "# pip install --user scipy\n",
    "# pip install --user nltk\n",
    "\n",
    "\n",
    "#python\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import scipy as sp\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Data\n",
    "\n",
    "Please download the zip file in the following url: https://drive.google.com/file/d/1032N1oZkytHlHs20AXE9jPMBQCTyhb6H/view?usp=sharing\n",
    "\n",
    "In this zip file, there are:\n",
    "\n",
    "1. test.json: This file contains queries (new questions) and the relevant threads(question + answer) for each one these queries.\n",
    "2. threads: It is a folder that contains the thread html sources of threads. Each html file name follows the pattern **thread_id.html**.\n",
    "\n",
    "\n",
    "Figure below depicts an thread page example:\n",
    "\n",
    "![thread_img](thread_example.png)\n",
    "\n",
    "The figure contains 4 hilighted areas. Area A, B, C, D and E are the question subject, question body, question comments, answer body, and anwer comments, respectively. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# define the folder path that contain the data\n",
    "# FOLDER_PATH = \"Define folder path that contain threads folder and test.json\"\n",
    "FOLDER_PATH = \"../../../datasets/TP1/dataset/\"\n",
    "THREAD_FOLDER = os.path.join(FOLDER_PATH, 'threads')\n",
    "\n",
    "\n",
    "# Load the evaluation dataset\n",
    "import json\n",
    "\n",
    "\n",
    "test = json.load(open(os.path.join(FOLDER_PATH, \"test.json\")))\n",
    "relevant_threads_by_query = dict()\n",
    "\n",
    "\n",
    "for (query_id, cand_id, label) in test: \n",
    "    if label == 'Irrelevant':\n",
    "        continue\n",
    "        \n",
    "    l = relevant_threads_by_query.setdefault(query_id, [])\n",
    "    l.append(cand_id)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Web scraping\n",
    "\n",
    "Web scraping consists in extracting relevant data from pages and prepare it for computational analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Question 1 (0.5 point)\n",
    "\n",
    "Special and non-ASCII characters can be encoded into their html representantion (html entities). For instance, apostrophe (') is encoded as **\\&apos;**. The webpage encoding in the data folder are incosistent. Only in a portion of the webpages, the **special** and **non-ASCII** characters were encoded into  html entities. We will fix this inconsistency by transforming the html entities into character representations, e.g., **\\&apos;** is represented as **'**.\n",
    "\n",
    "*Implement the function fix_encoding that encodes the html entities (special and non-ASCII characters) into their UTF-8 encoding.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.entities import html5\n",
    "from html import unescape\n",
    "\n",
    "def fix_encoding(text):\n",
    "    \"\"\"\n",
    "    Encodes the html entities in a text into UTF-8 encoding. For instance, \"I&amp;m ...\" => \"I'm ...\"\n",
    "    \n",
    "    :param text: string.\n",
    "    :return: fixed text(sting)\n",
    "    \"\"\"\n",
    "    \n",
    "    assert text != ''\n",
    "    \n",
    "#     start_index = text.find('&')\n",
    "#     while (start_index != -1):\n",
    "#         if text[start_index+1:start_index+4] == 'amp':\n",
    "#             start_index = text.find('&', start_index+1)\n",
    "#             continue\n",
    "#         end_index = text.find(';', start_index)\n",
    "#         text = text.replace(text[start_index:end_index+1], html5[text[start_index+1:end_index+1]])\n",
    "#         start_index = text.find('&', start_index+1)\n",
    "        \n",
    "#     text = text.replace('&amp;', html5['amp;'])\n",
    "    \n",
    "    return unescape(text)\n",
    "        \n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello I'm & oumayma ∾ messoussi &∾∾'&\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "txt = fix_encoding(\"hello I&apos;m &amp; oumayma &ac; messoussi &amp;&ac;&ac;&apos;&amp;\")\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Question 2 (3 points)\n",
    "\n",
    "Implement extract_data_from_page function. This function extracts question subject, question body, question comments, answer body, and anwer comments from the thread webpage. It returns a dictionary with the following structure: *{\"thread_id\": int,\"question\":{\"subject\": string, \"body\": string, \"comments\": [string]}, answers: [{\"body\": string, \"comments\": [string]}]}*\n",
    "\n",
    "**Use the fix_encoding function to fix the text encoding. You can use the library Beatiful Soap in this question. All html tags have to be removed from comment, question and answer textual data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data_from_page(pagepath):\n",
    "    \"\"\"\n",
    "    Scrap question, answer and comments from thread page.\n",
    "    \n",
    "    :param pagepath: the path of thread html file.\n",
    "    :return: \n",
    "        {\n",
    "            \"thread_id\": thread id,\n",
    "            \"question\":{\n",
    "                \"subject\": question subject text (Area A in the figure), \n",
    "                \"body\": question body text (Area B in the figure), \n",
    "                \"comments\": list of comment texts (Area C in the figure)\n",
    "                }, \n",
    "            \"answers\": [\n",
    "                {\n",
    "                    \"body\": answer body text (Area D in the figure),\n",
    "                    \"comments\": list of answer texts (Area E in the figure)\n",
    "                }\n",
    "                ]\n",
    "            }\n",
    "    \"\"\"\n",
    "    \n",
    "    data = {}\n",
    "    data['thread_id'] = pagepath.split('/')[-1][:-5]\n",
    "    data['question'] = {}\n",
    "    data['question']['comments'] = []\n",
    "    data['answers'] = []\n",
    "    \n",
    "    soup = BeautifulSoup(open(pagepath, encoding='utf8'))\n",
    "    \n",
    "    for a in soup.find_all('a'):\n",
    "        if a.get('class') and 'question-hyperlink' in a.get('class'):\n",
    "            data['question']['subject'] = a.contents\n",
    "            \n",
    "    QnA = soup.find(id='mainbar')\n",
    "    \n",
    "    question = QnA.find(id='question')     \n",
    "    raw_Q = question.find('div', class_='post-text')\n",
    "    Q_body = ''\n",
    "    if len(raw_Q.contents) == 1:\n",
    "        Q_body = raw_Q.contents\n",
    "    else:\n",
    "        for elem in raw_Q.contents:\n",
    "            if elem != '\\n':\n",
    "                Q_body += elem if type(elem) == str else elem.get_text()\n",
    "    data['question']['body'] = fix_encoding(Q_body)\n",
    "    \n",
    "    raw_Q_comm = question.find_all('div', class_='comments')\n",
    "    for Q_comm in raw_Q_comm:\n",
    "        comms = Q_comm.find_all('li')\n",
    "        for comm in comms:\n",
    "            span = comm.find('span', class_='comment-copy')\n",
    "            data['question']['comments'] += span.contents\n",
    "                        \n",
    "    answers = QnA.find(id='answers')\n",
    "    answers = answers.find_all('div', class_='answer')\n",
    "    for answer in answers:\n",
    "        tmp = {}\n",
    "        tmp['comments'] = []\n",
    "        ans = answer.find('div', class_='post-text')\n",
    "\n",
    "        A_body = ''\n",
    "        if len(ans.contents) == 1:\n",
    "            A_body = ans.contents\n",
    "        else:\n",
    "            for elem in ans.contents:\n",
    "                if elem != '\\n':\n",
    "                    A_body += elem if type(elem) == str else elem.get_text()\n",
    "        tmp['body'] = fix_encoding(A_body)\n",
    "        \n",
    "        raw_A_comms = answer.find_all('div', class_='comments')\n",
    "        for A_comm in raw_A_comms:\n",
    "            comms = A_comm.find_all('li')\n",
    "            for comm in comms:\n",
    "                span = comm.find('span', class_='comment-copy')\n",
    "                tmp['comments'] += span.contents\n",
    "\n",
    "        data['answers'].append(tmp)\n",
    "    \n",
    "    print('done')\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "{'thread_id': '100021749708', 'question': {'comments': ['There are official Microsoft controllers with permanently attached cables.  The hologram sounds suspicious though.', 'I own a counterfeit mouse-pad with the same Microsoft logo you described.', \"Buy from Amazon. You'll usually get a good price and you'll definitely get peace of mind. I'll pay $5 more for that any day.\", 'Good picture examples can be found on this dupe: http://gaming.stackexchange.com/questions/89194/is-this-xbox-360-controller-fake-how-can-i-tell'], 'subject': ['How can I identify a counterfeit Xbox 360 controller'], 'body': 'I recently bought a Wired Xbox 360 controller from Ebay, and I\\'m trying to figure out whether it\\'s counterfeit. It\\'s two-tone black and gray, which according to Wikipedia means it\\'s probably the Elite edition of the controller. Here\\'s what makes me suspicous: There\\'s no breakaway cable near the USB port (Microsoft\\'s term is Inline Cable Release), the cable is one piece from controller to USB port The cord is about 7 feet long The hologram sticker on the back says \"Microsoft\" but doesn\\'t have the real Microsoft logo, there\\'s no slice taken out of the \"o\" in \"Micro\" Here\\'s a number of details that are correct, as far as I can tell: The \"Microsoft\" logo etched on the top of the controller is correct, it has the split \"o\" and connected \"ft\" It seems to work correctly when plugged into my computer The front of the controller matches this image exactly At the USB end of the cord, the grey wider plastic piece you\\'re intended to pull on has the little nub on top. (You can see what I\\'m talking about in this picture, the nub is to the left of and below the first \"w\" in the watermark) I know pictures would probably be helpful, I\\'ll work on taking some. '}, 'answers': [{'comments': ['Does the hologram on yours have the split \"o\"', \"@daxelrod: No, it doesn't.  Each letter is close together, but slightly separated on the hologram, and the hologram also has XBOX360 (and a trademark symbol) in the background several times.\"], 'body': 'Microsoft has changed the looks of its controllers over time. For instance, my original wireless controller from my Elite looks identical to the images you showed, and says Microsoft twice on the top of the controller (one hologram, one etched in). My newer wireless controller, that came with the Wireless adapter for Windows, is all black, and only has the hologram on the top with Xbox 360 etched in the top. On both controllers, the hologram is right above the battery pack, so it may be located in a different location on a wired controller. '}]}\n"
     ]
    }
   ],
   "source": [
    "data = extract_data_from_page(FOLDER_PATH+'threads/100021749708.html')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "{'thread_id': '100216394556', 'question': {'comments': [], 'subject': ['Can you rebind the movement keys'], 'body': 'When I played through the demo of Kingdoms of Amalur: Reckoning, I could never figure out how to rebind the movement keys.  I prefer to use ESDF instead of the standard WASD, and while I was willing to adapt to WASD for the duration of the demo, I do not plan on purchasing the game unless I can use ESDF. '}, 'answers': [{'comments': ['Thanks for publishing the magic order in which keys must be bound. It was highly aggravating to see them stick together. And a hearty poke in the eyeball of whichever intern cobbled together the broken keybindings menu.'], 'body': \"Yes. You can. From the menu, simply select Options > Controls > Controls and you're met with this screen: By default: F is Interact E is Quick Mana Potion You'll have to remap those keys first, followed by remapping the movement keys in the following order: Right Down Left Up For some reason, mapping in another order bugs everything to F, rendering you immobile. If you follow this pattern, however, it works just fine! \"}]}\n"
     ]
    }
   ],
   "source": [
    "data = extract_data_from_page(FOLDER_PATH+'threads/100216394556.html')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Extract text from HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<map object at 0x000002B1B68492B0>\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-305c465a8ccb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mthreads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_data_from_page\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthreads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mthread_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'thread_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthreads\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Save preprocessed threads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-305c465a8ccb>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mthreads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_data_from_page\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthreads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mthread_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'thread_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthreads\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Save preprocessed threads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-18d9f515f0cd>\u001b[0m in \u001b[0;36mextract_data_from_page\u001b[1;34m(pagepath)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'question'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comments'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mspan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0manswers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQnA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'answers'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0manswers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manswers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'answer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0manswer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manswers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TF_Keras\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(self, name, attrs, recursive, text, **kwargs)\u001b[0m\n\u001b[0;32m   1721\u001b[0m         \"\"\"\n\u001b[0;32m   1722\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1723\u001b[1;33m         \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1724\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1725\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TF_Keras\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36mfind_all\u001b[1;34m(self, name, attrs, recursive, text, limit, **kwargs)\u001b[0m\n\u001b[0;32m   1748\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m             \u001b[0mgenerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1750\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1751\u001b[0m     \u001b[0mfindAll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_all\u001b[0m       \u001b[1;31m# BS3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m     \u001b[0mfindChildren\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_all\u001b[0m  \u001b[1;31m# BS2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TF_Keras\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m_find_all\u001b[1;34m(self, name, attrs, text, limit, generator, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 745\u001b[1;33m                 \u001b[0mfound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    746\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m                     \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TF_Keras\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m   2017\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2018\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2019\u001b[1;33m                 \u001b[0mfound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2020\u001b[0m         \u001b[1;31m# If it's text, make sure the text matches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2021\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNavigableString\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TF_Keras\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36msearch_tag\u001b[1;34m(self, markup_name, markup_attrs)\u001b[0m\n\u001b[0;32m   1959\u001b[0m             \u001b[0mmarkup_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m         call_function_with_tag_data = (\n\u001b[1;32m-> 1961\u001b[1;33m             \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1962\u001b[0m             and not isinstance(markup_name, Tag))\n\u001b[0;32m   1963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TF_Keras\\lib\\abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[1;34m(cls, instance)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;31m# Inline the cache checking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0msubclass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msubclass\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_abc_cache\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from multiprocessing import Pool, TimeoutError\n",
    "from time import time\n",
    "import json\n",
    "\n",
    "# Index each thread by its id\n",
    "index_path = os.path.join(THREAD_FOLDER, 'threads.json')\n",
    "if os.path.isfile(index_path):\n",
    "    \n",
    "    # Load threads that webpage content were already extracted.\n",
    "    thread_index = json.load(open(index_path))\n",
    "else:\n",
    "    \n",
    "    # Extract webpage content\n",
    "    # This can be slow (around 30 minutes). Test your code with a small sample. lxml parse is faster than html.parser\n",
    "    files = (os.path.join(THREAD_FOLDER, filename) for filename in os.listdir(THREAD_FOLDER))\n",
    "    threads = map(extract_data_from_page, files)\n",
    "    thread_index = dict(((thread['thread_id'], thread) for thread in threads ))\n",
    "    \n",
    "    # Save preprocessed threads\n",
    "    json.dump(thread_index, open(index_path,'w'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Data Preprocessing\n",
    "\n",
    "Preprocessing is a crucial task in data mining. This task clean and transform the raw data in a format that can better suit data analysis and machine learning techniques. In natural language processing (NLP), *tokenization* and *stemming* are two well known preprocessing steps. Besides these two steps, we will implement an additional step that is designed exclusively for the twitter domain.\n",
    "\n",
    "### 5.1 - Tokenization\n",
    "\n",
    "In this preprocessing step, a *tokenizer* is responsible for breaking a text in a sequence of tokens (words, symbols, and punctuations). \n",
    "\n",
    "For instance, the sentence *It's the student's notebook.* can be split into the following list of tokens: ['It', \"'s\", 'the', 'student', \"'s\", 'notebook', '.'].\n",
    "\n",
    "\n",
    "#### 5.1.1 - Question 3 (0.5 point) \n",
    "\n",
    "Implement the following functions: \n",
    "- **tokenize_space** tokenizes the tokens that are separated by whitespace (space, tab, newline). This is a naive tokenizer.\n",
    "- **tokenize_nltk** uses the default method of the nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "\n",
    "**All tokenizers have to lowercase the tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_space(object):\n",
    "    \"\"\"\n",
    "    Tokenize the tokens that are separated by whitespace (space, tab, newline). \n",
    "    We consider that any tokenization was applied in the text when we use this tokenizer.\n",
    "    \n",
    "    For example: \"hello\\tworld of\\nNLP\" is split in ['hello', 'world', 'of', 'NLP']\n",
    "    \"\"\"\n",
    "    # return a list of tokens\n",
    "    raise NotImplementedError(\"\")\n",
    "        \n",
    "def tokenize_nltk(object):\n",
    "    \"\"\"\n",
    "    This tokenizer uses the default function of nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "    \"\"\"\n",
    "    # return a list of tokens\n",
    "    raise NotImplementedError(\"\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - Filtering Insignificant Tokens\n",
    "\n",
    "#### 5.2.1 -  Question 4 (1 point)\n",
    "\n",
    "There are a set of tokens that are not signficant to the similarity comparison since they appear in many different threads pages. Thus, removing them decreases the vector dimensionality and turns the similarity calculation computationally cheaper. Describe the tokens that are insignificant for the thread similarity comparison? Moreover, implement the function filter_word that removes these words from a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_word():\n",
    "    raise NotImplementedError(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 - Stemming\n",
    "\n",
    "The process to convert words with the same stem (word reduction that keeps word prefixes) to a standard form is called *stemming*. For instance, \"fishing\", \"fished\" and \"fishes\" are transformed to the stem \"fish.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['visitor', 'from', 'all', 'over', 'the', 'world', 'fish', 'dure', 'the', 'summer', '.']\n",
      "['i', 'was', 'fish']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "\n",
    "word1 = [\"Visitors\", \"from\", \"all\", \"over\", \"the\", \"world\", \"fishes\", \"during\", \"the\", \"summer\",\".\"]\n",
    "\n",
    "print([ stemmer.stem(w) for w in word1])\n",
    "\n",
    "word2 = ['I', 'was', 'fishing',]\n",
    "print([ stemmer.stem(w) for w in word2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 - Question 5 (1 point) \n",
    "\n",
    "Explain how stemming can benift our search engine?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Data representation\n",
    "\n",
    "## 6.1 - Bag of Words\n",
    "\n",
    "Many algorithms only accept inputs that have the same size. However, there are some data types whose sizes are not fixed, for instance, a text can have an unlimited number of words. Imagine that we retrieve two sentences: ”Board games are much better than video games” and ”Monoply is an awesome game!”. These sentences are respectively named as Sentence 1 and 2. Table below depicts how we could represent both sentences using a fixed representation.\n",
    "\n",
    "|            | an | are | ! | monopoly | awesome | better | games | than | video | much | board | is | game |\n",
    "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
    "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
    "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n",
    "\n",
    "Each column of this table 2.1 represents one of 13 vocabulary words, whereas the rows contains the word\n",
    "frequencies in each sentence. For instance, the cell in row 1 and column 7 has the value 2\n",
    "because the word games occurs twice in Sentence 1. Since the rows have always 13 values, we\n",
    "could use those vectors to represent the Sentences 1 and 2. The table above illustrates a technique called bag-of-words. Bag-of-words represents a document as a vector whose dimensions are equal to the number of times that vocabulary words appeared in the document. Thus, each token will be related to a dimension, i.e., an integer.\n",
    "\n",
    "### 6.1.2 - Question 6 (2.5 points)\n",
    "\n",
    "Implement the bag-of-words model that weights the vector with the absolute word frequency.\n",
    "\n",
    "**For this exercise, you cannot use any external python library (e.g., scikit-learn). However, if you have a problem with memory size, you can use the class scipy.sparse.csr_matrix (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_count_bow(X):\n",
    "    \"\"\"\n",
    "    This method preprocesses the data using the pipeline object, relates each token to a specific integer and  \n",
    "    transforms the text in a vector. Vectors are weighted using the token frequencies in the sentence.\n",
    "\n",
    "    X: document tokens. e.g: [['I','will', 'be', 'back', '.'], ['Helllo', 'world', '!'], ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']]\n",
    "\n",
    "    :return: vector representation of each document\n",
    "    \"\"\"   \n",
    "    \n",
    "    raise NotImplementedError(\"\") \n",
    "    return []  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 - TF-IDF\n",
    "\n",
    "Using raw frequency in the bag-of-words can be problematic. The word frequency distribution\n",
    "is skewed - only a few words have high frequencies in a document. Consequently, the\n",
    "weight of these words will be much bigger than the other ones which can give them more\n",
    "impact on some tasks, like similarity comparison. Besides that, a set of words (including\n",
    "those with high frequency) appears in most of the documents and, therefore, they do not\n",
    "help to discriminate documents. For instance, the word *of* appears in a significant\n",
    "part of documents. Thus, having the word *of* does not make\n",
    "documents more or less similar. However, the word *terrible* is rarer and documents that\n",
    "have this word are more likely to be negative. TF-IDF is a technique that overcomes the word frequency disadvantages.\n",
    "\n",
    "TF-IDF weights the vector using inverse document frequency (IDF) and word frequency, called term frequency (TF).\n",
    "TF is the local information about how important is a word to a specific document.  IDF measures the discrimination level of the words in a dataset.  Common words in a domain are not helpful to discriminate documents since most of them contain these terms. So, to reduce their relevance in the documents, these words should have low weights in the vectors . \n",
    "The following equation calculates the word IDF:\n",
    "\\begin{equation}\n",
    "\tidf_i = \\log\\left( \\frac{N}{df_i} \\right),\n",
    "\\end{equation}\n",
    "where $N$ is the number of documents in the dataset, $df_i$ is the number of documents that contain a word $i$.\n",
    "The new weight $w_{ij}$ of a word $i$ in a document $j$ using TF-IDF is computed as:\n",
    "\\begin{equation}\n",
    "\tw_{ij} = tf_{ij} \\times idf_i,\n",
    "\\end{equation}\n",
    "where $tf_{ij}$ is the term frequency of word $i$ in the document $j$.\n",
    "\n",
    "\n",
    "### 6.2.1 - Question 7 (3.5 points)\n",
    "\n",
    "Implement a bag-of-words model that weights the vector using TF-IDF.\n",
    "\n",
    "**For this exercise, you cannot use any external python library (e.g., scikit-learn). However, if you have a problem with memory size, you can use the class scipy.sparse.csr_matrix (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tf_idf_bow(X):\n",
    "    \"\"\"\n",
    "    This method preprocesses the data using the pipeline object, calculates the IDF and TF and \n",
    "    transforms the text in vectors. Vectors are weighted using TF-IDF method.\n",
    "\n",
    "    X: document tokens. e.g: [['I','will', 'be', 'back', '.'], ['Helllo', 'world', '!'], ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']]\n",
    "\n",
    "    :return: vector representation of each document\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"\")\n",
    "    \n",
    "    return []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Our Recommendation System\n",
    "\n",
    "## 7.1 - Question 8 (1.5 points)\n",
    "\n",
    "The pipeline is a sequence of preprocessing steps that transform the raw data to a format that is suitable for your problem. For our problem, you have to implement a pipeline composed of the following steps:\n",
    "\n",
    "1. Concatenate answer, question and comment texts of thread $t$ in the dictionary thread_dict.\n",
    "2. Tokenize the thread texts.\n",
    "3. Filter the insignificant tokens.\n",
    "4. Stem the tokens\n",
    "5. Generate the vector representation using TFIDFBoW or CountBoW\n",
    "6. Returns thread ids and thread vector representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_pipeline(thread_dict, tokenization_type, vectorizer_type, enable_filter_tokens, enable_stemming):\n",
    "    \"\"\"\n",
    "    Preprocess and vectorize the threads.\n",
    "    \n",
    "    thread_dict: dictionary whose keys and values are thread ids and thread objects, respectively.\n",
    "    tokenization_type: two possible values \"space_tokenization\" and \"nltk_tokenization\".\n",
    "                            - space_tokenization: tokenize_space function is used to tokenize.\n",
    "                            - nltk_tokenization: tokenize_nltk function is used to tokenize.\n",
    "                            \n",
    "    vectorizer_type: two possible values \"count\" and \"tf_idf\".\n",
    "                            - count: use transform_count_bow to vectorize the text\n",
    "                            - tf_idf: use transform_tf_idf_bow to vectorize the text\n",
    "                            \n",
    "    enable_filter_tokens: enable the insignificant token removal;\n",
    "    \n",
    "    enable_stemming: enable stemming\n",
    "    \n",
    "    return: a list L with thread ids and matrix B that contains the vector of each thread. B[idx] is the fixed-length representation of L[idx].\n",
    "    \"\"\"\n",
    "    \n",
    "    raise NotImplementedError(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 - Question 9 (1.5 points)\n",
    "\n",
    "*Implement the function rank that returns a list of thread ids sorted by thread and query similarity*. We will use the [cosine similarity function](https://en.wikipedia.org/wiki/Cosine_similarity) to compare two threads. In this assignment, query is a thread without answers and comments.\n",
    "\n",
    "**Remove the query in the sorted list (rank output)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(query_id, all_thread_ids, X):\n",
    "    \"\"\"\n",
    "    Return a list of thread ids sorted by thread and query similarity. Cosine similarity is used to compare threads. \n",
    "    \n",
    "    query_id: thread id \n",
    "    all_thread_ids: list of thread ids\n",
    "    X: thread data representations\n",
    "    \n",
    "    return: ranked list of thread ids. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the similarity of thread representations(vectors) using cosine similarity function\n",
    "    # Sort the thread ids by the similarity\n",
    "    \n",
    "    raise NotImplementedError(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 - Evaluation\n",
    "\n",
    "We will test different configurations of our recommender system. These configurations are compared using the [mean average precision (MAP) metric](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision). Basically, the closer relevant threads are from ranked list begining, the higher MAP is. Additional materials to undertand MAP: [recall and precision over ranks](https://youtu.be/H7oAofuZjjE) and [MAP](https://youtu.be/pM6DJ0ZZee0).\n",
    "\n",
    "\n",
    "The function *eval* evaluates a specific configurantion of our recommender system\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def calculate_map(x):\n",
    "    res = 0.0\n",
    "    n = 0.0\n",
    "    for relevant_threads, ranked_list in x:\n",
    "        precisions = []\n",
    "        for k, thread_id in enumerate(ranked_list):\n",
    "            if thread_id in relevant_threads:\n",
    "                prec_at_k = (len(precisions) + 1)/(k+1)\n",
    "                precisions.append(prec_at_k)\n",
    "            if len(precisions) == len(relevant_threads):\n",
    "                break\n",
    "        res += mean(precisions)\n",
    "        n += 1\n",
    "    return res/n\n",
    "\n",
    "def eval(tokenization_type, vectorizer, enable_filter_tokens, enable_stemming):\n",
    "    all_thread_ids, X = nlp_pipeline(thread_index, tokenization_type, vectorizer, enable_filter_tokens, enable_stemming)\n",
    "    all_thread_ids = [int(t_id) for t_id in all_thread_ids]    \n",
    "    queries,relevant_threads = zip(*relevant_threads_by_query.items())\n",
    "    ranked_list = (rank(query_id, all_thread_ids, X) for query_id in queries)        \n",
    "    return calculate_map(zip(relevant_threads,ranked_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 - Question 10 (5 points)\n",
    "\n",
    "Evaluate our recommedation system performamnce(MAP) using each one of the following configurations:\n",
    "1. count(BoW) + space_tokenization (sans tokenizer)\n",
    "2. count(BoW) + nltk_tokenization\n",
    "3. count(BoW) + nltk_tokenization + Filtrer les tokens sans importance\n",
    "4. count(BoW) + nltk_tokenization + Filtrer les tokens sans importance + Stemming\n",
    "5. tf_idf + nltk_tokenization\n",
    "6. tf_idf + nltk_tokenization + Filtrer les tokens sans importance\n",
    "7. tf_idf + nltk_tokenization + Filtrer les tokens sans importance + Stemming \n",
    "\n",
    "Describe the results found by you and answer the following questions:\n",
    "- Was our recommendation system negatively or positively impacted by data preprocessing steps?\n",
    "- TF-IDF has achieved a better performance than CountBoW? If yes, why do you think that this has occurred? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
