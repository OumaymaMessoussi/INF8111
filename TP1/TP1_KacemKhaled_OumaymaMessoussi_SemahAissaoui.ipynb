{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e4pX9A7oHwn8"
   },
   "source": [
    "# INF8111 - Data Mining\n",
    "\n",
    "## TP1 SUMMER 2020 - recommendation system\n",
    "\n",
    "##### Team Members:\n",
    "\n",
    "    - Kacem Khaled ()\n",
    "    - Oumayma Messoussi ()\n",
    "    - Semah Aissaoui ()\n",
    "\n",
    "\n",
    "## 1 - Overview\n",
    "\n",
    "Stack Exchange is a network of question-and-answer (Q&A) websites on topics in diverse fields, each site covering a specific topic. On Stack Exchange website, a thread is composed of a question and their answers and comments. In this assignment, *we will implement a recommendation system that returns threads (question + answers) that are related to a specific question*. Before submitting questions, the  website will use this engine to show the most similar threads to users in order to reduce the number of duplicate questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8yrHdMqHwn9"
   },
   "source": [
    "## 2 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "StbM8sQsHwn-"
   },
   "source": [
    "Please run the code below to install the packages needed for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "XswlcaPnHwn_",
    "outputId": "a2a849a1-323b-4ca2-d45b-8655a63d0946"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want, you can use anaconda and install after nltk library\n",
    "# pip install --user numpy\n",
    "# pip install --user sklearn\n",
    "# pip install --user scipy\n",
    "# pip install --user nltk\n",
    "\n",
    "\n",
    "#python\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Cils0quHwoE"
   },
   "source": [
    "## 3 - Data\n",
    "\n",
    "Please download the zip file in the following url: https://drive.google.com/file/d/1032N1oZkytHlHs20AXE9jPMBQCTyhb6H/view?usp=sharing\n",
    "\n",
    "In this zip file, there are:\n",
    "\n",
    "1. test.json: This file contains queries (new questions) and the relevant threads(question + answer) for each one these queries.\n",
    "2. threads: It is a folder that contains the thread html sources of threads. Each html file name follows the pattern **thread_id.html**.\n",
    "\n",
    "\n",
    "Figure below depicts an thread page example:\n",
    "\n",
    "![thread_img](thread_example.png)\n",
    "\n",
    "The figure contains 4 hilighted areas. Area A, B, C, D and E are the question subject, question body, question comments, answer body, and anwer comments, respectively. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "10tBtP6uIT8r",
    "outputId": "7240128a-a0a1-4840-be2b-ca6d3e455f7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aBG6hlDPHwoF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# define the folder path that contain the data\n",
    "# FOLDER_PATH = \"Define folder path that contain threads folder and test.json\"\n",
    "#FOLDER_PATH = \"../../../datasets/TP1/dataset/\"\n",
    "#THREAD_FOLDER = os.path.join(FOLDER_PATH, 'threads')\n",
    "PATH = \"drive/My Drive/1 Polymtl/Ete20/INF8111/TP1/\" # changer le path avec votre path\n",
    "\n",
    "# Load the evaluation dataset\n",
    "import json\n",
    "\n",
    "\n",
    "#test = json.load(open(os.path.join(FOLDER_PATH, \"test.json\")))\n",
    "test = json.load(open(PATH+\"test.json\"))\n",
    "relevant_threads_by_query = dict()\n",
    "\n",
    "\n",
    "for (query_id, cand_id, label) in test: \n",
    "    if label == 'Irrelevant':\n",
    "        continue\n",
    "        \n",
    "    l = relevant_threads_by_query.setdefault(query_id, [])\n",
    "    l.append(cand_id)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yl-S0LaZHwoJ"
   },
   "source": [
    "## 4 - Web scraping\n",
    "\n",
    "Web scraping consists in extracting relevant data from pages and prepare it for computational analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yv_1MThAHwoK"
   },
   "source": [
    "### 4.1 - Question 1 (0.5 point)\n",
    "\n",
    "Special and non-ASCII characters can be encoded into their html representantion (html entities). For instance, apostrophe (') is encoded as **\\&amp;**. The webpage encoding in the data folder are incosistent. Only in a portion of the webpages, the **special** and **non-ASCII** characters were encoded into  html entities. We will fix this inconsistency by transforming the html entities into character representations, e.g., **\\&amp;** is represented as **'**.\n",
    "\n",
    "*Implement the function fix_encoding that encodes the html entities (special and non-ASCII characters) into their UTF-8 encoding.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_XYfE4BaHwoK"
   },
   "outputs": [],
   "source": [
    "from html import unescape\n",
    "\n",
    "def fix_encoding(text):\n",
    "    \"\"\"\n",
    "    Encodes the html entities in a text into UTF-8 encoding. For instance, \"I&apos;m ...\" => \"I'm ...\"\n",
    "    \n",
    "    :param text: string.\n",
    "    :return: fixed text(sting)\n",
    "    \"\"\"\n",
    "    return unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "u17eqOgdHwoO",
    "outputId": "707f13ba-2497-492f-9a6b-9b91b029b8b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello I'm & Foulen ‚àæ Fouelni &‚àæ‚àæ'&\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "txt = fix_encoding(\"Hello I&apos;m &amp; Foulen &ac; Fouelni &amp;&ac;&ac;&apos;&amp;\")\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DqoRWb3iHwoT"
   },
   "source": [
    "### 4.2 - Question 2 (3 points)\n",
    "\n",
    "Implement extract_data_from_page function. This function extracts question subject, question body, question comments, answer body, and anwer comments from the thread webpage. It returns a dictionary with the following structure: *{\"thread_id\": int,\"question\":{\"subject\": string, \"body\": string, \"comments\": [string]}, answers: [{\"body\": string, \"comments\": [string]}]}*\n",
    "\n",
    "**Use the fix_encoding function to fix the text encoding. You can use the library [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) in this question. All html tags have to be removed from comment, question and answer textual data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Y9eiiEfHwoU"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data_from_page(pagepath):\n",
    "    \"\"\"\n",
    "    Scrap question, answer and comments from thread page.\n",
    "    \n",
    "    :param pagepath: the path of thread html file.\n",
    "    :return: \n",
    "        {\n",
    "            \"thread_id\": thread id,\n",
    "            \"question\":{\n",
    "                \"subject\": question subject text (Area A in the figure), \n",
    "                \"body\": question body text (Area B in the figure), \n",
    "                \"comments\": list of comment texts (Area C in the figure)\n",
    "                }, \n",
    "            \"answers\": [\n",
    "                {\n",
    "                    \"body\": answer body text (Area D in the figure),\n",
    "                    \"comments\": list of answer texts (Area E in the figure)\n",
    "                }\n",
    "                ]\n",
    "            }\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    answer = {}\n",
    "    data['thread_id'] = pagepath.split('/')[-1][:-5]\n",
    "    data['question'] = {}\n",
    "    \n",
    "    soup = BeautifulSoup(open(pagepath,encoding='utf8'))\n",
    "    question =  soup.find(\"div\", class_=\"question\")\n",
    "    answers =  soup.find_all(\"div\", class_=\"answer\")\n",
    "\n",
    "    data['question']['subject'] = soup.find(\"a\", class_=\"question-hyperlink\").get_text()\n",
    "    data['question']['body'] = question.find(\"div\", class_=\"postcell post-layout--right\").find(\"div\", class_=\"post-text\").get_text().strip()\n",
    "    data['question']['comments'] = [s.get_text().strip() for s in question.find_all(\"span\", class_=\"comment-copy\")]\n",
    "    data['answers'] = []\n",
    "    for ans in answers:\n",
    "        answer['body'] = ans.find(\"div\", class_=\"post-text\").get_text().strip()\n",
    "        answer['comments'] = [s.get_text().strip() for s in ans.find_all(\"span\", class_=\"comment-copy\")]\n",
    "        data['answers'].append(answer)\n",
    "        answer = {}\n",
    "    return data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "mIAoe1NuHwoX",
    "outputId": "39535b40-35c4-4e5c-f6ea-5b60abeb7886"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-e9a1b6f6ae17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test on 255875915822.html, 100853498510.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpagepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTHREAD_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"255875915822.html\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_data_from_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpagepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'THREAD_FOLDER' is not defined"
     ]
    }
   ],
   "source": [
    "# test on 255875915822.html, 100853498510.html\n",
    "pagepath = os.path.join(THREAD_FOLDER, \"255875915822.html\") \n",
    "data = extract_data_from_page(pagepath)\n",
    "\n",
    "print(json.dumps(data, indent=4, sort_keys=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iavUxV3zHwoc"
   },
   "source": [
    "### 4.3 - Extract text from HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XyCldSU9Hwod"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool, TimeoutError\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "# Index each thread by its id\n",
    "#index_path = os.path.join(THREAD_FOLDER, 'threads.json')\n",
    "index_path = os.path.join(PATH, 'threads.json')\n",
    "if os.path.isfile(index_path):\n",
    "    # Load threads that webpage content were already extracted.\n",
    "    thread_index = json.load(open(index_path))\n",
    "else:\n",
    "    # Extract webpage content\n",
    "    # This can be slow (around 30 minutes). Test your code with a small sample. lxml parse is faster than html.parser\n",
    "    files = (os.path.join(THREAD_FOLDER, filename) for filename in os.listdir(THREAD_FOLDER))\n",
    "    threads = map(extract_data_from_page, files)\n",
    "    thread_index = dict(((thread['thread_id'], thread) for thread in tqdm(threads,total=28403)))\n",
    "    # Save preprocessed threads\n",
    "    json.dump(thread_index, open(index_path,'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H529U6FtHwoh"
   },
   "source": [
    "## 5 - Data Preprocessing\n",
    "\n",
    "Preprocessing is a crucial task in data mining. This task clean and transform the raw data in a format that can better suit data analysis and machine learning techniques. In natural language processing (NLP), *tokenization* and *stemming* are two well known preprocessing steps. Besides these two steps, we will implement an additional step that is designed exclusively for the twitter domain.\n",
    "\n",
    "### 5.1 - Tokenization\n",
    "\n",
    "In this preprocessing step, a *tokenizer* is responsible for breaking a text in a sequence of tokens (words, symbols, and punctuations). \n",
    "\n",
    "For instance, the sentence *It's the student's notebook.* can be split into the following list of tokens: ['It', \"'s\", 'the', 'student', \"'s\", 'notebook', '.'].\n",
    "\n",
    "\n",
    "#### 5.1.1 - Question 3 (0.5 point) \n",
    "\n",
    "Implement the following functions: \n",
    "- **tokenize_space** tokenizes the tokens that are separated by whitespace (space, tab, newline). This is a naive tokenizer.\n",
    "\n",
    "- **tokenize_nltk** uses the default method of the nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "\n",
    "**All tokenizers have to lowercase the tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yAnbM-K2Hwoh"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize_space(object):\n",
    "    \"\"\"\n",
    "    Tokenize the tokens that are separated by whitespace (space, tab, newline). \n",
    "    We consider that any tokenization was applied in the text when we use this tokenizer.\n",
    "    \n",
    "    For example: \"hello\\tworld of\\nNLP\" is split in ['hello', 'world', 'of', 'NLP']\n",
    "    \"\"\"\n",
    "    # return a list of tokens\n",
    "    return [w.lower() for w in object.split()] \n",
    "        \n",
    "def tokenize_nltk(object):\n",
    "    \"\"\"\n",
    "    This tokenizer uses the default function of nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "    \"\"\"\n",
    "    # return a list of tokens\n",
    "    return [w.lower() for w in word_tokenize(object)] \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "X8qzuU5EHwol",
    "outputId": "2f087874-ec29-4e1b-f5ab-5d93e9838618"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'of', 'nlp']\n",
      "['hello', 'world', 'of', 'nlp']\n",
      "[\"it's\", 'the', \"student's\", 'notebook.']\n",
      "['it', \"'s\", 'the', 'student', \"'s\", 'notebook', '.']\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "msg1 =  \"hello\\tworld of\\nNLP\"\n",
    "msg2 = \"It's the student's notebook.\"\n",
    "\n",
    "print(tokenize_space(msg1))\n",
    "print(tokenize_nltk(msg1))\n",
    "\n",
    "print(tokenize_space(msg2))\n",
    "print(tokenize_nltk(msg2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCM8M0ShHwoo"
   },
   "source": [
    "### 5.2 - Filtering Insignificant Tokens\n",
    "\n",
    "#### 5.2.1 -  Question 4 (1 point)\n",
    "\n",
    "There are a set of tokens that are not signficant to the similarity comparison since they appear in many different threads pages. Thus, removing them decreases the vector dimensionality and turns the similarity calculation computationally cheaper. Describe the tokens that are insignificant for the thread similarity comparison? Moreover, implement the function filter_tokens that removes these words from a list of tokens.\n",
    "\n",
    "**We ignore the words that does not increase/decrease comprehension of the document such as preopositions because they could not measure the similarity between documents. Part of those words is assembled in the predefined lass stopwords.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KULr0rbKHwop"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def filter_tokens(words):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w.lower() for w in words if not w.lower() in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4amdpENmHwot",
    "outputId": "d488185f-9ca8-4864-b419-bb78f9541767"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'nlp']\n",
      "[\"'s\", 'student', \"'s\", 'notebook', '.']\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(filter_tokens(tokenize_nltk(msg1)))\n",
    "print(filter_tokens(tokenize_nltk(msg2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23HKJoO_Hwow"
   },
   "source": [
    "## 5.3 - Stemming\n",
    "\n",
    "The process to convert words with the same stem (word reduction that keeps word prefixes) to a standard form is called *stemming*. For instance, \"fishing\", \"fished\" and \"fishes\" are transformed to the stem \"fish.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "O5M9_YijHwox",
    "outputId": "d3e17eda-d09f-4fe2-8007-603ee37e2d12",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['visitor', 'from', 'all', 'over', 'the', 'world', 'fish', 'dure', 'the', 'summer', '.']\n",
      "['i', 'was', 'fish']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "\n",
    "word1 = [\"Visitors\", \"from\", \"all\", \"over\", \"the\", \"world\", \"fishes\", \"during\", \"the\", \"summer\",\".\"]\n",
    "\n",
    "print([ stemmer.stem(w) for w in word1])\n",
    "\n",
    "word2 = ['I', 'was', 'fishing',]\n",
    "print([ stemmer.stem(w) for w in word2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V6UbanOdHwo0"
   },
   "source": [
    "### 5.3.1 - Question 5 (1 point) \n",
    "\n",
    "Explain how stemming can benift our search engine?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zrF40BQtHwo1"
   },
   "source": [
    "\n",
    "**The stemming is useful for our comparison since by looking at two subjects, we can find two words which have been written in different ways but they have the same root such as: fish and peach. So to distinguish if two words have the same meaning or the same root, we apply stemming to directly compare the words by their roots. In other words, Stemming can help our search engine through reducing the vocabulary and therefore reduce the search time. It helps focusing on the sense of a text instead of its deeper meaning.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RtLi7u2gHwo2"
   },
   "source": [
    "# 6 - Data representation\n",
    "\n",
    "## 6.1 - Bag of Words\n",
    "\n",
    "Many algorithms only accept inputs that have the same size. However, there are some data types whose sizes are not fixed, for instance, a text can have an unlimited number of words. Imagine that we retrieve two sentences: ‚ÄùBoard games are much better than video games‚Äù and ‚ÄùMonoply is an awesome game!‚Äù. These sentences are respectively named as Sentence 1 and 2. Table below depicts how we could represent both sentences using a fixed representation.\n",
    "\n",
    "|<i></i>        | an | are | ! | monopoly | awesome | better | games | than | video | much | board | is | game |\n",
    "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
    "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
    "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n",
    "\n",
    "Each column of this table 2.1 represents one of 13 vocabulary words, whereas the rows contains the word\n",
    "frequencies in each sentence. For instance, the cell in row 1 and column 7 has the value 2\n",
    "because the word games occurs twice in Sentence 1. Since the rows have always 13 values, we\n",
    "could use those vectors to represent the Sentences 1 and 2. The table above illustrates a technique called bag-of-words. Bag-of-words represents a document as a vector whose dimensions are equal to the number of times that vocabulary words appeared in the document. Thus, each token will be related to a dimension, i.e., an integer.\n",
    "\n",
    "### 6.1.2 - Question 6 (2.5 points)\n",
    "\n",
    "Implement the bag-of-words model that weights the vector with the absolute word frequency.\n",
    "\n",
    "**For this exercise, you cannot use any external python library (e.g., scikit-learn). However, if you have a problem with memory size, you can use the class scipy.sparse.csr_matrix (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)\n",
    "https://stackoverflow.com/questions/52299420/scipy-csr-matrix-understand-indptr\n",
    "https://en.wikipedia.org/wiki/Sparse_matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "g-2EZukXHwo2",
    "outputId": "6c2438c6-eb4a-4f92-a988-279f9ef5b45e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#DENSE MATRIX\n",
      "[[0 0 1 4]\n",
      " [2 0 0 5]\n",
      " [0 0 0 0]\n",
      " [5 6 0 7]]\n",
      "#SPARSE MATRIX\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t4\n",
      "  (1, 0)\t2\n",
      "  (1, 3)\t5\n",
      "  (3, 0)\t5\n",
      "  (3, 1)\t6\n",
      "  (3, 3)\t7\n",
      "#SPARSE MATRIX=> DENSE MATRIX\n",
      "[[0 0 1 4]\n",
      " [2 0 0 5]\n",
      " [0 0 0 0]\n",
      " [5 6 0 7]]\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "a = np.asarray([[0,0,1,4],[2,0,0,5], [0,0,0,0], [5,6,0,7]]) # Representation of 4 sentences. Vocabulary size = 4\n",
    "print(\"#DENSE MATRIX\")\n",
    "print(a)\n",
    "data = [1,4,2,5,5,6,7] # All non-zero values\n",
    "indices = [2,3,0,3,0,1,3] # Column index of each non-zero value\n",
    "indptr = [0,2,4,4,7] #  then maps the elements of data and indices to the rows of the sparse matrix. This is done with the following reasoning:.\n",
    "b = csr_matrix((data, indices, indptr))\n",
    "print(\"#SPARSE MATRIX\")\n",
    "print(b)\n",
    "print(\"#SPARSE MATRIX=> DENSE MATRIX\")\n",
    "print(b.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t9yl9Z8MHwpC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def transform_count_bow(X):\n",
    "    \"\"\"\n",
    "    This method preprocesses the data using the pipeline object, relates each token to a specific integer and  \n",
    "    transforms the text in a vector. Vectors are weighted using the token frequencies in the sentence.\n",
    "\n",
    "    X: document tokens. e.g: [['I','will', 'be', 'back', '.'], ['Helllo', 'world', '!'], ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']]\n",
    "\n",
    "    :return: vector representation of each document\n",
    "    \"\"\" \n",
    "\n",
    "    print(\"\\ngenerating count bow :\")\n",
    "    indices = []\n",
    "    data = []\n",
    "    indptr = [0]\n",
    "    vocab= {}  \n",
    "    for doc in X:\n",
    "        for word in doc:\n",
    "            index = vocab.setdefault(word, len(vocab))\n",
    "            indices.append(index)\n",
    "            data.append(1)\n",
    "        indptr.append(len(indices))\n",
    "    print('length of vocab:',len(vocab))\n",
    "    return csr_matrix((data, indices, indptr), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eu2y9vi_HwpF",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test\n",
    "X = [['I','will', 'be', 'back', '.'], ['I','you', 'I', 'you','?'] , ['Hello', 'world', '!'], ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']]\n",
    "bow = transform_count_bow(X)\n",
    "print(bow.toarray())\n",
    "#print(bow.data)\n",
    "#print(bow.indices)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-kUNjQLsHwpI"
   },
   "source": [
    "## 6.2 - TF-IDF\n",
    "\n",
    "Using raw frequency in the bag-of-words can be problematic. The word frequency distribution\n",
    "is skewed - only a few words have high frequencies in a document. Consequently, the\n",
    "weight of these words will be much bigger than the other ones which can give them more\n",
    "impact on some tasks, like similarity comparison. Besides that, a set of words (including\n",
    "those with high frequency) appears in most of the documents and, therefore, they do not\n",
    "help to discriminate documents. For instance, the word *of* appears in a significant\n",
    "part of documents. Thus, having the word *of* does not make\n",
    "documents more or less similar. However, the word *terrible* is rarer and documents that\n",
    "have this word are more likely to be negative. TF-IDF is a technique that overcomes the word frequency disadvantages.\n",
    "\n",
    "TF-IDF weights the vector using inverse document frequency (IDF) and word frequency, called term frequency (TF).\n",
    "TF is the local information about how important is a word to a specific document.  IDF measures the discrimination level of the words in a dataset.  Common words in a domain are not helpful to discriminate documents since most of them contain these terms. So, to reduce their relevance in the documents, these words should have low weights in the vectors . \n",
    "The following equation calculates the word IDF:\n",
    "\\begin{equation}\n",
    "\tidf_i = \\log\\left( \\frac{N}{df_i} \\right),\n",
    "\\end{equation}\n",
    "where $N$ is the number of documents in the dataset, $df_i$ is the number of documents that contain a word $i$.\n",
    "The new weight $w_{ij}$ of a word $i$ in a document $j$ using TF-IDF is computed as:\n",
    "\\begin{equation}\n",
    "\tw_{ij} = tf_{ij} \\times idf_i,\n",
    "\\end{equation}\n",
    "where $tf_{ij}$ is the term frequency of word $i$ in the document $j$.\n",
    "\n",
    "\n",
    "### 6.2.1 - Question 7 (3.5 points)\n",
    "\n",
    "Implement a bag-of-words model that weights the vector using TF-IDF.\n",
    "\n",
    "**For this exercise, you cannot use any external python library (e.g., scikit-learn). However, if you have a problem with memory size, you can use the class scipy.sparse.csr_matrix (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6O1MkloFHwpI"
   },
   "outputs": [],
   "source": [
    "def transform_tf_idf_bow(X):\n",
    "    \"\"\"\n",
    "    This method preprocesses the data using the pipeline object, calculates the IDF and TF and \n",
    "    transforms the text in vectors. Vectors are weighted using TF-IDF method.\n",
    "\n",
    "    X: document tokens. e.g: [['I','will', 'be', 'back', '.'], ['Hello', 'world', '!'], ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']]\n",
    "\n",
    "    :return: vector representation of each document\n",
    "    \"\"\"\n",
    "    print(\"\\ngenerating tf-idf bow :\")\n",
    "    indices = []\n",
    "    tf = []\n",
    "    idf = []\n",
    "    indptr = [0]\n",
    "    vocab= {} # dict to collect each new word [key] and its index [value]\n",
    "    df = {} # dict to collect nb of docs [value] that contain each word i [key]\n",
    "    N = len(X)\n",
    "    for doc in X:\n",
    "        for word in doc:\n",
    "            index = vocab.setdefault(word, len(vocab))\n",
    "            indices.append(index)\n",
    "            tf.append(1)\n",
    "            if word in df:\n",
    "                df[word] += 1\n",
    "            else:\n",
    "                df.setdefault(word, 1)\n",
    "        indptr.append(len(indices))\n",
    "    print('length of vocab:',len(vocab))\n",
    "    for doc in X:\n",
    "        for i in doc:\n",
    "            idf.append((np.log2(N / df[i])))\n",
    "    w = np.array(tf) * np.array(idf)\n",
    "    return csr_matrix((list(w), indices, indptr), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_23I45cHwpL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bow = transform_tf_idf_bow(X)\n",
    "print(bow.toarray())\n",
    "print(bow.data)\n",
    "print(bow.indices)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6X5bSLGjHwpO"
   },
   "source": [
    "# 7 - Our Recommendation System\n",
    "\n",
    "## 7.1 - Question 8 (1.5 points)\n",
    "\n",
    "The pipeline is a sequence of preprocessing steps that transform the raw data to a format that is suitable for your problem. For our problem, you have to implement a pipeline composed of the following steps:\n",
    "\n",
    "1. Concatenate answer, question and comment texts of thread $t$ in the dictionary thread_dict.\n",
    "2. Tokenize the thread texts.\n",
    "3. Filter the insignificant tokens.\n",
    "4. Stem the tokens\n",
    "5. Generate the vector representation using TFIDFBoW or CountBoW\n",
    "6. Returns thread ids and thread vector representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZP8NJCl2HwpP"
   },
   "outputs": [],
   "source": [
    "def nlp_pipeline(thread_dict, tokenization_type, vectorizer_type, enable_filter_tokens, enable_stemming):\n",
    "    \"\"\"\n",
    "    Preprocess and vectorize the threads.\n",
    "    \n",
    "    thread_dict: dictionary whose keys and values are thread ids and thread objects, respectively.\n",
    "    tokenization_type: two possible values \"space_tokenization\" and \"nltk_tokenization\".\n",
    "                            - space_tokenization: tokenize_space function is used to tokenize.\n",
    "                            - nltk_tokenization: tokenize_nltk function is used to tokenize.\n",
    "                            \n",
    "    vectorizer_type: two possible values \"count\" and \"tf_idf\".\n",
    "                            - count: use transform_count_bow to vectorize the text\n",
    "                            - tf_idf: use transform_tf_idf_bow to vectorize the text\n",
    "                            \n",
    "    enable_filter_tokens: enable the insignificant token removal;\n",
    "    \n",
    "    enable_stemming: enable stemming\n",
    "    \n",
    "    return: a list L with thread ids and matrix B that contains the vector of each thread. B[idx] is the fixed-length representation of L[idx].\n",
    "    \"\"\"\n",
    "    # enable this to limit the results on the first 5 threads of thread_dict\n",
    "    testing = False # to limit testing\n",
    "    \n",
    "    L = list(thread_dict.keys())\n",
    "    if testing: L = L[:5] # to limit testing\n",
    "    tokens_list = []\n",
    "    if testing: count=0 # to limit testing\n",
    "    print(\"Pipeline:\")\n",
    "    for thread in thread_dict.values():\n",
    "        thread_concat = []\n",
    "        # 1. Concatenate answer, question and comment texts of thread  ùë°  in the dictionary thread_dict.\n",
    "        thread_concat = thread[\"question\"][\"subject\"]+' '\n",
    "        thread_concat += thread[\"question\"][\"body\"] + ' '\n",
    "        for question_comment in thread[\"question\"][\"comments\"]:\n",
    "            thread_concat += ((question_comment + ' '))\n",
    "        for answer in thread[\"answers\"]:\n",
    "            thread_concat += ((answer['body'] + ' '))\n",
    "            for answer_comment in answer[\"comments\"]:\n",
    "                thread_concat += ((answer_comment + ' '))\n",
    "        #print(thread_concat)\n",
    "        # 2. Tokenize the thread texts.\n",
    "        if tokenization_type == \"space_tokenization\":\n",
    "            tokens = tokenize_space(thread_concat)\n",
    "        elif tokenization_type == \"nltk_tokenization\":\n",
    "            tokens = tokenize_nltk(thread_concat)\n",
    "        else:\n",
    "            raise Exception(\"Undefined tokenization_type : \",tokenization_type)\n",
    "        \n",
    "        # 3. Filter the insignificant tokens\n",
    "        if enable_filter_tokens:\n",
    "            tokens = filter_tokens(tokens)\n",
    "        elif type(enable_filter_tokens)!=bool:\n",
    "            raise Exception(\"enable_filter_tokens can only be True or False!\")\n",
    "\n",
    "        # 4. Stem the tokens\n",
    "        if enable_stemming:\n",
    "            tokens = [ stemmer.stem(token) for token in tokens]\n",
    "        elif type(enable_stemming)!=bool:\n",
    "            raise Exception(\"enable_stemming can only be True or False!\")\n",
    "        \n",
    "        tokens_list.append(tokens)\n",
    "        \n",
    "        if testing: count+=1 # to limit testing\n",
    "        if testing and count == 5: break # to limit testing\n",
    "            \n",
    "    # 5. Generate the vector representation using TFIDFBoW or CountBoW\n",
    "    if vectorizer_type == \"count\":\n",
    "        B = transform_count_bow(tokens_list)\n",
    "    elif vectorizer_type == \"tf_idf\":\n",
    "        B = transform_tf_idf_bow(tokens_list)\n",
    "    else:\n",
    "        raise Exception(\"Undefined vectorizer_type : \",vectorizer_type)\n",
    "    print(\"Pipeline : done\")\n",
    "    # 6. Returns thread ids and thread vector representations.\n",
    "    return L,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G6j2WQtgHwpS"
   },
   "outputs": [],
   "source": [
    "# test\n",
    "L,B = nlp_pipeline(thread_dict=thread_index, tokenization_type=\"nltk_tokenization\", vectorizer_type=\"count\", enable_filter_tokens=True, enable_stemming=True)\n",
    "print(L[:5])\n",
    "#print(len(B))\n",
    "#print(B.toarray().shape)\n",
    "#print(B.toarray())\n",
    "print(B)\n",
    "print(B[1].shape)\n",
    "print(B[0])\n",
    "thread = thread_index[\"100021749708\"]\n",
    "print(json.dumps(thread, indent=4, sort_keys=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vVl3OysgHwpV"
   },
   "source": [
    "## 7.2 - Question 9 (1.5 points)\n",
    "\n",
    "*Implement the function rank that returns a list of thread ids sorted by thread and query similarity*. We will use the [cosine similarity function](https://en.wikipedia.org/wiki/Cosine_similarity) to compare two threads. In this assignment, query is a thread without answers and comments.\n",
    "\n",
    "**Remove the query in the sorted list (rank output)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pMS-fiREHwpW"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def rank(query_id, all_thread_ids, X):\n",
    "    \"\"\"\n",
    "    Return a list of thread ids sorted by thread and query similarity. Cosine similarity is used to compare threads. \n",
    "    \n",
    "    query_id: thread id \n",
    "    all_thread_ids: list of thread ids\n",
    "    X: thread data representations\n",
    "    \n",
    "    return: ranked list of thread ids. \n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the similarity of thread representations(vectors) using cosine similarity function\n",
    "\n",
    "    i = all_thread_ids.index(query_id)\n",
    "    similarities = cosine_similarity(X, X[i])\n",
    "    similaritiesD = dict(zip(all_thread_ids, similarities.T.tolist()[0]))\n",
    "    del similaritiesD[query_id]\n",
    "    return [x[0] for x in sorted(similaritiesD.items(), key=lambda x:x[1], reverse=True)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RQC8_QPGHwpb"
   },
   "outputs": [],
   "source": [
    "# test\n",
    "all_thread_ids, X = nlp_pipeline(thread_dict=thread_index, tokenization_type=\"nltk_tokenization\", vectorizer_type=\"count\", enable_filter_tokens=True, enable_stemming=True)\n",
    "print(all_thread_ids)\n",
    "element = '100083250150'\n",
    "similarities = rank(element, all_thread_ids, X)\n",
    "print(similarities)\n",
    "print(all_thread_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HzuueGy4Hwpf"
   },
   "source": [
    "## 7.3 - Evaluation\n",
    "\n",
    "We will test different configurations of our recommender system. These configurations are compared using the [mean average precision (MAP) metric](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision). Basically, the closer relevant threads are from ranked list begining, the higher MAP is. Additional materials to undertand MAP: [recall and precision over ranks](https://youtu.be/H7oAofuZjjE) and [MAP](https://youtu.be/pM6DJ0ZZee0).\n",
    "\n",
    "\n",
    "The function *eval* evaluates a specific configurantion of our recommender system\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fc8saG_0Hwpf"
   },
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "\n",
    "\n",
    "def calculate_map(x):\n",
    "    res = 0.0\n",
    "    n = 0.0\n",
    "    \n",
    "    \n",
    "    for relevant_threads, ranked_list in x:\n",
    "        precisions = []\n",
    "               \n",
    "        for k, thread_id in enumerate(ranked_list):\n",
    "            if thread_id in relevant_threads:\n",
    "                prec_at_k = (len(precisions) + 1)/(k+1)\n",
    "                precisions.append(prec_at_k)\n",
    "                \n",
    "            if len(precisions) == len(relevant_threads):\n",
    "                break\n",
    "        res += mean(precisions)\n",
    "        n += 1\n",
    "\n",
    "    \n",
    "    return res/n\n",
    "            \n",
    "\n",
    "def eval(tokenization_type, vectorizer, enable_filter_tokens, enable_stemming):\n",
    "    all_thread_ids, X = nlp_pipeline(thread_index, tokenization_type, vectorizer, enable_filter_tokens, enable_stemming)\n",
    "    all_thread_ids = [int(t_id) for t_id in all_thread_ids]    \n",
    "    queries,relevant_threads = zip(*relevant_threads_by_query.items())\n",
    "    ranked_list = (rank(query_id, all_thread_ids, X) for query_id in queries)\n",
    "        \n",
    "        \n",
    "    return calculate_map(zip(relevant_threads,ranked_list))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5wYaOTyAHwpl"
   },
   "source": [
    "## 7.4 - Question 10 (5 points)\n",
    "\n",
    "Evaluate our recommedation system performamnce(MAP) using each one of the following configurations:\n",
    "1. count(BoW) + space_tokenization (sans tokenizer)\n",
    "2. count(BoW) + nltk_tokenization\n",
    "3. count(BoW) + nltk_tokenization + Filtrer les tokens sans importance\n",
    "4. count(BoW) + nltk_tokenization + Filtrer les tokens sans importance + Stemming\n",
    "5. tf_idf + nltk_tokenization\n",
    "6. tf_idf + nltk_tokenization + Filtrer les tokens sans importance\n",
    "7. tf_idf + nltk_tokenization + Filtrer les tokens sans importance + Stemming \n",
    "\n",
    "Describe the results found by you and answer the following questions:\n",
    "- Was our recommendation system negatively or positively impacted by data preprocessing steps?\n",
    "- TF-IDF has achieved a better performance than CountBoW? If yes, why do you think that this has occurred? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b6k_hTe3Hwpm"
   },
   "outputs": [],
   "source": [
    "# 1. count(BoW) + space_tokenization (sans tokenizer)\n",
    "test1= eval(tokenization_type = \"space_tokenization\", vectorizer=\"count\", enable_filter_tokens=False, enable_stemming=False)\n",
    "print(\"\\n1:\",test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ntnCZEKkHwpq"
   },
   "outputs": [],
   "source": [
    "# 2. count(BoW) + nltk_tokenization\n",
    "test2= eval(tokenization_type = \"nltk_tokenization\", vectorizer=\"count\", enable_filter_tokens=False, enable_stemming=False)\n",
    "print(\"\\n2:\",test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "czX-6kwqHwpt"
   },
   "outputs": [],
   "source": [
    "# 3. count(BoW) + nltk_tokenization + Filtrer les tokens sans importance\n",
    "test3= eval(tokenization_type = \"nltk_tokenization\", vectorizer=\"count\", enable_filter_tokens=True, enable_stemming=False)\n",
    "print(\"\\n3:\",test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YjXplNKHwpw"
   },
   "outputs": [],
   "source": [
    "# 4. count(BoW) + nltk_tokenization + Filtrer les tokens sans importance + Stemming\n",
    "test4= eval(tokenization_type = \"nltk_tokenization\", vectorizer=\"count\", enable_filter_tokens=True, enable_stemming=True)\n",
    "print(\"\\n4:\",test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v3QBFzUeHwp6"
   },
   "outputs": [],
   "source": [
    "# 5. tf_idf + nltk_tokenization\n",
    "test5= eval(tokenization_type = \"nltk_tokenization\", vectorizer=\"tf_idf\", enable_filter_tokens=False, enable_stemming=False)\n",
    "print(\"\\n5:\",test5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Jw6iZPSHwp-"
   },
   "outputs": [],
   "source": [
    "# 6. tf_idf + nltk_tokenization + Filtrer les tokens sans importance\n",
    "test6= eval(tokenization_type = \"nltk_tokenization\", vectorizer=\"tf_idf\", enable_filter_tokens=True, enable_stemming=False)\n",
    "print(\"\\n6:\",test6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HFID_CLHwqC"
   },
   "outputs": [],
   "source": [
    "# 7. tf_idf + nltk_tokenization + Filtrer les tokens sans importance + Stemming \n",
    "test7= eval(tokenization_type = \"nltk_tokenization\", vectorizer=\"tf_idf\", enable_filter_tokens=True, enable_stemming=True)\n",
    "print(\"\\n7:\",test7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WVGddKD1HwqG"
   },
   "source": [
    "|Test    | tokenization_type | vectorizer | enable_filter_tokens| enable_stemming |MPE |\n",
    "|------------|----|-----|---|----------|----------|\n",
    "| 1 | space_tokenization  | count  | False  | False | 0.09392091474140551 | \n",
    "| 2 | nltk_tokenization  | count  | False  | False | 0.09417570136174147 | \n",
    "| 3 | nltk_tokenization  | count  | True  | False | 0.18419627640997 | \n",
    "| 4 | nltk_tokenization  | count  | True  | True | 0.212339985842106 | \n",
    "| 5 | nltk_tokenization  | tf_idf  | False  | False | 0.016234752329669106 | \n",
    "| 6 | nltk_tokenization  | tf_idf  | True  | False | 0.06281772959310575 | \n",
    "| 7 | nltk_tokenization  | tf_idf  | True  | True | 0.06836937398190894 | \n",
    "\n",
    "\n",
    "Our recommendation system was positively influenced by the data pre-processing stages. This is proven by the test values ‚Äã‚Äãcarried out with filtration of unimportant tokens and the stemming\n",
    "\n",
    "Theoritically, yes, TF-IDF is more efficient than BoW since it eliminates the problem of redundance of words that do not matter. Seeing this word in common does not allow us to conclude that documents are similar. On the contrary, when the word is rare, documents containing this word are more likely to have similar content. TF-IDF is therefore a method which overcomes this problem. Unfortunately, here the results does not reflect that compared to CountBOW.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TP1_KacemKhaled_OumaymaMessoussi_SemahAissaoui.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
