{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF8111 - Data Mining\n",
    "\n",
    "## TP1 SUMMER 2020 - recommendation system\n",
    "\n",
    "##### Team Members:\n",
    "\n",
    "    - Kacem Khaled\n",
    "    - Oumayma Messoussi\n",
    "    - Semah Aissaoui\n",
    "\n",
    "\n",
    "## 1 - Overview\n",
    "\n",
    "Stack Exchange is a network of question-and-answer (Q&A) websites on topics in diverse fields, each site covering a specific topic. On Stack Exchange website, a thread is composed of a question and their answers and comments. In this assignment, *we will implement a recommendation system that returns threads (question + answers) that are related to a specific question*. Before submitting questions, the  website will use this engine to show the most similar threads to users in order to reduce the number of duplicate questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the code below to install the packages needed for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want, you can use anaconda and install after nltk library\n",
    "# pip install --user numpy\n",
    "# pip install --user sklearn\n",
    "# pip install --user scipy\n",
    "# pip install --user nltk\n",
    "\n",
    "\n",
    "#python\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import scipy as sp\n",
    "import nltk\n",
    "import time\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Data\n",
    "\n",
    "Please download the zip file in the following url: https://drive.google.com/file/d/1032N1oZkytHlHs20AXE9jPMBQCTyhb6H/view?usp=sharing\n",
    "\n",
    "In this zip file, there are:\n",
    "\n",
    "1. test.json: This file contains queries (new questions) and the relevant threads(question + answer) for each one these queries.\n",
    "2. threads: It is a folder that contains the thread html sources of threads. Each html file name follows the pattern **thread_id.html**.\n",
    "\n",
    "\n",
    "Figure below depicts an thread page example:\n",
    "\n",
    "![thread_img](thread_example.png)\n",
    "\n",
    "The figure contains 4 hilighted areas. Area A, B, C, D and E are the question subject, question body, question comments, answer body, and anwer comments, respectively. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# define the folder path that contain the data\n",
    "# FOLDER_PATH = \"Define folder path that contain threads folder and test.json\"\n",
    "FOLDER_PATH = \"../../../datasets/TP1/dataset/\"\n",
    "THREAD_FOLDER = os.path.join(FOLDER_PATH, 'threads')\n",
    "\n",
    "\n",
    "# Load the evaluation dataset\n",
    "import json\n",
    "\n",
    "\n",
    "test = json.load(open(os.path.join(FOLDER_PATH, \"test.json\")))\n",
    "relevant_threads_by_query = dict()\n",
    "\n",
    "\n",
    "for (query_id, cand_id, label) in test: \n",
    "    if label == 'Irrelevant':\n",
    "        continue\n",
    "        \n",
    "    l = relevant_threads_by_query.setdefault(query_id, [])\n",
    "    l.append(cand_id)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Web scraping\n",
    "\n",
    "Web scraping consists in extracting relevant data from pages and prepare it for computational analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Question 1 (0.5 point)\n",
    "\n",
    "Special and non-ASCII characters can be encoded into their html representantion (html entities). For instance, apostrophe (') is encoded as **\\&apos;**. The webpage encoding in the data folder are incosistent. Only in a portion of the webpages, the **special** and **non-ASCII** characters were encoded into  html entities. We will fix this inconsistency by transforming the html entities into character representations, e.g., **\\&apos;** is represented as **'**.\n",
    "\n",
    "*Implement the function fix_encoding that encodes the html entities (special and non-ASCII characters) into their UTF-8 encoding.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.entities import html5\n",
    "from html import unescape\n",
    "\n",
    "def fix_encoding(text):\n",
    "    \"\"\"\n",
    "    Encodes the html entities in a text into UTF-8 encoding. For instance, \"I&amp;m ...\" => \"I'm ...\"\n",
    "    \n",
    "    :param text: string.\n",
    "    :return: fixed text(sting)\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    assert text != ''\n",
    "    \n",
    "#     start_index = text.find('&')\n",
    "#     while (start_index != -1):\n",
    "#         if text[start_index+1:start_index+4] == 'amp':\n",
    "#             start_index = text.find('&', start_index+1)\n",
    "#             continue\n",
    "#         end_index = text.find(';', start_index)\n",
    "#         text = text.replace(text[start_index:end_index+1], html5[text[start_index+1:end_index+1]])\n",
    "#         start_index = text.find('&', start_index+1)\n",
    "        \n",
    "#     text = text.replace('&amp;', html5['amp;'])\n",
    "\n",
    "    text = unescape(text)\n",
    "    print('Total runtime in sec = ' + str(time.time() - start))\n",
    "    \n",
    "    return text\n",
    "        \n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime in sec = 0.0\n",
      "hello I'm & oumayma ∾ messoussi &∾∾'&\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "txt = fix_encoding(\"hello I&apos;m &amp; oumayma &ac; messoussi &amp;&ac;&ac;&apos;&amp;\")\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Question 2 (3 points)\n",
    "\n",
    "Implement extract_data_from_page function. This function extracts question subject, question body, question comments, answer body, and anwer comments from the thread webpage. It returns a dictionary with the following structure: *{\"thread_id\": int,\"question\":{\"subject\": string, \"body\": string, \"comments\": [string]}, answers: [{\"body\": string, \"comments\": [string]}]}*\n",
    "\n",
    "**Use the fix_encoding function to fix the text encoding. You can use the library Beatiful Soap in this question. All html tags have to be removed from comment, question and answer textual data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data_from_page(pagepath):\n",
    "    \"\"\"\n",
    "    Scrap question, answer and comments from thread page.\n",
    "    \n",
    "    :param pagepath: the path of thread html file.\n",
    "    :return: \n",
    "        {\n",
    "            \"thread_id\": thread id,\n",
    "            \"question\":{\n",
    "                \"subject\": question subject text (Area A in the figure), \n",
    "                \"body\": question body text (Area B in the figure), \n",
    "                \"comments\": list of comment texts (Area C in the figure)\n",
    "                }, \n",
    "            \"answers\": [\n",
    "                {\n",
    "                    \"body\": answer body text (Area D in the figure),\n",
    "                    \"comments\": list of answer texts (Area E in the figure)\n",
    "                }\n",
    "                ]\n",
    "            }\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    data = {}\n",
    "    answer = {}\n",
    "    data['thread_id'] = pagepath.split('/')[-1][:-5]\n",
    "    data['question'] = {}\n",
    "    data['question']['comments'] = []\n",
    "    data['answers'] = []\n",
    "    \n",
    "    soup = BeautifulSoup(open(pagepath, encoding='utf8'), features=\"lxml\")\n",
    "    \n",
    "    data['question']['subject'] = soup.find('a', class_='question-hyperlink').get_text()\n",
    "            \n",
    "    QnA = soup.find(id='mainbar')\n",
    "    \n",
    "    question = QnA.find(id='question')     \n",
    "    raw_Q = question.find('div', class_='post-text')\n",
    "    data['question']['body'] = fix_encoding(raw_Q.get_text().strip())\n",
    "    \n",
    "    raw_Q_comm = question.find_all('div', class_='comments')\n",
    "    for Q_comm in raw_Q_comm:\n",
    "        comms = Q_comm.find_all('li')\n",
    "        for comm in comms:\n",
    "            span = comm.find('span', class_='comment-copy')\n",
    "            data['question']['comments'].append(fix_encoding(span.get_text().strip()))\n",
    "                        \n",
    "    answers = QnA.find(id='answers')\n",
    "    answers = answers.find_all('div', class_='answer')\n",
    "    for answer in answers:\n",
    "        tmp = {}\n",
    "        tmp['comments'] = []\n",
    "        ans = answer.find('div', class_='post-text')\n",
    "        tmp['body'] = fix_encoding(ans.get_text().strip())\n",
    "        \n",
    "        raw_A_comms = answer.find_all('div', class_='comments')\n",
    "        for A_comm in raw_A_comms:\n",
    "            comms = A_comm.find_all('li')\n",
    "            for comm in comms:\n",
    "                span = comm.find('span', class_='comment-copy')\n",
    "                tmp['comments'].append(fix_encoding(span.get_text().strip()))\n",
    "\n",
    "        data['answers'].append(tmp)\n",
    "\n",
    "#     question =  soup.find(\"div\", class_=\"question\")\n",
    "#     answers =  soup.find_all(\"div\", class_=\"answer\")\n",
    "\n",
    "#     data['question']['subject'] = soup.find(\"a\", class_=\"question-hyperlink\").get_text()\n",
    "#     data['question']['body'] = question.find(\"div\", class_=\"postcell post-layout--right\").find(\"div\", class_=\"post-text\").get_text().strip()\n",
    "#     data['question']['comments'] = [s.get_text().strip() for s in question.find_all(\"span\", class_=\"comment-copy\")]\n",
    "#     data['answers'] = []\n",
    "#     for ans in answers:\n",
    "#         answer['body'] = ans.find(\"div\", class_=\"post-text\").get_text().strip()\n",
    "#         answer['comments'] = [s.get_text().strip() for s in ans.find_all(\"span\", class_=\"comment-copy\")]\n",
    "#         data['answers'].append(answer)\n",
    "#         answer = {}\n",
    "    \n",
    "    print('Total runtime in sec = ' + str(time.time() - start))\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"thread_id\": \"255875915822\",\n",
      "    \"question\": {\n",
      "        \"comments\": [\n",
      "            \"I found an outside gym thinking I could increase my strength, but I wasn't able to do anything with the weights. I would like to think hunting would improve your shooting.\",\n",
      "            \"Does anything cause your skills to decrease\"\n",
      "        ],\n",
      "        \"subject\": \"What's the fastest/easiest way to level up your skills\",\n",
      "        \"body\": \"Basically subj. The skills and what I found so far is: Stamina: just run, sprint, cycle, swim, whatever. Just move and it will grow(not that fast though) Shooting: personally I found shooting range to be a really fast way to improve your shooting skills Strength: fist fighting Stealth: performing stealth kills Flying: probably just flying, didn't bother about it yet Driving: got the most problems here. Playing as a Trevor I can't get past 1.8 or 1.9 bars(which is 36-38%). Drove for hours around and still can't get it any higher Lung Capacity: staying underwater for as long as possible can very slowly but surely increase your lung capacity Am I correct with the ones I know And what would you recommend for the other ones \"\n",
      "    },\n",
      "    \"answers\": [\n",
      "        {\n",
      "            \"comments\": [],\n",
      "            \"body\": \"For flying I just did the flying school. For stealth just crouch tie a hair band around the analog sticks and just wait for 90 min. Many go see a movie like \\\"gravity.\\\" \"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [],\n",
      "            \"body\": \"I do believe that strength may increase a bit faster if you do counters with x and then a on the xbox360 as they are about to punch you. \"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [],\n",
      "            \"body\": \"Strength: If you want to train your strength really fast: get on the roof of a (driving!) bus and just kick with your bare feet. It's harder to find the bus than training your strength on it, trust me.  \"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [],\n",
      "            \"body\": \"Go to Trevor Airfield, southwest of his home. With a bike, do wheelies until your drive skills are 100%. \"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [],\n",
      "            \"body\": \"Best and easiest way lung capacity get diving gear from submarine or dingy just swim around don't have to come out of the water at all untill full trust me it works pretty fast. flying complete flight school it's faster. shooting shooting range is the fastest. Driving drive in oncoming lane as fast as you can without hitting anything. Strength for Micheal  Trevor play sets in tennis  win. Playing sets seem to work faster to me but i don't know. \"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [],\n",
      "            \"body\": \"For stealth, I put on a movie and walked around in stealth mode until it maxed out. Took about 30-45 minutes. Lung capacity was tedious. I just swam up and down the shore as much as I could. Took about an hour to max that out. Stamina is easy. Get a bike or run around. Also, hooking up with a stripper will increase your stamina. Driving- easy. Flying, go to flight school when it's available. You will max out before the end of the challenges. Shooting, go to the shooting range. Same as flight school. I'm still working on strength. Takes forever. I played a bunch of golf and punched a lot of people. Still not even half way to being maxed. They should add a gym or MMA competition(cage fighting) to work on strength.  \"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [],\n",
      "            \"body\": \"Strength: (Online) The best way to increase stat is to just have a buddy to sit inside a car, while you stand on top of it, and keep stomping on the roof  (This will method takes about 10min to max)  Stealth:: (Online) The best way to increase this is just to go into stealth mode and walk around. You don't even have to knock anyone out. (But it will take about 40min) \"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [],\n",
      "            \"body\": \"Once you get the hang of tennis, play a few sets with Michael and Trevor. One set seems to raise strength by at least 20%, and it can be done on easy mode. Takes way less time to get strength to 100 this way as opposed to punching people (and running from cops). \"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [],\n",
      "            \"body\": \"Lung Capacity. Since no one else mentioned this, your lung capacity actually increases when you're underwater while using a scuba mask. So if you have the sonar collections  dock purchased and started the Strangers and Freaks mission to collect the submarine parts, there will be a dinghy at the dock. Just hop in the dinghy go out to sea a bit and jump out. Your character should automatically put on the mask so just dive under the water and swim around till it maxes out. You can still do this after you've found all the sub parts, and maybe even before getting the mission if you just grab any dinghy but I'm not sure about that. \"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [],\n",
      "            \"body\": [\n",
      "                \"\\n    Stealth Set your player in stealth mode and tie a piece of string round your left stick and leave it for 30 or 40 mins, come back and Stealth will be 100% \\n    \"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [\n",
      "                \"can you please make this easier to understand\"\n",
      "            ],\n",
      "            \"body\": \"Only thing I would add is that strength is a lot easier to raise by punching cows.  You don't get the cops after you making it take less time.  Took me about an hour on Franklin (he starts with the lowest strength) \"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [\n",
      "                \"`Lung Capacity: this stats is increased by 1% for every minute spent underwater.` Even while you're in a submarine or using a diving suit\",\n",
      "                \"for Lung Capacity: is it 1 minute of in-game time or a real 1 minute\",\n",
      "                \"@Novarg I assume it's real time. 1 minute ingame is equivalent to 1 second real time. Spending only 2 real minutes underwater for 100% lung capacity doesn't seem very reasonable.\",\n",
      "                \"@Nolonar: yes, real time\",\n",
      "                \"*Stamina: this skill is increased by 1% for every 17 meters (18 yards) ran, every minute swam, every minute cycled* - so basically running will boost your stamina like gazillion times faster than cycling/swimming\",\n",
      "                \"@Novarg : there's fatigue while running, though, isn't it\",\n",
      "                \"@C.B. I think it also exists for cycling and swimming. When you are mashing *X* at least(PS3)\",\n",
      "                \"But stamina still increases while cycling/swimming *even without* mashing X...\",\n",
      "                \"So will getting bronze, silver, gold on the same shooting range challenge get you a 6% increase or only 3%\",\n",
      "                \"where's the flight school  is that online\",\n",
      "                \"@FistOfFury no, it's part of the single player story\",\n",
      "                \"@Nolonar I got a notification of improved Lung Capacity while swimming  in scuba gear.\"\n",
      "            ],\n",
      "            \"body\": \"According to the official GTA 5 Strategy Guide, here is a comprehensive list: Stamina: this skill is increased by 1% for every 17 meters (18 yards) ran, every minute swam, every minute cycled. Participating in triathlons is a great way to increase this ability. Shooting: this is increased quicker by landing hits on enemies, scoring headshots and successfully completing Shooting Range challenges. This skill is increased by 3% for every Gold challenge completed, 2% for every Silver and 1% for every Bronze.  Strength: this is increased 1% for every 20 punches. Playing sports (golf, tennis, darts) also increases strength. Stealth: this skill is increased by 1% for every 45 meters (49 yards) walked in Stealth Mode and by 1.5% for every two stealth kills.  Flying: flying for 10 minutes increases this skill by 1%. Completing the following training at the Flying School increases this skill by 3% for Gold, 2% for Silver and 1% for Bronze: Training take off, Runway landing, Inverted flight, Knife flight, Flat hatting, Loop the loop, Sky diving, Drop zone. Completing the following training at the Flying School increases this skill by 5% for Gold, 3% for Silver and 2% for Bronze: Touch down, Helicopter course. Completing the Helicopter speed run training increases the stats by 6%/4%/3%. Driving: this skill is increased by 1% for every 10 seconds of wheelie action and an additional 1% for every second spent in the air in a vehicle or four-wheel landing. Performing Stunt Jumps with four-wheel landings gives the greatest boost to this skill. Lung Capacity: this stats is increased by 1% for every minute spent underwater.  \"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [],\n",
      "            \"body\": \"Special Skilled by using up your Special gauge (press the left and right stick simultaneously) Stamina Skilled by using up your Stamina. Simply moving won't do, you need to move fast. It shouldn't be too difficult to skill, as long as you mash the A button (360) or the X button (PS3) while on foot, on a bicycle, or while swimming/diving. Shooting Skilled by shooting things, this skill will come along naturally. Fastest way of improving this skill is by visiting the shooting range, just like Michael recommended to Franklin. Strength Skilled by punching people. It's possible that visiting gyms can help improve this skill faster than punching everyone. Stealth Skilled by moving around stealthily. Whether people see you or not, does not seem to matter. Just press the left stick and move around. Apparently, silent takedowns also add to the stealth skill. Flying Skilled by flying around with planes, helicopters, or even the parachute. Fastest and easiest way of improving this skill is by visiting the flight school at the Los Santos airport. Driving According to the ingame description, this is skilled by performing perfect landings (when your land vehicle is airborne) or by doing wheelies. Based on that, it seems like the easiest and fastest way of improving this skill, is by driving over uneven terrain (offroad) instead of regular roads, whenever possible. Lung Capacity Skilled by diving (possibly without diving suit). \"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [],\n",
      "            \"body\": \"My suggestion for driving would be driving close to other cars very fast, so you get near misses. \"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [\n",
      "                \"Hello and welcome to gaming.stackexchange.com. We appreciate your willingness to help, but in the future please try to improve your wording and elaborate a little more than a one-sentence answer.\",\n",
      "                \"Your answer is lacking. There are 8 skills; are you telling us, beating people up will improve *all of them* Those gangsters also tend to gang up on you and shoot you to death upon even the slightest provocation, so it's definitely *not* an efficient way to skill strength.\"\n",
      "            ],\n",
      "            \"body\": \"By beating people up. Look around where Franklin lives - the gangsters are great practice. \"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [\n",
      "                \"What's \\\"nonlinear now\\\"\"\n",
      "            ],\n",
      "            \"body\": [\n",
      "                \"\\n    If you go to nonlinear now you can walk underwater if you sprint into it I just did that made my character go into stealth and then ruberbanded my controller to walk forward Works quite well just make sure you have a grenade or something to kill yourself so you don't have to walk all that way back unless your trying to get your stamina up Just check back everyonce and a while because you might come back and be stuck on a rock or coral just jump over it continue and watch out for dropoffs \\n    \"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [],\n",
      "            \"body\": \"STEALTH - Find a quite steep hill and stealth down that, you walk a lot faster downhill in stealth, therefore covering more ground in a quicker time, I found this the best way to improve that particular skill. \"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [],\n",
      "            \"body\": \"To train your strength, and mainly with Franklin, standing on a driving bus and kick, really works!(Can be mostly found in the area of Franklin's first house) Very fast too. The rest (Michael and Trevor) could also play Tennis to improve it fast. For Shooting go to the shooting range. For Stamina running and riding a bicycle. For Lung capacity for Michael you can do Yoga (goes fast) for the rest (Franklin and Trevor) just go diving and get up each time you're out of breath. Driving speaks for itself, drive fast and close past other cars and it will be full within an hour or so. These all work and are the best ways to do it! \"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [],\n",
      "            \"body\": \"The FASTEST way to increase strength in UNDER AN HOUR, is get on top of a bus, and using the attack button, stomp the crap out of the roof, even when it's speeding down the road.  Buses are easy to stay on top of.  Punching is a waste of time, believe me. \"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [],\n",
      "            \"body\": \"Stealth : go to the pier with the farris wheel and sneak behind people, kill them with a knife. Cops have a hard time getting out to you and the stealth kills quickly raise your stat. \"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [],\n",
      "            \"body\": \"For raising the strength stat (at least as Michael-haven't tried others yet), I managed to max him out with tennis. First I tried the end option (5 games I think) don't really KNOW tennis  assumed (wrongly) that'd be the max play-saw no noticeable change. What you want is SETS (I guess each set is 6 games)...first I played 5 sets (you only have to win 3 full ones)  my stat was 1/2 a bar away from full, so then I played 3 sets (only need win 2). I blew Amanda away/didn't lose a single game  BAM! Maxed out...also don't bother tracking in-game (can't hit down button, but CAN check via start menu), it doesn't boost your stat until game is over (so don't quit thinking nothing's happening-I didn't quit at all, so can't say how that may effect things). Hope that helps! *As a side-note, I didn't try yet but wonder if you call up one of the others/play a game if BOTH characters stat's will go up or just the one you control at the time-may be worth a look... \"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [],\n",
      "            \"body\": \"THE BEST way to upgrade STRENGTH.   Beat up on an unoccupied car with bare hands.  (do not use knife or bat or anything else)  It will only take you about 10 minutes. \"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [],\n",
      "            \"body\": \"Stamina - Do triathlons. Online, run or cycle to get around. Shooting - Get police on you and get headshots. Strength - Stomp the roof of an occupied bus. Stealth - Go into stealth mode, place your controller upside-down on the ground so that the           left stick moves your character and go AFK. Come back in half an hour. Flying - Flight School. Online, Fly to get around. Helicopters can be found easily or can be           called in from Merryweather. Driving - Get a motorcycle and wheelie in the middle of the oncoming lane on a moterway at            top-speed. Do the same stunt jumps for a while. Lung Cap. - Get diving gear and swim underwater for a while. Online, Swim around underwater             although you need to come back up for air. Done this and got all my characters (including Online) stats maxed in one day. \"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "data = extract_data_from_page(FOLDER_PATH+'threads/255875915822.html')\n",
    "print(json.dumps(data, indent=4, sort_keys=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime in sec = 0.0\n",
      "Total runtime in sec = 0.0\n",
      "Total runtime in sec = 0.0\n",
      "Total runtime in sec = 0.0\n",
      "Total runtime in sec = 0.0\n",
      "Total runtime in sec = 0.0\n",
      "Total runtime in sec = 0.192124605178833\n",
      "{\n",
      "    \"thread_id\": \"100410376731\",\n",
      "    \"question\": {\n",
      "        \"comments\": [\n",
      "            \"\\\"I have a number of schools\\\" and that number is...\",\n",
      "            \"I modified my question to show that I have 8 schools and a population of 475/47/54.\"\n",
      "        ],\n",
      "        \"subject\": \"How do you get to 100% Education\",\n",
      "        \"body\": \"I have a 8 schools set up in the game, and the are always fully staffed.  The Population is 475/47/54.  I have had less population, and still the same results.  However, I can't seem to get past about 72% educated in my 200-300 person city.\\n  Will more schools help, or do I need to do something different\"\n",
      "    },\n",
      "    \"answers\": [\n",
      "        {\n",
      "            \"comments\": [\n",
      "                \"And if they're not dying fast enough, send them to the mines ;-)\"\n",
      "            ],\n",
      "            \"body\": \"Only children can be educated, once someone becomes an uneducated adult, they remain uneducated until they die.\\n Eight staffed schools will be able to educate 160 students simultaneously, so should serve your youngsters well for the immediate future, as your older workers die off you should see your education percentage rise.\"\n",
      "        },\n",
      "        {\n",
      "            \"comments\": [],\n",
      "            \"body\": \"There are a few challenges to attaining a 100% education rate in Banished. 1) You must have school capacity greater than your maximum number of children. If you fill up and one slips through, it will take a long time for that one adult to die off to attain 100%. 2) You must make sure you never let a teacher die without a replacement. The students who are enrolled at the time will be expelled and not be marked as educated, as they have no teacher to teach them (essentially, the capacity decreases). 3) Don't accept nomads. Nomads are uneducated and include adults, who can't be educated. 4) Time. You'll need to wait for nearly complete turnover of your population, or at least until the last uneducated person dies off. Since it's difficult to identify and kill off the uneducated, the foolproof way is to simply wait them out.\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "data = extract_data_from_page(FOLDER_PATH+'threads/100410376731.html')\n",
    "print(json.dumps(data, indent=4, sort_keys=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Extract text from HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool, TimeoutError\n",
    "from time import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# FOLDER_PATH = \"../../../datasets/TP1/dataset/\"\n",
    "# THREAD_FOLDER = os.path.join(FOLDER_PATH, 'test')\n",
    "\n",
    "# Index each thread by its id\n",
    "index_path = os.path.join(THREAD_FOLDER, 'threads.json')\n",
    "if os.path.isfile(index_path):\n",
    "    \n",
    "    # Load threads that webpage content were already extracted.\n",
    "    thread_index = json.load(open(index_path))\n",
    "else:\n",
    "    \n",
    "    # Extract webpage content\n",
    "    # This can be slow (around 30 minutes). Test your code with a small sample. lxml parse is faster than html.parser\n",
    "    files = (os.path.join(THREAD_FOLDER, filename) for filename in os.listdir(THREAD_FOLDER))\n",
    "    threads = map(extract_data_from_page, files)\n",
    "    thread_index = dict(((thread['thread_id'], thread) for thread in tqdm(threads,total=28403)))\n",
    "    \n",
    "    # Save preprocessed threads\n",
    "    json.dump(thread_index, open(index_path,'w'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Data Preprocessing\n",
    "\n",
    "Preprocessing is a crucial task in data mining. This task cleans and transforms the raw data in a format that can better suit data analysis and machine learning techniques. In natural language processing (NLP), *tokenization* and *stemming* are two well known preprocessing steps. Besides these two steps, we will implement an additional step that is designed exclusively for the twitter domain.\n",
    "\n",
    "### 5.1 - Tokenization\n",
    "\n",
    "In this preprocessing step, a *tokenizer* is responsible for breaking a text in a sequence of tokens (words, symbols, and punctuations). \n",
    "\n",
    "For instance, the sentence *It's the student's notebook.* can be split into the following list of tokens: ['It', \"'s\", 'the', 'student', \"'s\", 'notebook', '.'].\n",
    "\n",
    "\n",
    "#### 5.1.1 - Question 3 (0.5 point) \n",
    "\n",
    "Implement the following functions: \n",
    "- **tokenize_space** tokenizes the tokens that are separated by whitespace (space, tab, newline). This is a naive tokenizer.\n",
    "- **tokenize_nltk** uses the default method of the nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "\n",
    "**All tokenizers have to lowercase the tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_space(object):\n",
    "    \"\"\"\n",
    "    Tokenize the tokens that are separated by whitespace (space, tab, newline). \n",
    "    We consider that any tokenization was applied in the text when we use this tokenizer.\n",
    "    \n",
    "    For example: \"hello\\tworld of\\nNLP\" is split in ['hello', 'world', 'of', 'NLP']\n",
    "    \"\"\"\n",
    "    # return a list of tokens\n",
    "    \n",
    "    assert isinstance(object, str)\n",
    "    return object.split()\n",
    "        \n",
    "def tokenize_nltk(object):\n",
    "    \"\"\"\n",
    "    This tokenizer uses the default function of nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "    \"\"\"\n",
    "    # return a list of tokens\n",
    "    \n",
    "    assert isinstance(object, str)\n",
    "    \n",
    "#     sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "#     return sent_detector.tokenize(object.strip())\n",
    "    return nltk.word_tokenize(object)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - Filtering Insignificant Tokens\n",
    "\n",
    "#### 5.2.1 -  Question 4 (1 point)\n",
    "\n",
    "There are a set of tokens that are not signficant to the similarity comparison since they appear in many different threads pages. Thus, removing them decreases the vector dimensionality and turns the similarity calculation computationally cheaper. Describe the tokens that are insignificant for the thread similarity comparison? Moreover, implement the function filter_word that removes these words from a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def filter_word(tokens):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w.lower() for w in tokens if w.lower() not in stop_words and w not in symbols]\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 - Stemming\n",
    "\n",
    "The process to convert words with the same stem (word reduction that keeps word prefixes) to a standard form is called *stemming*. For instance, \"fishing\", \"fished\" and \"fishes\" are transformed to the stem \"fish.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['visitor', 'from', 'all', 'over', 'the', 'world', 'fish', 'dure', 'the', 'summer', '.']\n",
      "['i', 'was', 'fish']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "word1 = [\"Visitors\", \"from\", \"all\", \"over\", \"the\", \"world\", \"fishes\", \"during\", \"the\", \"summer\",\".\"]\n",
    "print([stemmer.stem(w) for w in word1])\n",
    "\n",
    "word2 = ['I', 'was', 'fishing']\n",
    "print([stemmer.stem(w) for w in word2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 - Question 5 (1 point) \n",
    "\n",
    "Explain how stemming can benefit our search engine?\n",
    "\n",
    "--> Answer: It allows us to get more search results from more forms of words with the same stem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Data representation\n",
    "\n",
    "## 6.1 - Bag of Words\n",
    "\n",
    "Many algorithms only accept inputs that have the same size. However, there are some data types whose sizes are not fixed, for instance, a text can have an unlimited number of words. Imagine that we retrieve two sentences: ”Board games are much better than video games” and ”Monoply is an awesome game!”. These sentences are respectively named as Sentence 1 and 2. Table below depicts how we could represent both sentences using a fixed representation.\n",
    "\n",
    "|            | an | are | ! | monopoly | awesome | better | games | than | video | much | board | is | game |\n",
    "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
    "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
    "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n",
    "\n",
    "Each column of this table 2.1 represents one of 13 vocabulary words, whereas the rows contains the word\n",
    "frequencies in each sentence. For instance, the cell in row 1 and column 7 has the value 2\n",
    "because the word games occurs twice in Sentence 1. Since the rows have always 13 values, we\n",
    "could use those vectors to represent the Sentences 1 and 2. The table above illustrates a technique called bag-of-words. Bag-of-words represents a document as a vector whose dimensions are equal to the number of times that vocabulary words appeared in the document. Thus, each token will be related to a dimension, i.e., an integer.\n",
    "\n",
    "### 6.1.2 - Question 6 (2.5 points)\n",
    "\n",
    "Implement the bag-of-words model that weights the vector with the absolute word frequency.\n",
    "\n",
    "**For this exercise, you cannot use any external python library (e.g., scikit-learn). However, if you have a problem with memory size, you can use the class scipy.sparse.csr_matrix (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def transform_count_bow(X):\n",
    "    \"\"\"\n",
    "    This method preprocesses the data using the pipeline object, relates each token to a specific integer and  \n",
    "    transforms the text in a vector. Vectors are weighted using the token frequencies in the sentence.\n",
    "\n",
    "    X: document tokens. e.g: [['I','will', 'be', 'back', '.'], ['Hello', 'world', '!'], ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']]\n",
    "\n",
    "    :return: vector representation of each document\n",
    "    \"\"\"   \n",
    "    start = time.time()\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    vocab = []\n",
    "    for l in X:\n",
    "        for w in l:\n",
    "            vocab.append(w.lower())\n",
    "    vocab = list(set(vocab))\n",
    "    print(vocab)\n",
    "    \n",
    "    bow = np.zeros((len(X), len(vocab)))\n",
    "    for i, sentence in enumerate(X):\n",
    "        sentence = [w.lower() for w in sentence]\n",
    "        print(sentence)\n",
    "#         words = filter_word(sentence)\n",
    "#         print(words)\n",
    "#         stems = [stemmer.stem(w) for w in words]\n",
    "#         print(stems)\n",
    "        for j, v in enumerate(vocab):\n",
    "            bow[i,j] = sentence.count(v)\n",
    "        print('\\n')\n",
    "     \n",
    "    print(bow)\n",
    "    print('Total runtime in sec = ' + str(time.time() - start))\n",
    "    return bow\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['back', '!', 'on', 'damp', 'will', '.', 'using', 'you', 'a', 'if', 'insist', 'be', 'world', 'hello', 'i', 'cloth', 'call']\n",
      "['i', 'will', 'be', 'back', 'if', 'you', 'call', 'back', '.']\n",
      "\n",
      "\n",
      "['hello', 'world', '!']\n",
      "\n",
      "\n",
      "['if', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']\n",
      "\n",
      "\n",
      "[[2. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "text = [['I','will', 'be', 'back', 'if', 'you', 'call', 'back', '.'], ['Hello', 'world', '!'], ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']]\n",
    "boww = transform_count_bow(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 - TF-IDF\n",
    "\n",
    "Using raw frequency in the bag-of-words can be problematic. The word frequency distribution\n",
    "is skewed - only a few words have high frequencies in a document. Consequently, the\n",
    "weight of these words will be much bigger than the other ones which can give them more\n",
    "impact on some tasks, like similarity comparison. Besides that, a set of words (including\n",
    "those with high frequency) appears in most of the documents and, therefore, they do not\n",
    "help to discriminate documents. For instance, the word *of* appears in a significant\n",
    "part of documents. Thus, having the word *of* does not make\n",
    "documents more or less similar. However, the word *terrible* is rarer and documents that\n",
    "have this word are more likely to be negative. TF-IDF is a technique that overcomes the word frequency disadvantages.\n",
    "\n",
    "TF-IDF weights the vector using inverse document frequency (IDF) and word frequency, called term frequency (TF).\n",
    "TF is the local information about how important is a word to a specific document.  IDF measures the discrimination level of the words in a dataset.  Common words in a domain are not helpful to discriminate documents since most of them contain these terms. So, to reduce their relevance in the documents, these words should have low weights in the vectors . \n",
    "The following equation calculates the word IDF:\n",
    "\\begin{equation}\n",
    "\tidf_i = \\log\\left( \\frac{N}{df_i} \\right),\n",
    "\\end{equation}\n",
    "where $N$ is the number of documents in the dataset, $df_i$ is the number of documents that contain a word $i$.\n",
    "The new weight $w_{ij}$ of a word $i$ in a document $j$ using TF-IDF is computed as:\n",
    "\\begin{equation}\n",
    "\tw_{ij} = tf_{ij} \\times idf_i,\n",
    "\\end{equation}\n",
    "where $tf_{ij}$ is the term frequency of word $i$ in the document $j$.\n",
    "\n",
    "\n",
    "### 6.2.1 - Question 7 (3.5 points)\n",
    "\n",
    "Implement a bag-of-words model that weights the vector using TF-IDF.\n",
    "\n",
    "**For this exercise, you cannot use any external python library (e.g., scikit-learn). However, if you have a problem with memory size, you can use the class scipy.sparse.csr_matrix (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tf_idf_bow(X):\n",
    "    \"\"\"\n",
    "    This method preprocesses the data using the pipeline object, calculates the IDF and TF and \n",
    "    transforms the text in vectors. Vectors are weighted using TF-IDF method.\n",
    "\n",
    "    X: document tokens. e.g: [['I','will', 'be', 'back', '.'], ['Helllo', 'world', '!'], ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']]\n",
    "\n",
    "    :return: vector representation of each document\n",
    "    \"\"\"    \n",
    "    start = time.time()\n",
    "    print('*** in tf-idf')\n",
    "    vocab = []\n",
    "    N = len(X)\n",
    "    for i, l in enumerate(X):\n",
    "        for j, w in enumerate(l):\n",
    "            vocab.append(w.lower())\n",
    "    vocab = list(set(vocab))\n",
    "    print(vocab)\n",
    "    \n",
    "    df = np.zeros((len(vocab)))\n",
    "    for sentence in X:\n",
    "        for j, word in enumerate(vocab):\n",
    "            if word in [s.lower() for s in sentence]:\n",
    "                df[j] += 1\n",
    "                \n",
    "    tfidf = np.zeros((len(X), len(vocab)))\n",
    "    for i, sentence in enumerate(X):\n",
    "        print(sentence)\n",
    "        for j, v in enumerate(vocab):\n",
    "            tfidf[i,j] = [wd.lower() for wd in sentence].count(v) * np.log(N/df[j])\n",
    "    \n",
    "    print('*** end tf-idf')\n",
    "    print('Total runtime in sec = ' + str(time.time() - start))\n",
    "    return tfidf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** in tf-idf\n",
      "['back', '!', 'on', 'damp', 'will', '.', 'using', 'you', 'a', 'if', 'insist', 'be', 'world', 'hello', 'i', 'cloth', 'call']\n",
      "['I', 'will', 'be', 'back', 'if', 'you', 'call', 'back', '.']\n",
      "['Hello', 'world', '!']\n",
      "['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']\n",
      "*** end tf-idf\n",
      "[[2.19722458 0.         0.         0.         1.09861229 1.09861229\n",
      "  0.         0.40546511 0.         0.40546511 0.         1.09861229\n",
      "  0.         0.         1.09861229 0.         1.09861229]\n",
      " [0.         1.09861229 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.09861229 1.09861229 0.         0.         0.        ]\n",
      " [0.         0.         1.09861229 1.09861229 0.         0.\n",
      "  1.09861229 0.40546511 1.09861229 0.40546511 1.09861229 0.\n",
      "  0.         0.         0.         1.09861229 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "text = [['I','will', 'be', 'back', 'if', 'you', 'call', 'back', '.'], ['Hello', 'world', '!'], ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']]\n",
    "idf = transform_tf_idf_bow(text)\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** in tf-idf\n",
      "['back', '!', 'on', 'damp', 'will', '.', 'using', 'you', '?', 'a', 'if', 'insist', 'be', 'world', 'hello', 'i', 'cloth']\n",
      "['I', 'will', 'be', 'back', '.']\n",
      "['you', 'I', 'you', '?']\n",
      "['Hello', 'world', '!']\n",
      "['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']\n",
      "*** end tf-idf\n",
      "[[1.38629436 0.         0.         0.         1.38629436 1.38629436\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.38629436 0.         0.         0.69314718 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         1.38629436 1.38629436 0.         0.         0.\n",
      "  0.         0.         0.         0.69314718 0.        ]\n",
      " [0.         1.38629436 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.38629436 1.38629436 0.         0.        ]\n",
      " [0.         0.         1.38629436 1.38629436 0.         0.\n",
      "  1.38629436 0.69314718 0.         1.38629436 1.38629436 1.38629436\n",
      "  0.         0.         0.         0.         1.38629436]]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "X = [['I','will', 'be', 'back', '.'], ['you', 'I', 'you','?'] , ['Hello', 'world', '!'], ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']]\n",
    "idf = transform_tf_idf_bow(X)\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Our Recommendation System\n",
    "\n",
    "## 7.1 - Question 8 (1.5 points)\n",
    "\n",
    "The pipeline is a sequence of preprocessing steps that transform the raw data to a format that is suitable for your problem. For our problem, you have to implement a pipeline composed of the following steps:\n",
    "\n",
    "1. Concatenate answer, question and comment texts of thread $t$ in the dictionary thread_dict.\n",
    "2. Tokenize the thread texts.\n",
    "3. Filter the insignificant tokens.\n",
    "4. Stem the tokens\n",
    "5. Generate the vector representation using TFIDFBoW or CountBoW\n",
    "6. Returns thread ids and thread vector representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def nlp_pipeline(thread_dict, tokenization_type, vectorizer_type, enable_filter_tokens, enable_stemming):\n",
    "    \"\"\"\n",
    "    Preprocess and vectorize the threads.\n",
    "    \n",
    "    thread_dict: dictionary whose keys and values are thread ids and thread objects, respectively.\n",
    "    tokenization_type: two possible values \"space_tokenization\" and \"nltk_tokenization\".\n",
    "                            - space_tokenization: tokenize_space function is used to tokenize.\n",
    "                            - nltk_tokenization: tokenize_nltk function is used to tokenize.\n",
    "                            \n",
    "    vectorizer_type: two possible values \"count\" and \"tf_idf\".\n",
    "                            - count: use transform_count_bow to vectorize the text\n",
    "                            - tf_idf: use transform_tf_idf_bow to vectorize the text\n",
    "                            \n",
    "    enable_filter_tokens: enable the insignificant token removal;\n",
    "    \n",
    "    enable_stemming: enable stemming\n",
    "    \n",
    "    return: a list L with thread ids and matrix B that contains the vector of each thread. B[idx] is the fixed-length representation of L[idx].\n",
    "    \"\"\"\n",
    "    B = []\n",
    "    for thread_id, thread_obj in zip(thread_dict.keys(), thread_dict.values()):\n",
    "        thread_cont = []\n",
    "        thread_cont = thread_obj['question']['subject'] + ' ' + thread_obj['question']['body']\n",
    "        for Q_com in thread_obj['question']['comments']:\n",
    "            thread_cont += ' ' + Q_com\n",
    "        for answer in thread_obj['answers']:\n",
    "            thread_cont += ' ' + answer['body']\n",
    "            for A_com in answer['comments']:\n",
    "                thread_cont += ' ' + A_com\n",
    "                \n",
    "        print(thread_cont)\n",
    "        print('\\n')\n",
    "        assert tokenization_type == 'space_tokenization' or tokenization_type == 'nltk_tokenization', 'invalid tokenization_type'\n",
    "        if 'nltk' in tokenization_type:\n",
    "            tokenized_text = tokenize_nltk(thread_cont)  \n",
    "        else: \n",
    "            tokenized_text = tokenize_space(thread_cont)\n",
    "        print(tokenized_text)\n",
    "        print('\\n')\n",
    "        \n",
    "        if enable_filter_tokens:\n",
    "            filtered = filter_word(tokenized_text)  \n",
    "        else:\n",
    "            filtered = tokenized_text\n",
    "        print(filtered)\n",
    "        print('\\n')\n",
    "\n",
    "        if enable_stemming:\n",
    "            stemmer = SnowballStemmer(\"english\")\n",
    "            stemmed = [stemmer.stem(w) for w in filtered]\n",
    "        else:\n",
    "            stemmed = filtered\n",
    "        print(stemmed)\n",
    "        print('\\n')\n",
    "\n",
    "        assert vectorizer_type == 'count' or vectorizer_type == 'tf_idf', 'invalid vectorizer_type'\n",
    "        if vectorizer_type == 'count':\n",
    "            vectorized = transform_count_bow([stemmed])  \n",
    "        else: \n",
    "            vectorized = transform_tf_idf_bow([stemmed])\n",
    "        print(vectorized)\n",
    "        print('\\n')\n",
    "    \n",
    "        B.append(vectorized)\n",
    "        \n",
    "    return list(thread_dict.keys()), B\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"thread_id\": \"100410376731\",\n",
      "    \"question\": {\n",
      "        \"comments\": [\n",
      "            \"\\\"I have a number of schools\\\" and that number is...\",\n",
      "            \"I modified my question to show that I have 8 schools and a population of 475/47/54.\"\n",
      "        ],\n",
      "        \"subject\": \"How do you get to 100% Education\",\n",
      "        \"body\": \"I have a 8 schools set up in the game, and the are always fully staffed.  The Population is 475/47/54.  I have had less population, and still the same results.  However, I can't seem to get past about 72% educated in my 200-300 person city.\\n  Will more schools help, or do I need to do something different\"\n",
      "    },\n",
      "    \"answers\": [\n",
      "        {\n",
      "            \"body\": \"Only children can be educated, once someone becomes an uneducated adult, they remain uneducated until they die.\\n Eight staffed schools will be able to educate 160 students simultaneously, so should serve your youngsters well for the immediate future, as your older workers die off you should see your education percentage rise.\",\n",
      "            \"comments\": [\n",
      "                \"And if they're not dying fast enough, send them to the mines ;-)\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"body\": \"There are a few challenges to attaining a 100% education rate in Banished. 1) You must have school capacity greater than your maximum number of children. If you fill up and one slips through, it will take a long time for that one adult to die off to attain 100%. 2) You must make sure you never let a teacher die without a replacement. The students who are enrolled at the time will be expelled and not be marked as educated, as they have no teacher to teach them (essentially, the capacity decreases). 3) Don't accept nomads. Nomads are uneducated and include adults, who can't be educated. 4) Time. You'll need to wait for nearly complete turnover of your population, or at least until the last uneducated person dies off. Since it's difficult to identify and kill off the uneducated, the foolproof way is to simply wait them out.\",\n",
      "            \"comments\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "\n",
      "How do you get to 100% Education I have a 8 schools set up in the game, and the are always fully staffed.  The Population is 475/47/54.  I have had less population, and still the same results.  However, I can't seem to get past about 72% educated in my 200-300 person city.\n",
      "  Will more schools help, or do I need to do something different \"I have a number of schools\" and that number is... I modified my question to show that I have 8 schools and a population of 475/47/54. Only children can be educated, once someone becomes an uneducated adult, they remain uneducated until they die.\n",
      " Eight staffed schools will be able to educate 160 students simultaneously, so should serve your youngsters well for the immediate future, as your older workers die off you should see your education percentage rise. And if they're not dying fast enough, send them to the mines ;-) There are a few challenges to attaining a 100% education rate in Banished. 1) You must have school capacity greater than your maximum number of children. If you fill up and one slips through, it will take a long time for that one adult to die off to attain 100%. 2) You must make sure you never let a teacher die without a replacement. The students who are enrolled at the time will be expelled and not be marked as educated, as they have no teacher to teach them (essentially, the capacity decreases). 3) Don't accept nomads. Nomads are uneducated and include adults, who can't be educated. 4) Time. You'll need to wait for nearly complete turnover of your population, or at least until the last uneducated person dies off. Since it's difficult to identify and kill off the uneducated, the foolproof way is to simply wait them out.\n",
      "\n",
      "\n",
      "['How', 'do', 'you', 'get', 'to', '100', '%', 'Education', 'I', 'have', 'a', '8', 'schools', 'set', 'up', 'in', 'the', 'game', ',', 'and', 'the', 'are', 'always', 'fully', 'staffed', '.', 'The', 'Population', 'is', '475/47/54', '.', 'I', 'have', 'had', 'less', 'population', ',', 'and', 'still', 'the', 'same', 'results', '.', 'However', ',', 'I', 'ca', \"n't\", 'seem', 'to', 'get', 'past', 'about', '72', '%', 'educated', 'in', 'my', '200-300', 'person', 'city', '.', 'Will', 'more', 'schools', 'help', ',', 'or', 'do', 'I', 'need', 'to', 'do', 'something', 'different', '``', 'I', 'have', 'a', 'number', 'of', 'schools', \"''\", 'and', 'that', 'number', 'is', '...', 'I', 'modified', 'my', 'question', 'to', 'show', 'that', 'I', 'have', '8', 'schools', 'and', 'a', 'population', 'of', '475/47/54', '.', 'Only', 'children', 'can', 'be', 'educated', ',', 'once', 'someone', 'becomes', 'an', 'uneducated', 'adult', ',', 'they', 'remain', 'uneducated', 'until', 'they', 'die', '.', 'Eight', 'staffed', 'schools', 'will', 'be', 'able', 'to', 'educate', '160', 'students', 'simultaneously', ',', 'so', 'should', 'serve', 'your', 'youngsters', 'well', 'for', 'the', 'immediate', 'future', ',', 'as', 'your', 'older', 'workers', 'die', 'off', 'you', 'should', 'see', 'your', 'education', 'percentage', 'rise', '.', 'And', 'if', 'they', \"'re\", 'not', 'dying', 'fast', 'enough', ',', 'send', 'them', 'to', 'the', 'mines', ';', '-', ')', 'There', 'are', 'a', 'few', 'challenges', 'to', 'attaining', 'a', '100', '%', 'education', 'rate', 'in', 'Banished', '.', '1', ')', 'You', 'must', 'have', 'school', 'capacity', 'greater', 'than', 'your', 'maximum', 'number', 'of', 'children', '.', 'If', 'you', 'fill', 'up', 'and', 'one', 'slips', 'through', ',', 'it', 'will', 'take', 'a', 'long', 'time', 'for', 'that', 'one', 'adult', 'to', 'die', 'off', 'to', 'attain', '100', '%', '.', '2', ')', 'You', 'must', 'make', 'sure', 'you', 'never', 'let', 'a', 'teacher', 'die', 'without', 'a', 'replacement', '.', 'The', 'students', 'who', 'are', 'enrolled', 'at', 'the', 'time', 'will', 'be', 'expelled', 'and', 'not', 'be', 'marked', 'as', 'educated', ',', 'as', 'they', 'have', 'no', 'teacher', 'to', 'teach', 'them', '(', 'essentially', ',', 'the', 'capacity', 'decreases', ')', '.', '3', ')', 'Do', \"n't\", 'accept', 'nomads', '.', 'Nomads', 'are', 'uneducated', 'and', 'include', 'adults', ',', 'who', 'ca', \"n't\", 'be', 'educated', '.', '4', ')', 'Time', '.', 'You', \"'ll\", 'need', 'to', 'wait', 'for', 'nearly', 'complete', 'turnover', 'of', 'your', 'population', ',', 'or', 'at', 'least', 'until', 'the', 'last', 'uneducated', 'person', 'dies', 'off', '.', 'Since', 'it', \"'s\", 'difficult', 'to', 'identify', 'and', 'kill', 'off', 'the', 'uneducated', ',', 'the', 'foolproof', 'way', 'is', 'to', 'simply', 'wait', 'them', 'out', '.']\n",
      "\n",
      "\n",
      "['get', '100', 'education', '8', 'schools', 'set', 'game', ',', 'always', 'fully', 'staffed', 'population', '475/47/54', 'less', 'population', ',', 'still', 'results', 'however', ',', 'ca', \"n't\", 'seem', 'get', 'past', '72', 'educated', '200-300', 'person', 'city', 'schools', 'help', ',', 'need', 'something', 'different', '``', 'number', 'schools', \"''\", 'number', '...', 'modified', 'question', 'show', '8', 'schools', 'population', '475/47/54', 'children', 'educated', ',', 'someone', 'becomes', 'uneducated', 'adult', ',', 'remain', 'uneducated', 'die', 'eight', 'staffed', 'schools', 'able', 'educate', '160', 'students', 'simultaneously', ',', 'serve', 'youngsters', 'well', 'immediate', 'future', ',', 'older', 'workers', 'die', 'see', 'education', 'percentage', 'rise', \"'re\", 'dying', 'fast', 'enough', ',', 'send', 'mines', 'challenges', 'attaining', '100', 'education', 'rate', 'banished', '1', 'must', 'school', 'capacity', 'greater', 'maximum', 'number', 'children', 'fill', 'one', 'slips', ',', 'take', 'long', 'time', 'one', 'adult', 'die', 'attain', '100', '2', 'must', 'make', 'sure', 'never', 'let', 'teacher', 'die', 'without', 'replacement', 'students', 'enrolled', 'time', 'expelled', 'marked', 'educated', ',', 'teacher', 'teach', 'essentially', ',', 'capacity', 'decreases', '3', \"n't\", 'accept', 'nomads', 'nomads', 'uneducated', 'include', 'adults', ',', 'ca', \"n't\", 'educated', '4', 'time', \"'ll\", 'need', 'wait', 'nearly', 'complete', 'turnover', 'population', ',', 'least', 'last', 'uneducated', 'person', 'dies', 'since', \"'s\", 'difficult', 'identify', 'kill', 'uneducated', ',', 'foolproof', 'way', 'simply', 'wait']\n",
      "\n",
      "\n",
      "['get', '100', 'educ', '8', 'school', 'set', 'game', ',', 'alway', 'fulli', 'staf', 'popul', '475/47/54', 'less', 'popul', ',', 'still', 'result', 'howev', ',', 'ca', \"n't\", 'seem', 'get', 'past', '72', 'educ', '200-300', 'person', 'citi', 'school', 'help', ',', 'need', 'someth', 'differ', '``', 'number', 'school', \"''\", 'number', '...', 'modifi', 'question', 'show', '8', 'school', 'popul', '475/47/54', 'children', 'educ', ',', 'someon', 'becom', 'uneduc', 'adult', ',', 'remain', 'uneduc', 'die', 'eight', 'staf', 'school', 'abl', 'educ', '160', 'student', 'simultan', ',', 'serv', 'youngster', 'well', 'immedi', 'futur', ',', 'older', 'worker', 'die', 'see', 'educ', 'percentag', 'rise', 're', 'die', 'fast', 'enough', ',', 'send', 'mine', 'challeng', 'attain', '100', 'educ', 'rate', 'banish', '1', 'must', 'school', 'capac', 'greater', 'maximum', 'number', 'children', 'fill', 'one', 'slip', ',', 'take', 'long', 'time', 'one', 'adult', 'die', 'attain', '100', '2', 'must', 'make', 'sure', 'never', 'let', 'teacher', 'die', 'without', 'replac', 'student', 'enrol', 'time', 'expel', 'mark', 'educ', ',', 'teacher', 'teach', 'essenti', ',', 'capac', 'decreas', '3', \"n't\", 'accept', 'nomad', 'nomad', 'uneduc', 'includ', 'adult', ',', 'ca', \"n't\", 'educ', '4', 'time', 'll', 'need', 'wait', 'near', 'complet', 'turnov', 'popul', ',', 'least', 'last', 'uneduc', 'person', 'die', 'sinc', \"'s\", 'difficult', 'identifi', 'kill', 'uneduc', ',', 'foolproof', 'way', 'simpli', 'wait']\n",
      "\n",
      "\n",
      "*** in tf-idf\n",
      "['staf', 'replac', 'person', 'school', 'time', 'popul', 'must', 'identifi', 'student', 'citi', 'challeng', 'accept', 'seem', '200-300', 'teacher', 'howev', '475/47/54', 'ca', 'rise', 'slip', 'foolproof', '2', 'decreas', 'wait', 'someth', 'abl', 'serv', 'mine', 'essenti', 'never', 'teach', 'result', '3', 'futur', 'alway', 'becom', 'number', 'greater', 'sinc', 'immedi', 'modifi', 'fulli', 'without', 'show', 'expel', 'simpli', 'set', 'enrol', 'game', 'complet', 'mark', '72', \"''\", 'still', 'send', 'one', 'includ', 'kill', '100', 'maximum', 're', 'well', 'children', 'sure', 'long', 'youngster', 'make', 'get', 'fill', 'uneduc', '160', 'take', '8', 'adult', 'least', 'question', 'percentag', 'rate', 'see', 'near', 'turnov', 'older', 'capac', 'need', 'nomad', 'eight', 'worker', 'fast', 'difficult', 'attain', ',', 'past', 'someon', '...', 'let', 'help', 'die', 'last', 'differ', 'remain', 'll', 'way', '``', 'less', '4', '1', \"n't\", 'banish', \"'s\", 'enough', 'educ', 'simultan']\n",
      "['get', '100', 'educ', '8', 'school', 'set', 'game', ',', 'alway', 'fulli', 'staf', 'popul', '475/47/54', 'less', 'popul', ',', 'still', 'result', 'howev', ',', 'ca', \"n't\", 'seem', 'get', 'past', '72', 'educ', '200-300', 'person', 'citi', 'school', 'help', ',', 'need', 'someth', 'differ', '``', 'number', 'school', \"''\", 'number', '...', 'modifi', 'question', 'show', '8', 'school', 'popul', '475/47/54', 'children', 'educ', ',', 'someon', 'becom', 'uneduc', 'adult', ',', 'remain', 'uneduc', 'die', 'eight', 'staf', 'school', 'abl', 'educ', '160', 'student', 'simultan', ',', 'serv', 'youngster', 'well', 'immedi', 'futur', ',', 'older', 'worker', 'die', 'see', 'educ', 'percentag', 'rise', 're', 'die', 'fast', 'enough', ',', 'send', 'mine', 'challeng', 'attain', '100', 'educ', 'rate', 'banish', '1', 'must', 'school', 'capac', 'greater', 'maximum', 'number', 'children', 'fill', 'one', 'slip', ',', 'take', 'long', 'time', 'one', 'adult', 'die', 'attain', '100', '2', 'must', 'make', 'sure', 'never', 'let', 'teacher', 'die', 'without', 'replac', 'student', 'enrol', 'time', 'expel', 'mark', 'educ', ',', 'teacher', 'teach', 'essenti', ',', 'capac', 'decreas', '3', \"n't\", 'accept', 'nomad', 'nomad', 'uneduc', 'includ', 'adult', ',', 'ca', \"n't\", 'educ', '4', 'time', 'll', 'need', 'wait', 'near', 'complet', 'turnov', 'popul', ',', 'least', 'last', 'uneduc', 'person', 'die', 'sinc', \"'s\", 'difficult', 'identifi', 'kill', 'uneduc', ',', 'foolproof', 'way', 'simpli', 'wait']\n",
      "*** end tf-idf\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['100410376731'],\n",
       " [array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = extract_data_from_page(FOLDER_PATH+'threads/100410376731.html')\n",
    "print(json.dumps(data, indent=4, sort_keys=False))\n",
    "print('\\n')\n",
    "nlp_pipeline({data['thread_id']: data}, 'nltk_tokenization', 'tf_idf', True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 - Question 9 (1.5 points)\n",
    "\n",
    "*Implement the function rank that returns a list of thread ids sorted by thread and query similarity*. We will use the [cosine similarity function](https://en.wikipedia.org/wiki/Cosine_similarity) to compare two threads. In this assignment, query is a thread without answers and comments.\n",
    "\n",
    "**Remove the query in the sorted list (rank output)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def rank(query_id, all_thread_ids, X):\n",
    "    \"\"\"\n",
    "    Return a list of thread ids sorted by thread and query similarity. Cosine similarity is used to compare threads. \n",
    "    \n",
    "    query_id: thread id \n",
    "    all_thread_ids: list of thread ids\n",
    "    X: thread data representations\n",
    "    \n",
    "    return: ranked list of thread ids. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the similarity of thread representations (vectors) using cosine similarity function\n",
    "    # Sort the thread ids by the similarity\n",
    "    \n",
    "    query = all_thread_ids.index(query_id)\n",
    "    vector = np.array(X[query])\n",
    "    \n",
    "    cos = cosine_similarity(vector, X) #Should return size (1, len(X))\n",
    "    \n",
    "    return sorted(cos)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 - Evaluation\n",
    "\n",
    "We will test different configurations of our recommender system. These configurations are compared using the [mean average precision (MAP) metric](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision). Basically, the closer relevant threads are from ranked list begining, the higher MAP is. Additional materials to undertand MAP: [recall and precision over ranks](https://youtu.be/H7oAofuZjjE) and [MAP](https://youtu.be/pM6DJ0ZZee0).\n",
    "\n",
    "\n",
    "The function *eval* evaluates a specific configurantion of our recommender system\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def calculate_map(x):\n",
    "    res = 0.0\n",
    "    n = 0.0\n",
    "    for relevant_threads, ranked_list in x:\n",
    "        precisions = []\n",
    "        for k, thread_id in enumerate(ranked_list):\n",
    "            if thread_id in relevant_threads:\n",
    "                prec_at_k = (len(precisions) + 1)/(k+1)\n",
    "                precisions.append(prec_at_k)\n",
    "            if len(precisions) == len(relevant_threads):\n",
    "                break\n",
    "        res += mean(precisions)\n",
    "        n += 1\n",
    "    return res/n\n",
    "\n",
    "def eval(tokenization_type, vectorizer, enable_filter_tokens, enable_stemming):\n",
    "    all_thread_ids, X = nlp_pipeline(thread_index, tokenization_type, vectorizer, enable_filter_tokens, enable_stemming)\n",
    "    all_thread_ids = [int(t_id) for t_id in all_thread_ids]    \n",
    "    queries,relevant_threads = zip(*relevant_threads_by_query.items())\n",
    "    ranked_list = (rank(query_id, all_thread_ids, X) for query_id in queries)        \n",
    "    return calculate_map(zip(relevant_threads,ranked_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 - Question 10 (5 points)\n",
    "\n",
    "Evaluate our recommedation system performamnce(MAP) using each one of the following configurations:\n",
    "1. count(BoW) + space_tokenization (sans tokenizer)\n",
    "2. count(BoW) + nltk_tokenization\n",
    "3. count(BoW) + nltk_tokenization + Filtrer les tokens sans importance\n",
    "4. count(BoW) + nltk_tokenization + Filtrer les tokens sans importance + Stemming\n",
    "5. tf_idf + nltk_tokenization\n",
    "6. tf_idf + nltk_tokenization + Filtrer les tokens sans importance\n",
    "7. tf_idf + nltk_tokenization + Filtrer les tokens sans importance + Stemming \n",
    "\n",
    "Describe the results found by you and answer the following questions:\n",
    "- Was our recommendation system negatively or positively impacted by data preprocessing steps?\n",
    "- TF-IDF has achieved a better performance than CountBoW? If yes, why do you think that this has occurred? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
