{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "emtKQCNYDFDt"
   },
   "source": [
    "# INF8111 - Fouille de données\n",
    "\n",
    "\n",
    "## TP1 ÉTÉ 2020 - Système de recommandation\n",
    "\n",
    "##### Membres de l'équipe:\n",
    "\n",
    "    - Nom (Matricule) 1\n",
    "    - Nom (Matricule) 2\n",
    "    - Nom (Matricule) 3\n",
    "\n",
    "\n",
    "## Résumé\n",
    "\n",
    "*Stack Exchange* est un réseau de sites chacun contenant des discussions sur un sujet. Une discussion, ou *thread* en anglais, contient une question et potentiellement plusieurs réponses et commentaires. Dans ce TP, vous implémenterez un système de recommandations qui retourne les discussions en rapport à une question spécifique. Avant de soumettre une question, le site utilisera ce système de recommandations pour proposer les discussions les plus similaires afin de limiter le nombre de discussions dupliquées.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_XNyO01_DFDw"
   },
   "source": [
    "## 2 - Installation\n",
    "\n",
    "Pour ce TP, vous aurez besoin des librairies `numpy`, `sklearn` et `scipy` (que vous avez sans doute déjà), ainsi que la librairie `nltk`, qui est une libraire utilisée pour faire du traitement du language (Natural Language Processing, NLP)\n",
    "\n",
    "Installez les libraires en question et exécutez le code ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "12cNx_GiDFD0"
   },
   "outputs": [],
   "source": [
    "# Si vous le souhaitez, vous pouvez utiliser anaconda\n",
    "\n",
    "# pip install --user numpy\n",
    "# pip install --user sklearn\n",
    "# pip install --user scipy\n",
    "# pip install --user nltk\n",
    "\n",
    "\n",
    "#python\n",
    "#import nltk\n",
    "#nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JFEEtio4DFEG"
   },
   "source": [
    "## 3 - Jeu de données\n",
    "\n",
    "Téléchargez l'archive à l'adresse suivante: https://drive.google.com/file/d/1032N1oZkytHlHs20AXE9jPMBQCTyhb6H/view?usp=sharing\n",
    "\n",
    "L'archive contient:\n",
    "1. test.json: Ce fichier contient les nouvelles questions avec les discussions pertinentes associées.\n",
    "2. threads: Ce dossier contient le code HTML des discussions. Chaque fichier HTML est nommé selon le motif **thread_id.html**.\n",
    "\n",
    "L'image ci-dessous illustre un exemple de discussion :\n",
    "\n",
    "![thread_img](https://i.imgur.com/fjzBQss.png)\n",
    "\n",
    "- A : le sujet de la question\n",
    "- B : le corps de la question\n",
    "- C : les commentaires de la question\n",
    "- D : le corps de la réponse\n",
    "- E : les commentaires de la réponse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "16ifmWINDFEI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# define the folder path that contain the data\n",
    "# FOLDER_PATH = \"Define folder path that contain threads folder and test.json\"\n",
    "FOLDER_PATH = \"dataset/\"\n",
    "THREAD_FOLDER = os.path.join(FOLDER_PATH, 'threads')\n",
    "\n",
    "\n",
    "# Load the evaluation dataset\n",
    "import json\n",
    "\n",
    "\n",
    "test = json.load(open(os.path.join(FOLDER_PATH, \"test.json\")))\n",
    "relevant_threads_by_query = dict()\n",
    "\n",
    "\n",
    "for (query_id, cand_id, label) in test: \n",
    "    if label == 'Irrelevant':\n",
    "        continue\n",
    "        \n",
    "    l = relevant_threads_by_query.setdefault(query_id, [])\n",
    "    l.append(cand_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "euzefhShDFES"
   },
   "source": [
    "# 4 - Web scraping\n",
    "\n",
    "\"Le *web scraping* (parfois appelé harvesting) est une technique d'extraction du contenu de sites Web, via un script ou un programme, dans le but de le transformer pour permettre son utilisation dans un autre contexte, par exemple le référencement.\" [Wikipedia](https://fr.wikipedia.org/wiki/Web_scraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hOy5LnmLDFET"
   },
   "source": [
    "## 4.1 - Question 1 (0.5 point)\n",
    "\n",
    "Les caractères spéciaux et non-ASCII peuvent être encodés avec leur représentation HTML. Par exemple, l'apostrophe (') est encodée par **\\&AMP;**. L'encodage des pages web n’est pas uniforme, seuls les caractères spéciaux et non-ASCII sont encodés avec leur représentation HTML. Cela sera rectifié en convertissant les représentations HTML en caractère UTF-8. Par exemple, **\\&AMP;** sera remplacé par **'**.\n",
    "\n",
    "Implémentez la fonction fix_encoding qui convertit les représentations HTML en caractères UTF-8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8-2nMmBvDFEV"
   },
   "outputs": [],
   "source": [
    "def fix_encoding(text):\n",
    "    \"\"\"\n",
    "    Encodes the html entities in a text into UTF-8 encoding. For instance, \"I&amp;m ...\" => \"I'm ...\"\n",
    "    \n",
    "    :param text: string.\n",
    "    :return: fixed text(sting)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p0eUvmwjDFEd"
   },
   "source": [
    "## 4.2 - Question 2 (3 points)\n",
    "\n",
    "Implémentez la fonction extract_data_from_page. Cette fonction extrait le sujet de la question, son corps, ses commentaires, le corps des réponses et leurs commentaires des pages HTML. La fonction retourne un dictionnaire avec la structure suivante : *{\"thread_id\": int,\"question\":{\"subject\": string, \"body\": string, \"comments\": [string]}, answers: [{\"body\": string, \"comments\": [string]}]}*\n",
    "\n",
    "**Utilisez la fonction fix_encoding pour convertir le texte. Vouz pouvez utiliser la bibliothèque [Beatiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) pour cette question. Vous devez retirer toutes HTML du texte de la question, des commentaires et des réponses. **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SeTViZ5ODFEf"
   },
   "outputs": [],
   "source": [
    "def extract_data_from_page(pagepath):\n",
    "    \"\"\"\n",
    "    Scrap question, answer and comments from thread page.\n",
    "    \n",
    "    :param pagepath: the path of thread html file.\n",
    "    :return: \n",
    "        {\n",
    "            \"thread_id\": thread id,\n",
    "            \"question\":{\n",
    "                \"subject\": question subject text (Area A in the figure), \n",
    "                \"body\": question body text (Area B in the figure), \n",
    "                \"comments\": list of comment texts (Area C in the figure)\n",
    "                }, \n",
    "            \"answers\": [\n",
    "                {\n",
    "                    \"body\": answer body text (Area D in the figure),\n",
    "                    \"comments\": list of answer texts (Area E in the figure)\n",
    "                }\n",
    "                ]\n",
    "            }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the fix_encoding function to fix the text encoding\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8lPFvyu4DFEn"
   },
   "source": [
    "## 4.3 - Extraire le texte du code HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRmPmVhcDFEp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool, TimeoutError\n",
    "from time import time\n",
    "import json\n",
    "\n",
    "# Index each thread by its id\n",
    "index_path = os.path.join(THREAD_FOLDER, 'threads.json')\n",
    "\n",
    "if os.path.isfile(index_path):\n",
    "    # Load threads that webpage content were already extracted.\n",
    "    thread_index = json.load(open(index_path))\n",
    "else:\n",
    "    # Extract webpage content\n",
    "\n",
    "    # This can be slow (around 20 minutes). Test your code with a small sample. lxml parse is faster than html.parser\n",
    "    with Pool(processes=2) as pool:\n",
    "        files = [os.path.join(THREAD_FOLDER, filename) for filename in os.listdir(THREAD_FOLDER)]\n",
    "        threads = pool.map(extract_data_from_page, files)\n",
    "        thread_index = dict(((thread['thread_id'], thread) for thread in threads ))\n",
    "\n",
    "    # Save preprocessed threads\n",
    "    json.dump(thread_index, open(index_path,'w'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ta2zYC2_DFEy"
   },
   "source": [
    "## 5 - Prétraitement des données\n",
    "\n",
    "Le prétraitement des données est une tache cruciale en fouille de données. Cette étape nettoie et transforme les données brutes dans un format qui permet leur analyse, et leur utilisation avec des algorithmes de *machine learning*. En traitement des langages (natural language processing, NLP), la *tokenization* et le *stemming* sont des étapes cruciales. De plus, vous implémenterez une étape supplémentaire pour filtrer les mots sans importance.\n",
    "\n",
    "### 5.1 - Tokenization\n",
    "\n",
    "Cette étape permet de séparer un texte en séquence de *tokens* (= jetons, ici des mots, symboles ou ponctuation).\n",
    "\n",
    "Par exemple, la phrase *\"It's the student's notebook.\"* peut être séparé en liste de tokens de cette manière: [\"it\", \" 's\", \"the\", \"student\", \" 's\", \"notebook\", \".\"].\n",
    "\n",
    "**De plus, tous les tokenizers ont également le rôle de mettre le texte en minuscule.**\n",
    "\n",
    "#### 5.1.1 - Question 3 (0.5 point) \n",
    "\n",
    "Implémentez la fonction suivante :\n",
    "\n",
    "- **tokenize_space** qui tokenize le texte à partir des blancs (espace, tabulation, nouvelle ligne). Ce tokenizer est naïf.\n",
    "- **tokenize_nltk** qui utilise le tokenizer par défaut de la librairie nltk (https://www.nltk.org/api/nltk.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A8L-dWmTDFEz",
    "outputId": "871f4fb6-7125-4837-c1a8-7a48def32dee"
   },
   "outputs": [],
   "source": [
    "def tokenize_space(text):\n",
    "\n",
    "    \"\"\"\n",
    "    Tokenize the tokens that are separated by whitespace (space, tab, newline). \n",
    "    We consider that any tokenization was applied in the text when we use this tokenizer.\n",
    "    \n",
    "    For example: \"hello\\tworld of\\nNLP\" is split in ['hello', 'world', 'of', 'NLP']\n",
    "    \"\"\"\n",
    "    # return a list of tokens\n",
    "    raise NotImplementedError(\"\")\n",
    "        \n",
    "def tokenize_nltk(text):\n",
    "    \"\"\"\n",
    "    This tokenizer uses the default function of nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "    \"\"\"\n",
    "    # return a list of tokens\n",
    "    raise NotImplementedError(\"\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kca8UrrPDFE8"
   },
   "source": [
    "### 5.2 - Filtrer les tokens sans importance\n",
    "\n",
    "#### 5.2.1 - Question 4 (1 point)\n",
    "\n",
    "Certains tokens sont sans importance pour la comparaison, car ils apparaissent dans la majorité des discussions. Les supprimer réduit la dimension du vecteur et accélère les calculs.\n",
    "\n",
    "Expliquez quels tokens sont sans importances pour la comparaison des discussions. Implémentez la fonction filter_word qui retire ces mots de la liste des tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZpjbYOxHDFE9"
   },
   "outputs": [],
   "source": [
    "def filter_tokens(tokens):\n",
    "    raise NotImplementedError(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-VmSKeIWDFFE"
   },
   "source": [
    "### 5.3 - Stemming\n",
    "\n",
    "La racinisation (stemming) est un procédé de transformation des flexions en leur radical ou racine. Par example, en anglais, la racinisation de \"fishing\", \"fished\" and \"fish\" donne \"fish\" (stem). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jc6jI56cDFFF",
    "outputId": "5f03b1ba-247d-4442-a0d8-61be5304778f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['visitor', 'from', 'all', 'over', 'the', 'world', 'fish', 'dure', 'the', 'summer', '.']\n",
      "['i', 'was', 'fish']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "\n",
    "word1 = [\"Visitors\", \"from\", \"all\", \"over\", \"the\", \"world\", \"fishes\", \"during\", \"the\", \"summer\",\".\"]\n",
    "\n",
    "print([ stemmer.stem(w) for w in word1])\n",
    "\n",
    "word2 = ['I', 'was', 'fishing',]\n",
    "print([ stemmer.stem(w) for w in word2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9s-2nB8DFFN"
   },
   "source": [
    "#### 5.3.1 - Question 5 (1 point) \n",
    "\n",
    "*Expliquez comment et pourquoi le stemming est utile à note système de comparaison.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVh4-98CDFFP"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NSMlU0adDFFQ"
   },
   "source": [
    "# 6 - Data representation\n",
    "\n",
    "## 6.1 - Bag of Words\n",
    "\n",
    "De nombreux algorithmes demande des entrées qui soient toutes de la même taille, ce qui n'est forcément le cas pour des types de données comme les textes, qui peuvent avoir un nombre variable de mots.  \n",
    "\n",
    "Par exemple, considérons la phrase 1, ”Board games are much better than video games” et la phrase 2, ”Monopoly is an awesome game!”. La table ci-dessous montre un exemple d'un moyen de représentation de ces deux phrases en utilisant une représentation fixe : \n",
    "\n",
    "|<i></i>     | an | are | ! | Monopoly | awesome | better | games | than | video | much | board | is | game |\n",
    "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
    "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
    "| Sentence 2 | 1  | 0   | 1 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n",
    "\n",
    "Chaque colonne représente un mot du vocabulaire (de longueur 13), tandis que chaque ligne contient l'occurence des mots dans une phrase. Ainsi, la valeur 2 à la position (1,7) est due au fait que le mot *\"games\"* apparait deux fois dans la phrase 1. \n",
    "\n",
    "Ainsi, chaque ligne étant de longueur 13, on peut les utiliser comme vecteur pour représenter les phrases 1 et 2. Ainsi, c'est cette méthode que l'on appelle *Bag-of-Words* : c'est une représentation de documents par des vecteurs dont la dimension est égale à la taille du vocabulaire, et qui est construit en comptant le nombre d'occurence de chaque mot. Ainsi, chaque token est ici associé à une dimension.\n",
    "\n",
    "### 6.1.2 - Question 6 (2.5 points)\n",
    "\n",
    "\n",
    "*Implémentez le Bag-of-Words*\n",
    "\n",
    "Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn, hormis si vous avez des problèmes de mémoire, vous pouvez utiliser la classe sparse.csr_matrix de scipy (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OZYyMINoDFFR"
   },
   "outputs": [],
   "source": [
    "def transform_count_bow(X):\n",
    "    \"\"\"\n",
    "    This method preprocesses the data using the pipeline object, relates each token to a specific integer and  \n",
    "    transforms the text in a vector. Vectors are weighted using the token frequencies in the sentence.\n",
    "\n",
    "    X: document tokens. e.g: [['I','will', 'be', 'back', '.'], ['Helllo', 'world', '!'], ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']]\n",
    "\n",
    "    :return: vector representation of each document\n",
    "    \"\"\"   \n",
    "    \n",
    "    raise NotImplementedError(\"\") \n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p8C1YzuUDFFY"
   },
   "source": [
    "## 6.2 - TF-IDF\n",
    "\n",
    "L'utilisation de la fréquence d'apparition brute des mots, comme c'est le cas avec le bag-of-words, peut être problématique. En effet, peu de tokens auront une fréquence très élevée dans un document, et de ce fait, le poids de ces mots sera beaucoup plus grand que les autres, ce qui aura tendance à biaiser l'ensemble des poids. De plus, les mots qui apparaissent dans la plupart des documents n'aident pas à les discriminer. Par exemple, le mot \"*de*\" apparaît dans beaucoup de documents de la base de données, et pour autant, avoir ce mot en commun ne permet pas de conclure que des documents sont similaires. Au contraire, le mot \"*génial*\" est plus rare, mais les documents qui contiennent ce mot sont plus susceptibles d'être positif. TF-IDF est donc une méthode qui permet de pallier à ce problème.\n",
    "\n",
    "TF-IDF pondère le vecteur en utilisant une fréquence de document inverse (IDF) et une fréquence de termes (TF).\n",
    "\n",
    "TF est l'information locale sur l'importance qu'a un mot dans un document donné, tandis que IDF mesure la capacité de discrimination des mots dans un jeu de données. \n",
    "\n",
    "L'IDF d'un mot se calcule de la façon suivante:\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\text{idf}_i = \\log\\left( \\frac{N}{\\text{df}_i} \\right),\n",
    "\\end{equation}\n",
    "\n",
    "avec $N$ le nombre de documents dans la base de donnée, et $\\text{df}_i$ le nombre de documents qui contiennent le mot $i$.\n",
    "\n",
    "Le nouveau poids $w_{ij}$ d'un mot $i$ dans un document $j$ peut ensuite être calculé de la façon suivante:\n",
    "\n",
    "\\begin{equation}\n",
    "\tw_{ij} = \\text{tf}_{ij} \\times \\text{idf}_i,\n",
    "\\end{equation}\n",
    "\n",
    "avec $\\text{tf}_{ij}$ la fréquence du mot $i$ dans le document $j$.\n",
    "\n",
    "\n",
    "\n",
    "### 6.2.1 - Question 7 (3.5 points)\n",
    "\n",
    "Implémentez le bag-of-words avec la pondération de TF-IDF\n",
    "\n",
    "Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn, hormis si vous avez des problèmes de mémoire, vous pouvez utiliser la classe sparse.csr_matrix de scipy (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uz_HujS1DFFa"
   },
   "outputs": [],
   "source": [
    "def transform_tf_idf_bow(X):\n",
    "    \"\"\"\n",
    "    This method preprocesses the data using the pipeline object, calculates the IDF and TF and \n",
    "    transforms the text in vectors. Vectors are weighted using TF-IDF method.\n",
    "\n",
    "    X: document tokens. e.g: [['I','will', 'be', 'back', '.'], ['Helllo', 'world', '!'], ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']]\n",
    "\n",
    "    :return: vector representation of each document\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"\")\n",
    "    \n",
    "    return []\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zHvDn9D1DFFi"
   },
   "source": [
    "# 7 - Système de recommandations\n",
    "\n",
    "## 7.1 - Question 8 (1.5 points)\n",
    "\n",
    "\n",
    "La *pipeline* est la séquence d'étapes de prétraitement des données qui transforme les données brutes dans un format qui permet leur analyse. Pour le problème de système de recommandations, implémentez un pipeline composé des étapes suivantes :\n",
    "\n",
    "1. Concatène le texte de la question, des réponses et des commentaires pour chaque discussion $t$ dans le dictionnaire thread_dict.\n",
    "2. Tokenize le texte.\n",
    "3. Filtre les tokens sans importance.\n",
    "4. Stem les tokens\n",
    "5. Génère la représentation vectorielle avec transform_tf_idf_bow ou transform_count_bow.\n",
    "6. Retourne les identifiants des discussions et les représentations vectorielles des discussions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KNjgKgI7DFFj"
   },
   "outputs": [],
   "source": [
    "def nlp_pipeline(thread_dict, tokenization_type, vectorizer_type, enable_filter_tokens, enable_stemming):\n",
    "    \"\"\"\n",
    "    Preprocess and vectorize the threads.\n",
    "    \n",
    "    thread_dict: dictionary whose keys and values are thread ids and thread objects, respectively.\n",
    "    tokenization_type: two possible values \"space_tokenization\" and \"nltk_tokenization\".\n",
    "                            - space_tokenization: tokenize_space function is used to tokenize.\n",
    "                            - nltk_tokenization: tokenize_nltk function is used to tokenize.\n",
    "                            \n",
    "    vectorizer_type: two possible values \"count\" and \"tf_idf\".\n",
    "                            - count: use transform_count_bow to vectorize the text\n",
    "                            - tf_idf: use transform_tf_idf_bow to vectorize the text\n",
    "                            \n",
    "    enable_filter_tokens: enable the insignificant token removal;\n",
    "    \n",
    "    enable_stemming: enable stemming\n",
    "    \n",
    "    return: a list L with thread ids and matrix B that contains the vector of each thread. B[idx] is the fixed-length representation of L[idx].\n",
    "    \"\"\"\n",
    "    \n",
    "    raise NotImplementedError(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bK_rRY70DFFn"
   },
   "source": [
    "## 7.2 - Question 9 (1.5 points)\n",
    "\n",
    "Implémentez la fonction rank qui retourne la liste des identifiants des discussions triés par leur similarité avec la nouvelle question (query). Vous utiliserez la [cosine similarity function](https://en.wikipedia.org/wiki/Cosine_similarity) pour comparer deux discussions. Considérez la nouvelle question comme une discussion sans réponse ni commentaire.\n",
    "\n",
    "**Retirez la nouvelle question dans la liste de recommandations**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-_0Z40BCDFFp"
   },
   "outputs": [],
   "source": [
    "def rank(query_id, all_thread_ids, X):\n",
    "    \"\"\"\n",
    "    Return a list of thread ids sorted by thread and query similarity. Cosine similarity is used to compare threads. \n",
    "    \n",
    "    query_id: thread id \n",
    "    all_thread_ids: list of thread ids\n",
    "    X: thread data representations\n",
    "    \n",
    "    return: ranked list of thread ids. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the similarity of thread representations(vectors) using cosine similarity function\n",
    "    # Sort the thread ids by the similarity\n",
    "    \n",
    "    raise NotImplementedError(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BtGn9DWSDFFx"
   },
   "source": [
    "## 7.3 - Évaluation\n",
    "\n",
    "Vous allez tester différentes configurations du système de recommandations. Ces configurations seront comparées avec la [mean average precision (MAP) metric](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision). Plus les discussions pertinentes sont recommandées rapidement (c.-à-d. en haut de la liste), plus élevé sera le score MAP. \n",
    "\n",
    "Ressources supplémentaires pour comprendre MAP: [recall and precision over ranks](https://youtu.be/H7oAofuZjjE) et [MAP](https://youtu.be/pM6DJ0ZZee0).\n",
    "\n",
    "\n",
    "La fonction *eval* évalue une configuration spécifique du système de recommandations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5vIW_jz4DFFz"
   },
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "\n",
    "\n",
    "def calculate_map(x):\n",
    "    res = 0.0\n",
    "    n = 0.0\n",
    "    \n",
    "    \n",
    "    for relevant_threads, ranked_list in x:\n",
    "        precisions = []\n",
    "               \n",
    "        for k, thread_id in enumerate(ranked_list):\n",
    "            if thread_id in relevant_threads:\n",
    "                prec_at_k = (len(precisions) + 1)/(k+1)\n",
    "                precisions.append(prec_at_k)\n",
    "                \n",
    "            if len(precisions) == len(relevant_threads):\n",
    "                break\n",
    "        res += mean(precisions)\n",
    "        n += 1\n",
    "    \n",
    "    return res/n\n",
    "            \n",
    "\n",
    "def eval(tokenization_type, vectorizer, enable_filter_tokens, enable_stemming):\n",
    "    all_thread_ids, X = nlp_pipeline(thread_index, tokenization_type, vectorizer, enable_filter_tokens, enable_stemming)\n",
    "    all_thread_ids = [int(t_id) for t_id in all_thread_ids]    \n",
    "    queries,relevant_threads = zip(*relevant_threads_by_query.items())\n",
    "        \n",
    "    def generate_rank_list(query_id):\n",
    "        return rank(query_id, all_thread_ids, X)\n",
    "    \n",
    "    with Pool(processes=2) as pool:\n",
    "        ranked_list = pool.map(generate_rank_list, queries)\n",
    "        \n",
    "        \n",
    "    return calculate_map(zip(relevant_threads,ranked_list))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVm0pBJ1DFF4"
   },
   "source": [
    "## 7.4 - Question 10 (5 points)\n",
    "\n",
    "Évaluez la précision (MAP) du système de recommandations avec chacune des configurations suivantes :\n",
    "1. count(BoW) + space_tokenization (sans tokenizer)\n",
    "2. count(BoW) + nltk_tokenization\n",
    "3. count(BoW) + nltk_tokenization + Filtrer les tokens sans importance\n",
    "4. count(BoW) + nltk_tokenization + Filtrer les tokens sans importance + Stemming\n",
    "5. tf_idf + nltk_tokenization\n",
    "6. tf_idf + nltk_tokenization + Filtrer les tokens sans importance\n",
    "7. tf_idf + nltk_tokenization + Filtrer les tokens sans importance + Stemming \n",
    "\n",
    "Enfin, commentez vos résultats en répondant aux questions suivantes :\n",
    "- Notre système de recommandation a-t-il été influencé négativement ou positivement par les étapes de prétraitement des données ??\n",
    "- Est-ce que TF-IDF est plus performant que BoW? Si oui, à votre avis, pourquoi?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jtI70zJADFF5"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Laboratoire_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
