{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF8111 - Data Mining\n",
    "\n",
    "## TP1 SUMMER 2020 - recommendation system\n",
    "\n",
    "##### Team Members:\n",
    "\n",
    "    - Name (student id) 1\n",
    "    - Name (student id) 2\n",
    "    - Name (student id) 3\n",
    "\n",
    "\n",
    "## 1 - Overview\n",
    "\n",
    "Stack Exchange is a network of question-and-answer (Q&A) websites on topics in diverse fields, each site covering a specific topic. On Stack Exchange website, a thread is composed of a question and their answers and comments. In this assignment, *we will implement a recommendation system that returns threads (question + answers) that are related to a specific question*. Before submitting questions, the  website will use this engine to show the most similar threads to users in order to reduce the number of duplicate questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the code below to install the packages needed for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want, you can use anaconda and install after nltk library\n",
    "# pip install --user numpy\n",
    "# pip install --user sklearn\n",
    "# pip install --user scipy\n",
    "# pip install --user nltk\n",
    "\n",
    "\n",
    "#python\n",
    "#import nltk\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Data\n",
    "\n",
    "Please download the zip file in the following url: https://drive.google.com/file/d/1032N1oZkytHlHs20AXE9jPMBQCTyhb6H/view?usp=sharing\n",
    "\n",
    "In this zip file, there are:\n",
    "\n",
    "1. test.json: This file contains queries (new questions) and the relevant threads(question + answer) for each one these queries.\n",
    "2. threads: It is a folder that contains the thread html sources of threads. Each html file name follows the pattern **thread_id.html**.\n",
    "\n",
    "\n",
    "Figure below depicts an thread page example:\n",
    "\n",
    "![thread_img](https://i.imgur.com/fjzBQss.png)\n",
    "\n",
    "The figure contains 4 hilighted areas. Area A, B, C, D and E are the question subject, question body, question comments, answer body, and anwer comments, respectively. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# define the folder path that contain the data\n",
    "# FOLDER_PATH = \"Define folder path that contain threads folder and test.json\"\n",
    "FOLDER_PATH = \"dataset/\"\n",
    "THREAD_FOLDER = os.path.join(FOLDER_PATH, 'threads')\n",
    "\n",
    "\n",
    "# Load the evaluation dataset\n",
    "import json\n",
    "\n",
    "\n",
    "test = json.load(open(os.path.join(FOLDER_PATH, \"test.json\")))\n",
    "relevant_threads_by_query = dict()\n",
    "\n",
    "\n",
    "for (query_id, cand_id, label) in test: \n",
    "    if label == 'Irrelevant':\n",
    "        continue\n",
    "        \n",
    "    l = relevant_threads_by_query.setdefault(query_id, [])\n",
    "    l.append(cand_id)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Web scraping\n",
    "\n",
    "Web scraping consists in extracting relevant data from pages and prepare it for computational analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Question 1 (0.5 point)\n",
    "\n",
    "Special and non-ASCII characters can be encoded into their html representantion (html entities). For instance, apostrophe (') is encoded as **\\&amp;**. The webpage encoding in the data folder are incosistent. Only in a portion of the webpages, the **special** and **non-ASCII** characters were encoded into  html entities. We will fix this inconsistency by transforming the html entities into character representations, e.g., **\\&amp;** is represented as **'**.\n",
    "\n",
    "*Implement the function fix_encoding that encodes the html entities (special and non-ASCII characters) into their UTF-8 encoding.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fix_encoding(text):\n",
    "    \"\"\"\n",
    "    Encodes the html entities in a text into UTF-8 encoding. For instance, \"I&amp;m ...\" => \"I'm ...\"\n",
    "    \n",
    "    :param text: string.\n",
    "    :return: fixed text(sting)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Question 2 (3 points)\n",
    "\n",
    "Implement extract_data_from_page function. This function extracts question subject, question body, question comments, answer body, and anwer comments from the thread webpage. It returns a dictionary with the following structure: *{\"thread_id\": int,\"question\":{\"subject\": string, \"body\": string, \"comments\": [string]}, answers: [{\"body\": string, \"comments\": [string]}]}*\n",
    "\n",
    "**Use the fix_encoding function to fix the text encoding. You can use the library Beatiful Soap in this question. All html tags have to be removed from comment, question and answer textual data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_page(pagepath):\n",
    "    \"\"\"\n",
    "    Scrap question, answer and comments from thread page.\n",
    "    \n",
    "    :param pagepath: the path of thread html file.\n",
    "    :return: \n",
    "        {\n",
    "            \"thread_id\": thread id,\n",
    "            \"question\":{\n",
    "                \"subject\": question subject text (Area A in the figure), \n",
    "                \"body\": question body text (Area B in the figure), \n",
    "                \"comments\": list of comment texts (Area C in the figure)\n",
    "                }, \n",
    "            \"answers\": [\n",
    "                {\n",
    "                    \"body\": answer body text (Area D in the figure),\n",
    "                    \"comments\": list of answer texts (Area E in the figure)\n",
    "                }\n",
    "                ]\n",
    "            }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the fix_encoding function to fix the text encoding\n",
    "    raise NotImplementedError(\"\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Extract text from HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool, TimeoutError\n",
    "from time import time\n",
    "import json\n",
    "\n",
    "# Index each thread by its id\n",
    "index_path = os.path.join(THREAD_FOLDER, 'threads.json')\n",
    "\n",
    "if os.path.isfile(index_path):\n",
    "    # Load threads that webpage content were already extracted.\n",
    "    thread_index = json.load(open(index_path))\n",
    "else:\n",
    "    # Extract webpage content\n",
    "\n",
    "    # This can be slow (around 20 minutes). Test your code with a small sample. lxml parse is faster than html.parser\n",
    "    with Pool(processes=2) as pool:\n",
    "        files = [os.path.join(THREAD_FOLDER, filename) for filename in os.listdir(THREAD_FOLDER)]\n",
    "        threads = pool.map(extract_data_from_page, files)\n",
    "        thread_index = dict(((thread['thread_id'], thread) for thread in threads ))\n",
    "\n",
    "    # Save preprocessed threads\n",
    "    json.dump(thread_index, open(index_path,'w'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Data Preprocessing\n",
    "\n",
    "Preprocessing is a crucial task in data mining. This task clean and transform the raw data in a format that can better suit data analysis and machine learning techniques. In natural language processing (NLP), *tokenization* and *stemming* are two well known preprocessing steps. Besides these two steps, we will implement an additional step that is designed exclusively for the twitter domain.\n",
    "\n",
    "### 5.1 - Tokenization\n",
    "\n",
    "In this preprocessing step, a *tokenizer* is responsible for breaking a text in a sequence of tokens (words, symbols, and punctuations). \n",
    "\n",
    "For instance, the sentence *It's the student's notebook.* can be split into the following list of tokens: ['It', \"'s\", 'the', 'student', \"'s\", 'notebook', '.'].\n",
    "\n",
    "\n",
    "#### 5.1.1 - Question 3 (0.5 point) \n",
    "\n",
    "Implement the following functions: \n",
    "- **tokenize_space** tokenizes the tokens that are separated by whitespace (space, tab, newline). This is a naive tokenizer.\n",
    "- **tokenize_nltk** uses the default method of the nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "\n",
    "**All tokenizers have to lowercase the tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_space(object):\n",
    "    \"\"\"\n",
    "    Tokenize the tokens that are separated by whitespace (space, tab, newline). \n",
    "    We consider that any tokenization was applied in the text when we use this tokenizer.\n",
    "    \n",
    "    For example: \"hello\\tworld of\\nNLP\" is split in ['hello', 'world', 'of', 'NLP']\n",
    "    \"\"\"\n",
    "    # return a list of tokens\n",
    "    raise NotImplementedError(\"\")\n",
    "        \n",
    "def tokenize_nltk(object):\n",
    "    \"\"\"\n",
    "    This tokenizer uses the default function of nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "    \"\"\"\n",
    "    # return a list of tokens\n",
    "    raise NotImplementedError(\"\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - Filtering Insignificant Tokens\n",
    "\n",
    "#### 5.2.1 -  Question 4 (1 point)\n",
    "\n",
    "There are a set of tokens that are not signficant to the similarity comparison since they appear in many different threads pages. Thus, removing them decreases the vector dimensionality and turns the similarity calculation computationally cheaper. Describe the tokens that are insignificant for the thread similarity comparison? Moreover, implement the function filter_word that removes these words from a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_word():\n",
    "    raise NotImplementedError(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 - Stemming\n",
    "\n",
    "The process to convert words with the same stem (word reduction that keeps word prefixes) to a standard form is called *stemming*. For instance, \"fishing\", \"fished\" and \"fishes\" are transformed to the stem \"fish.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['visitor', 'from', 'all', 'over', 'the', 'world', 'fish', 'dure', 'the', 'summer', '.']\n",
      "['i', 'was', 'fish']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "\n",
    "word1 = [\"Visitors\", \"from\", \"all\", \"over\", \"the\", \"world\", \"fishes\", \"during\", \"the\", \"summer\",\".\"]\n",
    "\n",
    "print([ stemmer.stem(w) for w in word1])\n",
    "\n",
    "word2 = ['I', 'was', 'fishing',]\n",
    "print([ stemmer.stem(w) for w in word2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 - Question 5 (1 point) \n",
    "\n",
    "*Explain how stemming can benift our search engine?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Data representation\n",
    "\n",
    "## 6.1 - Bag of Words\n",
    "\n",
    "Many algorithms only accept inputs that have the same size. However, there are some data types whose sizes are not fixed, for instance, a text can have an unlimited number of words. Imagine that we retrieve two sentences: ”Board games are much better than video games” and ”Monoply is an awesome game!”. These sentences are respectively named as Sentence 1 and 2. Table below depicts how we could represent both sentences using a fixed representation.\n",
    "\n",
    "|            | an | are | ! | monopoly | awesome | better | games | than | video | much | board | is | game |\n",
    "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
    "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
    "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n",
    "\n",
    "Each column of this table 2.1 represents one of 13 vocabulary words, whereas the rows contains the word\n",
    "frequencies in each sentence. For instance, the cell in row 1 and column 7 has the value 2\n",
    "because the word games occurs twice in Sentence 1. Since the rows have always 13 values, we\n",
    "could use those vectors to represent the Sentences 1 and 2. The table above illustrates a technique called bag-of-words. Bag-of-words represents a document as a vector whose dimensions are equal to the number of times that vocabulary words appeared in the document. Thus, each token will be related to a dimension, i.e., an integer.\n",
    "\n",
    "### 6.1.2 - Question 6 (2.5 points)\n",
    "\n",
    "Implement the bag-of-words model that weights the vector with the absolute word frequency.\n",
    "\n",
    "**For this exercise, you cannot use any external python library (e.g., scikit-learn). However, if you have a problem with memory size, you can use the class scipy.sparse.csr_matrix (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_count_bow(X):\n",
    "    \"\"\"\n",
    "    This method preprocesses the data using the pipeline object, relates each token to a specific integer and  \n",
    "    transforms the text in a vector. Vectors are weighted using the token frequencies in the sentence.\n",
    "\n",
    "    X: document tokens. e.g: [['I','will', 'be', 'back', '.'], ['Helllo', 'world', '!'], ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']]\n",
    "\n",
    "    :return: vector representation of each document\n",
    "    \"\"\"   \n",
    "    \n",
    "    raise NotImplementedError(\"\") \n",
    "    return []  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 - TF-IDF\n",
    "\n",
    "Using raw frequency in the bag-of-words can be problematic. The word frequency distribution\n",
    "is skewed - only a few words have high frequencies in a document. Consequently, the\n",
    "weight of these words will be much bigger than the other ones which can give them more\n",
    "impact on some tasks, like similarity comparison. Besides that, a set of words (including\n",
    "those with high frequency) appears in most of the documents and, therefore, they do not\n",
    "help to discriminate documents. For instance, the word *of* appears in a significant\n",
    "part of documents. Thus, having the word *of* does not make\n",
    "documents more or less similar. However, the word *terrible* is rarer and documents that\n",
    "have this word are more likely to be negative. TF-IDF is a technique that overcomes the word frequency disadvantages.\n",
    "\n",
    "TF-IDF weights the vector using inverse document frequency (IDF) and word frequency, called term frequency (TF).\n",
    "TF is the local information about how important is a word to a specific document.  IDF measures the discrimination level of the words in a dataset.  Common words in a domain are not helpful to discriminate documents since most of them contain these terms. So, to reduce their relevance in the documents, these words should have low weights in the vectors . \n",
    "The following equation calculates the word IDF:\n",
    "\\begin{equation}\n",
    "\tidf_i = \\log\\left( \\frac{N}{df_i} \\right),\n",
    "\\end{equation}\n",
    "where $N$ is the number of documents in the dataset, $df_i$ is the number of documents that contain a word $i$.\n",
    "The new weight $w_{ij}$ of a word $i$ in a document $j$ using TF-IDF is computed as:\n",
    "\\begin{equation}\n",
    "\tw_{ij} = tf_{ij} \\times idf_i,\n",
    "\\end{equation}\n",
    "where $tf_{ij}$ is the term frequency of word $i$ in the document $j$.\n",
    "\n",
    "\n",
    "### 6.2.1 - Question 7 (3.5 points)\n",
    "\n",
    "Implement a bag-of-words model that weights the vector using TF-IDF.\n",
    "\n",
    "**For this exercise, you cannot use any external python library (e.g., scikit-learn). However, if you have a problem with memory size, you can use the class scipy.sparse.csr_matrix (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tf_idf_bow(X):\n",
    "    \"\"\"\n",
    "    This method preprocesses the data using the pipeline object, calculates the IDF and TF and \n",
    "    transforms the text in vectors. Vectors are weighted using TF-IDF method.\n",
    "\n",
    "    X: document tokens. e.g: [['I','will', 'be', 'back', '.'], ['Helllo', 'world', '!'], ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']]\n",
    "\n",
    "    :return: vector representation of each document\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"\")\n",
    "    \n",
    "    return []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Our Recommendation System\n",
    "\n",
    "## 7.1 - Question 8 (1.5 points)\n",
    "\n",
    "The pipeline is a sequence of preprocessing steps that transform the raw data to a format that is suitable for your problem. For our problem, you have to implement a pipeline composed of the following steps:\n",
    "\n",
    "1. Concatenate answer, question and comment texts of thread $t$ in the dictionary thread_dict.\n",
    "2. Tokenize the thread texts.\n",
    "3. Filter the insignificant tokens.\n",
    "4. Stem the tokens\n",
    "5. Generate the vector representation using TFIDFBoW or CountBoW\n",
    "6. Returns thread ids and thread vector representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_pipeline(thread_dict, tokenization_type, vectorizer_type, enable_filter_tokens, enable_stemming):\n",
    "    \"\"\"\n",
    "    Preprocess and vectorize the threads.\n",
    "    \n",
    "    thread_dict: dictionary whose keys and values are thread ids and thread objects, respectively.\n",
    "    tokenization_type: two possible values \"space_tokenization\" and \"nltk_tokenization\".\n",
    "                            - space_tokenization: tokenize_space function is used to tokenize.\n",
    "                            - nltk_tokenization: tokenize_nltk function is used to tokenize.\n",
    "                            \n",
    "    vectorizer_type: two possible values \"count\" and \"tf_idf\".\n",
    "                            - count: use transform_count_bow to vectorize the text\n",
    "                            - tf_idf: use transform_tf_idf_bow to vectorize the text\n",
    "                            \n",
    "    enable_filter_tokens: enable the insignificant token removal;\n",
    "    \n",
    "    enable_stemming: enable stemming\n",
    "    \n",
    "    return: a list L with thread ids and matrix B that contains the vector of each thread. B[idx] is the fixed-length representation of L[idx].\n",
    "    \"\"\"\n",
    "    \n",
    "    raise NotImplementedError(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 - Question 9 (1.5 points)\n",
    "\n",
    "*Implement the function rank that returns a list of thread ids sorted by thread and query similarity*. We will use the [cosine similarity function](https://en.wikipedia.org/wiki/Cosine_similarity) to compare two threads. In this assignment, query is a thread without answers and comments.\n",
    "\n",
    "**Remove the query in the sorted list (rank output)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(query_id, all_thread_ids, X):\n",
    "    \"\"\"\n",
    "    Return a list of thread ids sorted by thread and query similarity. Cosine similarity is used to compare threads. \n",
    "    \n",
    "    query_id: thread id \n",
    "    all_thread_ids: list of thread ids\n",
    "    X: thread data representations\n",
    "    \n",
    "    return: ranked list of thread ids. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the similarity of thread representations(vectors) using cosine similarity function\n",
    "    # Sort the thread ids by the similarity\n",
    "    \n",
    "    raise NotImplementedError(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 - Evaluation\n",
    "\n",
    "We will test different configurations of our recommender system. These configurations are compared using the [mean average precision (MAP) metric](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision). Basically, the closer relevant threads are from ranked list begining, the higher MAP is. Additional materials to undertand MAP: [recall and precision over ranks](https://youtu.be/H7oAofuZjjE) and [MAP](https://youtu.be/pM6DJ0ZZee0).\n",
    "\n",
    "\n",
    "The function *eval* evaluates a specific configurantion of our recommender system\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "\n",
    "\n",
    "def calculate_map(x):\n",
    "    res = 0.0\n",
    "    n = 0.0\n",
    "    \n",
    "    \n",
    "    for relevant_threads, ranked_list in x:\n",
    "        precisions = []\n",
    "               \n",
    "        for k, thread_id in enumerate(ranked_list):\n",
    "            if thread_id in relevant_threads:\n",
    "                prec_at_k = (len(precisions) + 1)/(k+1)\n",
    "                precisions.append(prec_at_k)\n",
    "                \n",
    "            if len(precisions) == len(relevant_threads):\n",
    "                break\n",
    "        res += mean(precisions)\n",
    "        n += 1\n",
    "    \n",
    "    return res/n\n",
    "            \n",
    "\n",
    "def eval(tokenization_type, vectorizer, enable_filter_tokens, enable_stemming):\n",
    "    all_thread_ids, X = nlp_pipeline(thread_index, tokenization_type, vectorizer, enable_filter_tokens, enable_stemming)\n",
    "    all_thread_ids = [int(t_id) for t_id in all_thread_ids]    \n",
    "    queries,relevant_threads = zip(*relevant_threads_by_query.items())\n",
    "        \n",
    "    def generate_rank_list(query_id):\n",
    "        return rank(query_id, all_thread_ids, X)\n",
    "    \n",
    "    with Pool(processes=2) as pool:\n",
    "        ranked_list = pool.map(generate_rank_list, queries)\n",
    "        \n",
    "        \n",
    "    return calculate_map(zip(relevant_threads,ranked_list))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 - Question 10 (5 points)\n",
    "\n",
    "Evaluate our recommedation system performamnce(MAP) using each one of the following configurations:\n",
    "1. count(BoW) + space_tokenization (sans tokenizer)\n",
    "2. count(BoW) + nltk_tokenization\n",
    "3. count(BoW) + nltk_tokenization + Filtrer les tokens sans importance\n",
    "4. count(BoW) + nltk_tokenization + Filtrer les tokens sans importance + Stemming\n",
    "5. tf_idf + nltk_tokenization\n",
    "6. tf_idf + nltk_tokenization + Filtrer les tokens sans importance\n",
    "7. tf_idf + nltk_tokenization + Filtrer les tokens sans importance + Stemming \n",
    "\n",
    "Describe the results found by you and answer the following questions:\n",
    "- Was our recommendation system negatively or positively impacted by data preprocessing steps?\n",
    "- TF-IDF has achieved a better performance than CountBoW? If yes, why do you think that this has occurred? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
