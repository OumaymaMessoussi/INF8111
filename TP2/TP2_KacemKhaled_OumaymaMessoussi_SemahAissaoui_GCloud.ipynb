{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP2_KacemKhaled_OumaymaMessoussi_SemahAissaoui_GCloud.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.14"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r02mfwtj8q14"
      },
      "source": [
        "# TP2 - Market Basket Analysis \n",
        "INF8111 - Fouille de données, Summer 2020\n",
        "### Team Components\n",
        "    - Kacem Khaled\n",
        "    - Oumayma Messoussi\n",
        "    - Semah Aissaoui\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hr5BXQys8q16"
      },
      "source": [
        "## Date et directives de remise\n",
        "Vous remettrez ce fichier nommé TP2\\_NomDuMembre1\\_NomDuMembre2\\_NomDuMembre3.ipynb dans la boîte de remise sur moodle. \n",
        "\n",
        "Tout devra être remis avant le **7 juin à 23h55**.\n",
        "\n",
        "## Market Basket Analysis\n",
        "\n",
        "Market Basket Analysis (MBA) is a data mining analytics technique to uncover associations between products or product grouping. By exploring interesting patterns from an extensive collection of data, MBA aims to understand/reveal customer purchase behaviors based upon the theory that if you purchased a certain set of products, then you are more (or less) likely to buy another group of products. In other words, MBA allows retailers to identify the relationship between the items that customers buy, revealing patterns of items often purchased together.\n",
        "\n",
        "A widely used approach to explore these patterns is by constructing ***association rules*** such as\n",
        "- **if** bought *ITEM_1* **then** will buy *ITEM_2* with **confidence** *X*.\n",
        "\n",
        "These associations do not have to be 1-to-1 rules. They can involve many items. For example, a person in a supermarket may add eggs to his/her cart, then an MBA application may suggest that the person will also buy some bread and/or flour: \n",
        "\n",
        "+ **if** bought *EGGS* **then** will buy [*BREAD* with confidence *0.2*; *FLOUR* with confidence 0.05].\n",
        "\n",
        "However, if the person now decides to add flour to his/her cart, the new association rule could be as showing below, suggesting ingredients to make a cake.\n",
        "\n",
        "+ **if** bought [*EGGS, FLOUR*] **then** will buy [*SUGGAR* with confidence 0.45; BAKING POWDER with confidence 0.12; *BREAD* with confidence *0.03*].\n",
        "\n",
        "There are many real scenarios where MBA plays a central role in data analysis, such as supermarket transactions, online orders or credit card history. Marketers may use these association rules to arrange correlated products closer to each other on store shelves or make online suggestions so that customers buy more items. Some questions that an MBA can usually help retailers to answer are:\n",
        "\n",
        "- What items are often purchased together?\n",
        "- Given a basket, what items should be suggested?\n",
        "- How should items be placed together on the shelves?\n",
        "\n",
        "### Objective\n",
        "\n",
        "Your goal in this TP is to develop an MBA algorithm for revealing patterns by creating association rules in a big dataset with more than three million supermarket transactions. However, mining association rules for large datasets is a very computationally intensive problem, which makes it almost impractical to perform it without a distributed system. Hence, to run your algorithm, you will have access to a distributed cloud computing cluster with hundreds of cores. \n",
        "\n",
        "To this end, a **MapReduce** algorithm will be implemented upon the [Apache Spark](http://spark.apache.org) framework, a fast cluster computing system. In a nutshell, Spark is an open source framework designed with a *scale-out* methodology which makes it a very powerful tool for programmers or application developers to perform a massive volume of computations and data processing in distributed environments. Spark provides high-level APIs that make it easy to build parallel apps without needing to worry about how your code and data are parallelized/distributed thought the computing cluster. Spark does it all for you.\n",
        "\n",
        "The implementation will follow the Market Basket Analysis algorithm presented by Jongwook Woo and Yuhang Xu (2012). The image **workflow.svg** Illustrates the algorithm's workflow, and is to be used for consultation throughout this TP. The blue boxes are the ones where you must implement a method to perform a map or reduce function, and the gray boxes represent their expected output. **All these operations are explained in detail in the following sections.** \n",
        "\n",
        "\n",
        "## 1. Setting up Spark\n",
        "\n",
        "Spark runs on both Windows and UNIX-like systems (e.g., Linux, Mac OS). It's easy to run locally on one machine — all you need is to have Java installed on your system PATH, or the JAVA_HOME environment variable pointing to a Java installation. It is mandatory that you have the **JDK v8** installed in your system, as Spark currently only support this version. If you haven't, go to [Java's web page](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html) to download and install a Java Virtual Machine. Remember to set the environment variable JAVA_HOME to use JDK v8 if your installation does not do it automatically for you. \n",
        "\n",
        "The interface between Python and Spark is done through **PySpark**, which can be installed by running `pip install pyspark` or set up following the sequence below:\n",
        "\n",
        "1. First, go to http://spark.apache.org/downloads \n",
        "2. Select the newest Spark release and the Pre-built for Apache Hadoop 2.7 package \n",
        "3. Click for download **spark-2.4.5-bin-hadoop2.7.tgz** and unzip it in any folder of your preference. \n",
        "4. Next, export the following variables to link PYSPARK (Spark's python interface) to your python distribution in your `~/.bash_profile` file.\n",
        "\n",
        "``\n",
        "export SPARK_HOME=/path/to/spark-2.4.5-bin-hadoop2.7\n",
        "export PYTHONPATH=\"$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$SPARK_HOME/python/lib/pyspark.zip:$PYTHONPATH\"\n",
        "export PYSPARK_PYTHON=/path/to/your/python3\n",
        "``\n",
        "\n",
        "5. Run `source ~./bash_profile` to effectuate the changes and restart this jupyter notebook session.\n",
        "\n",
        "#### Alternative for using Google Collab\n",
        "\n",
        "If you are planning on using Google Colaboratory platform, run the following code cell to set up Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xqBDLHXy8q17",
        "outputId": "e03f1fe3-0480-4c74-b7c0-38d5fdb8690e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "source": [
        "import os\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "!pip install pyspark"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/e4/5c15ab8d354c4e3528510821865e6748209a9b0ff6a1788f4cd36cc2a5dc/pyspark-2.4.6.tar.gz (218.4MB)\n",
            "\u001b[K     |████████████████████████████████| 218.4MB 54kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 33.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.6-py2.py3-none-any.whl size=218814406 sha256=10f62805e8b43367f749ec31eaff142b68706ef26156e1e7f260523ebc13d9be\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/5e/6a/17e906c94ec7246f260330a66e44a06a0809033ba2738a74a8\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y4ItP9Ga8q27"
      },
      "source": [
        "### 1.1 Products Counting Example \n",
        "\n",
        "To test your installation and start to get familiarized with Spark, we will follow an example that counts how many times the products of a toy dataset were purchased.\n",
        "\n",
        "The main entry point to start programming with Spark is the [RDD API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD), an excellent Spark abstraction to work with the MapReduce framework.  RDD is a collection of elements partitioned across the nodes of the cluster that can operate in parallel. In other words, RDD is how Spark keeps your data ready to perform some function (e.g., a map or reduce function) in parallel. **Do not worry if this still sounds confusing, it will be clear once you start implementing**. However, it is part of this TP to study/consult the [Spark python API](https://spark.apache.org/docs/latest/api/python/) and learn how to use it. Some useful functions that the RDD API offers are:\n",
        "\n",
        "1. **map**: return a new RDD by applying a function to each element of this RDD.\n",
        "2. **flatMap**: return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. **Should be used when each entry will yield more than one mapped element**\n",
        "3. **reduce**: reduces the elements of this RDD using the specified commutative and associative binary operator.\n",
        "4. **reduceByKey**: merge the values for each key using an associative and commutative reduce function\n",
        "5. **groupByKey**: group the values for each key in the RDD into a single sequence\n",
        "6. **collect**: return a list that contains all of the elements in this RDD. **Should not be used when working with a lot of data**\n",
        "7. **sample**: return a sampled subset of this RDD\n",
        "8. **count**: return the number of elements in this RDD.\n",
        "9. **filter**: return a new RDD containing only the elements that satisfy a predicate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_ozEKZvx8q28",
        "outputId": "b0258ae9-19e9-4184-dcff-03c909549c5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def map_to_product(row):\n",
        "    \"\"\"\n",
        "    Map each transaction into a set of KEY-VALUE elements.\n",
        "    The KEY is the word (product) itself and the VALUE is its number of apparitions.\n",
        "    \"\"\"\n",
        "    products = row.transaction.split(';') # split products from the column transaction\n",
        "    for p in products:\n",
        "        yield (p, 1)\n",
        "\n",
        "def reduce_product_by_key(value1, value2):\n",
        "    \"Reduce the mapped objects to unique words by merging (summing ) their values\"\n",
        "    return value1+value2\n",
        "\n",
        "# Initializates a object of SparkSession class, main entry point to Spark's funcionalites\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "        \n",
        "# Read a toy dataset\n",
        "toy = spark.read.csv('gs://bucket_tp2_inf8111_kacemoumaymasemah/toy.csv', header=True)\n",
        "print(\"Toy dataset\")\n",
        "toy.show()\n",
        "\n",
        "# Obtain a RDD object to call a map function\n",
        "toy_rdd = toy.rdd\n",
        "print(\"Toy dataframe as a RDD object (list of Row objects):\\n\\t\", toy_rdd.collect())\n",
        "\n",
        "# Map function to identify all products\n",
        "toy_rdd = toy_rdd.flatMap(map_to_product)\n",
        "print(\"\\nMapped products:\\n\\t\", toy_rdd.collect())\n",
        "\n",
        "# Reduce function to merge values of elements that share the same KEY\n",
        "toy_rdd = toy_rdd.reduceByKey(reduce_product_by_key)\n",
        "print(\"\\nReduced (merged) products:\\n\\t\", toy_rdd.collect())\n",
        "\n",
        "print(\"\\nVisualizing as a dataframe:\")\n",
        "toy_rdd.toDF([\"product\", \"count_product\"]).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Toy dataset\n",
            "+--------+-----------+\n",
            "|order_id|transaction|\n",
            "+--------+-----------+\n",
            "|       1|    a;b;c;f|\n",
            "|       2|    d;b;a;e|\n",
            "|       3|        c;b|\n",
            "|       4|        b;c|\n",
            "+--------+-----------+\n",
            "\n",
            "('Toy dataframe as a RDD object (list of Row objects):\\n\\t', [Row(order_id=u'1', transaction=u'a;b;c;f'), Row(order_id=u'2', transaction=u'd;b;a;e'), Row(order_id=u'3', transaction=u'c;b'), Row(order_id=u'4', transaction=u'b;c')])\n",
            "('\\nMapped products:\\n\\t', [(u'a', 1), (u'b', 1), (u'c', 1), (u'f', 1), (u'd', 1), (u'b', 1), (u'a', 1), (u'e', 1), (u'c', 1), (u'b', 1), (u'b', 1), (u'c', 1)])\n",
            "('\\nReduced (merged) products:\\n\\t', [(u'a', 2), (u'c', 3), (u'b', 4), (u'e', 1), (u'd', 1), (u'f', 1)])\n",
            "\n",
            "Visualizing as a dataframe:\n",
            "+-------+-------------+\n",
            "|product|count_product|\n",
            "+-------+-------------+\n",
            "|      a|            2|\n",
            "|      c|            3|\n",
            "|      b|            4|\n",
            "|      e|            1|\n",
            "|      d|            1|\n",
            "|      f|            1|\n",
            "+-------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UqH2nKRI8q3N"
      },
      "source": [
        "### 1.2 Working with Spark's Dataframe\n",
        "\n",
        "In the example above, we briefly used a Spark's Dataframe class, but only to obtain an RDD object with ```toy.rdd``` and to print the data as a structured table with the ```show()``` function. However, [Dataframe](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#) is a crucial part of the current Spark release and is built upon the RDD API. It is a distributed collection of rows under named columns, the same as a table in a relational database. Spark's Dataframe works similarly as [Pandas'](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html). In fact, we can export (obtain) a Spark's dataframe to (from) a pandas' data frame with the function ```toPandas()``` (```spark.createDataFrame```).\n",
        "\n",
        "A central functionality of the data frame is to profit from the [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html#sql), a module that allows SQL queries over structured data. For example, the same 'product counting example' could have been implemented as a sequence of SQL operations over the data:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h68P1Pkr8q3N",
        "outputId": "9c85233f-d049-42ce-e8b8-0a1a6cdb08ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "import pyspark.sql.functions as f\n",
        "\n",
        "# Creates a new column, products, with all products appering in each transaction\n",
        "print('New column \\'products\\': exploding the transaction\\'s products to a new row')\n",
        "df_toy = toy.withColumn('products', f.explode(f.split(toy.transaction, ';')))\n",
        "df_toy.show()\n",
        "\n",
        "# Performs a select query and group rows by the product name, aggreagating by counting\n",
        "print('Couting unique products:')\n",
        "df_toy.select(df_toy.products)\\\n",
        "      .groupBy(df_toy.products)\\\n",
        "      .agg(f.count('products').alias('count_product'))\\\n",
        "      .sort('count_product', ascending=False)\\\n",
        "      .show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New column 'products': exploding the transaction's products to a new row\n",
            "+--------+-----------+--------+\n",
            "|order_id|transaction|products|\n",
            "+--------+-----------+--------+\n",
            "|       1|    a;b;c;f|       a|\n",
            "|       1|    a;b;c;f|       b|\n",
            "|       1|    a;b;c;f|       c|\n",
            "|       1|    a;b;c;f|       f|\n",
            "|       2|    d;b;a;e|       d|\n",
            "|       2|    d;b;a;e|       b|\n",
            "|       2|    d;b;a;e|       a|\n",
            "|       2|    d;b;a;e|       e|\n",
            "|       3|        c;b|       c|\n",
            "|       3|        c;b|       b|\n",
            "|       4|        b;c|       b|\n",
            "|       4|        b;c|       c|\n",
            "+--------+-----------+--------+\n",
            "\n",
            "Couting unique products:\n",
            "+--------+-------------+\n",
            "|products|count_product|\n",
            "+--------+-------------+\n",
            "|       b|            4|\n",
            "|       c|            3|\n",
            "|       a|            2|\n",
            "|       d|            1|\n",
            "|       e|            1|\n",
            "|       f|            1|\n",
            "+--------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F_IU_rf18q3S"
      },
      "source": [
        "Also, the same SQL operations performed above could have been done with a traditional SQL language query as showing below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c68l__Eb8q3S",
        "outputId": "414b55c5-f2f1-4f60-fb53-bdc32c7572f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "# Creates a relational table TOY in the Spark session\n",
        "df_toy.createOrReplaceTempView(\"TOY\")\n",
        "\n",
        "spark.sql(\"SELECT t.products, COUNT(t.products) AS product_count\"\n",
        "          \" FROM TOY t\"\n",
        "          \" GROUP BY t.products\"\n",
        "          \" ORDER BY product_count DESC\").show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------------+\n",
            "|products|product_count|\n",
            "+--------+-------------+\n",
            "|       b|            4|\n",
            "|       c|            3|\n",
            "|       a|            2|\n",
            "|       f|            1|\n",
            "|       d|            1|\n",
            "|       e|            1|\n",
            "+--------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8Ck1hRFC8q3U"
      },
      "source": [
        "These SQL concepts are being mentioned here because they will be useful to us during the TP, mainly in Section 3, to manipulate the supermarket data, which is structured in tables. Thus, if you are not familiar with SQL, it is recommended that you follow a [tutorial](https://www.w3schools.com/sql/) to understand the basics.\n",
        "\n",
        "## 2. MBA Algorithm \n",
        "The following sections explain how you should develop each step of the MapReduce algorithm for our supermarket application. Figure workflow.png illustrates each step of the algorithm.\n",
        "\n",
        "### 2.1 Map to Patterns (10 points)\n",
        "For a given set of transactions (i.e., the rows of our toy dataset), each transaction must be **mapped** into a set of *purchase patterns* found within the transaction. Formally, these patterns are subsets of products that represent a group of items bought together. \n",
        "\n",
        "For the MapReduce framework, each pattern must be created as a *KEY-VALUE* element, where the KEY can take the form of a singleton, a pair or a trio of products that are present in the transaction. More precisely, for each transaction, the mapping function must generate all possible **UNIQUE** subsets of size **ONE, TWO or THREE**.  The VALUE associated with each KEY is the number of times that the KEY appeared in the transaction (if we assume that no product appears more than once in the transaction, this value is always equal to one). \n",
        "\n",
        "Now, implement the **map_to_patterns** function that receives a transaction (a row from the data frame) and returns the patterns found in the transaction. The mapped elements are a tuple (KEY, VALUE), where KEY is also a tuple of product names. It is crucial to notice that, since each entry (transaction) of the map function will **yield** more than one KEY-VALUE element, a *flatMap* must be invoked for this step.\n",
        "\n",
        "For the toy dataset, the expected output is similar to:\n",
        "\n",
        "\n",
        "<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 20em; padding-left:1px\">\n",
        "<code>\n",
        "+---------------+-----------+\n",
        "|       patterns|occurrences|\n",
        "+---------------+-----------+\n",
        "|         ('a',)|          1|\n",
        "|     ('a', 'b')|          1|\n",
        "|('a', 'b', 'c')|          1|\n",
        "|('a', 'b', 'f')|          1|\n",
        "|     ('a', 'c')|          1|\n",
        "|('a', 'c', 'f')|          1|\n",
        "|     ('a', 'f')|          1|\n",
        "|         ('b',)|          1|\n",
        "|     ('b', 'c')|          1|\n",
        "|('b', 'c', 'f')|          1|\n",
        "|     ('b', 'f')|          1|\n",
        "|         ('c',)|          1|\n",
        "|     ('c', 'f')|          1|\n",
        "|         ('f',)|          1|\n",
        "|         ('a',)|          1|\n",
        "|     ('a', 'b')|          1|\n",
        "|('a', 'b', 'd')|          1|\n",
        "|('a', 'b', 'e')|          1|\n",
        "|     ('a', 'd')|          1|\n",
        "|('a', 'd', 'e')|          1|\n",
        "|     ('a', 'e')|          1|\n",
        "|         ('b',)|          1|\n",
        "|     ('b', 'd')|          1|\n",
        "|('b', 'd', 'e')|          1|\n",
        "|     ('b', 'e')|          1|\n",
        "|         ('d',)|          1|\n",
        "|     ('d', 'e')|          1|\n",
        "|         ('e',)|          1|\n",
        "|         ('b',)|          1|\n",
        "|     ('b', 'c')|          1|\n",
        "|         ('c',)|          1|\n",
        "|         ('b',)|          1|\n",
        "|     ('b', 'c')|          1|\n",
        "|         ('c',)|          1|\n",
        "+---------------+-----------+\n",
        "</code>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XgcZFavR8q3U",
        "outputId": "3ee0e4a0-e5d3-499c-c3e9-e14481fd9007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        }
      },
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def format_tuples(pattern):\n",
        "    \"\"\"\n",
        "    Used for visualizition.\n",
        "    Transforms tuples to a string since Dataframe does not support column of tuples with different sizes\n",
        "    (a,b,c) -> '(a,b,c)'\n",
        "    \"\"\"\n",
        "    return (str(pattern[0]), str(pattern[1]))\n",
        "\n",
        "def map_to_patterns(row):\n",
        "    \n",
        "    to_yield = []\n",
        "    products = row.transaction.split(';')\n",
        "    for i in range(1, 4):\n",
        "        to_yield.extend(list(combinations(sorted(products), i)))\n",
        "    \n",
        "    for pattern in to_yield:\n",
        "        yield (pattern, 1)\n",
        "    \n",
        "\n",
        "toy_rdd = toy.rdd\n",
        "patterns_rdd = toy_rdd.flatMap(map_to_patterns)\n",
        "\n",
        "# Output as dataframe\n",
        "patterns_rdd.map(format_tuples).toDF(['patterns', 'occurrences']).show(150,truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+-----------+\n",
            "|patterns          |occurrences|\n",
            "+------------------+-----------+\n",
            "|(u'a',)           |1          |\n",
            "|(u'b',)           |1          |\n",
            "|(u'c',)           |1          |\n",
            "|(u'f',)           |1          |\n",
            "|(u'a', u'b')      |1          |\n",
            "|(u'a', u'c')      |1          |\n",
            "|(u'a', u'f')      |1          |\n",
            "|(u'b', u'c')      |1          |\n",
            "|(u'b', u'f')      |1          |\n",
            "|(u'c', u'f')      |1          |\n",
            "|(u'a', u'b', u'c')|1          |\n",
            "|(u'a', u'b', u'f')|1          |\n",
            "|(u'a', u'c', u'f')|1          |\n",
            "|(u'b', u'c', u'f')|1          |\n",
            "|(u'a',)           |1          |\n",
            "|(u'b',)           |1          |\n",
            "|(u'd',)           |1          |\n",
            "|(u'e',)           |1          |\n",
            "|(u'a', u'b')      |1          |\n",
            "|(u'a', u'd')      |1          |\n",
            "|(u'a', u'e')      |1          |\n",
            "|(u'b', u'd')      |1          |\n",
            "|(u'b', u'e')      |1          |\n",
            "|(u'd', u'e')      |1          |\n",
            "|(u'a', u'b', u'd')|1          |\n",
            "|(u'a', u'b', u'e')|1          |\n",
            "|(u'a', u'd', u'e')|1          |\n",
            "|(u'b', u'd', u'e')|1          |\n",
            "|(u'b',)           |1          |\n",
            "|(u'c',)           |1          |\n",
            "|(u'b', u'c')      |1          |\n",
            "|(u'b',)           |1          |\n",
            "|(u'c',)           |1          |\n",
            "|(u'b', u'c')      |1          |\n",
            "+------------------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "djN0Eb-68q3W"
      },
      "source": [
        "### 2.2 Reduce patterns (2.5 points)\n",
        "Once different CPUs processed the transactions, a **reduce** function must take place to combine identical KEYS (the subset of products) and compute the total number of its occurrences in the entire dataset. In other words, this reduce procedure must sum the *VALUE* of each identical KEY.\n",
        "\n",
        "Create a **reduce_patterns** function below that must sum the VALUE of each pattern.\n",
        "For the toy dataset, the expected output is:\n",
        "<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 28em; padding-left:5px\">\n",
        "<code>\n",
        "+---------------+--------------------+\n",
        "|       patterns|combined_occurrences|\n",
        "+---------------+--------------------+\n",
        "|         ('a',)|                   2|\n",
        "|     ('a', 'b')|                   2|\n",
        "|('a', 'b', 'c')|                   1|\n",
        "|('a', 'b', 'f')|                   1|\n",
        "|     ('a', 'c')|                   1|\n",
        "|('a', 'c', 'f')|                   1|\n",
        "|     ('a', 'f')|                   1|\n",
        "|         ('b',)|                   4|\n",
        "|     ('b', 'c')|                   3|\n",
        "|('b', 'c', 'f')|                   1|\n",
        "|     ('b', 'f')|                   1|\n",
        "|         ('c',)|                   3|\n",
        "|     ('c', 'f')|                   1|\n",
        "|         ('f',)|                   1|\n",
        "|('a', 'b', 'd')|                   1|\n",
        "|('a', 'b', 'e')|                   1|\n",
        "|     ('a', 'd')|                   1|\n",
        "|('a', 'd', 'e')|                   1|\n",
        "|     ('a', 'e')|                   1|\n",
        "|     ('b', 'd')|                   1|\n",
        "|('b', 'd', 'e')|                   1|\n",
        "|     ('b', 'e')|                   1|\n",
        "|         ('d',)|                   1|\n",
        "|     ('d', 'e')|                   1|\n",
        "|         ('e',)|                   1|\n",
        "+---------------+--------------------+\n",
        "</code>\n",
        "</pre>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VHA6boqv8q3W",
        "outputId": "138c776b-1089-4a0c-ca5e-b31693b13111",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "def reduce_patterns(value1, value2):\n",
        "    return value1+value2\n",
        "\n",
        "combined_patterns_rdd = patterns_rdd.reduceByKey(reduce_patterns)\n",
        "\n",
        "# Output as dataframe\n",
        "combined_patterns_rdd.map(format_tuples).toDF(['patterns', 'combined_occurrences']).show(150,truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+--------------------+\n",
            "|patterns          |combined_occurrences|\n",
            "+------------------+--------------------+\n",
            "|(u'b', u'c')      |3                   |\n",
            "|(u'b',)           |4                   |\n",
            "|(u'd', u'e')      |1                   |\n",
            "|(u'd',)           |1                   |\n",
            "|(u'a', u'd')      |1                   |\n",
            "|(u'f',)           |1                   |\n",
            "|(u'a', u'b', u'c')|1                   |\n",
            "|(u'a', u'b')      |2                   |\n",
            "|(u'b', u'd')      |1                   |\n",
            "|(u'c', u'f')      |1                   |\n",
            "|(u'b', u'd', u'e')|1                   |\n",
            "|(u'b', u'c', u'f')|1                   |\n",
            "|(u'b', u'e')      |1                   |\n",
            "|(u'a', u'b', u'f')|1                   |\n",
            "|(u'a',)           |2                   |\n",
            "|(u'a', u'b', u'e')|1                   |\n",
            "|(u'c',)           |3                   |\n",
            "|(u'a', u'b', u'd')|1                   |\n",
            "|(u'a', u'f')      |1                   |\n",
            "|(u'e',)           |1                   |\n",
            "|(u'b', u'f')      |1                   |\n",
            "|(u'a', u'd', u'e')|1                   |\n",
            "|(u'a', u'c', u'f')|1                   |\n",
            "|(u'a', u'c')      |1                   |\n",
            "|(u'a', u'e')      |1                   |\n",
            "+------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PvHh4-RR8q3Y"
      },
      "source": [
        "### 2.3 Map to subpatterns (15 points)\n",
        "Next, another **map** function should be applied to generate subpatterns. Once again, the subpatterns are KEY-VALUE elements, where the KEY is a subset of products as well. However, creating the subpattern's KEY is a different procedure. This time, the idea is to break down the list of products of each pattern (pattern KEY), remove one product at a time, and yield the resulting list as the new subpattern KEY. \n",
        "\n",
        "For example, for a given pattern $P$ with three products, $p_1, p_2 $ and $p_3$, three new subpatterns KEYs are going to be created: (i) remove $p_1$ and yield ($p_2, p_3$); (ii) remove $p_2$ and yield ($p_1,p_3$); and (iii) remove $p_3$ and yield ($p_1,p_2$). \n",
        "\n",
        "Additionally, the subpattern's VALUE structure will also be different. Instead of just single integer value as we had in the patterns, this time a *tuple* should be created for the subpattern VALUE. This tuple contains the product that was removed when yielding the KEY and the number of times the pattern appeared. For example above, the values should be ($p_1,v$), ($p_2,v$) and ($p_3,v$), respectively, where $v$ is the VALUE of the pattern. \n",
        "\n",
        "The idea behind subpatterns is to create **rules** such as: when the products of KEY were bought, the item present in the VALUE was also bought *v* times. Furthermore, each pattern should also yield a subpattern where the KEY is the same list of products of the pattern, but the VALUE is a tuple with a null product (None) and the number of times the pattern appeared. This element will be useful to keep track of how many times such a pattern was found and later will be used to compute the confidence value when generating the association rules. \n",
        "\n",
        "Now, implement the  **map_to_subpatterns** function that receives a pattern and yields all found subpatterns. Once again, each entry (pattern) will generate more than one KEY-VALUE element, then a flatMap function must be called.\n",
        "\n",
        "For the toy dataset, the expected output is:\n",
        "\n",
        "<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 20em; padding-left:5px\">\n",
        "<code>\n",
        "+---------------+---------+\n",
        "|    subpatterns|    rules|\n",
        "+---------------+---------+\n",
        "|         ('a',)|(None, 2)|\n",
        "|     ('a', 'b')|(None, 2)|\n",
        "|         ('b',)| ('a', 2)|\n",
        "|         ('a',)| ('b', 2)|\n",
        "|('a', 'b', 'c')|(None, 1)|\n",
        "|     ('b', 'c')| ('a', 1)|\n",
        "|     ('a', 'c')| ('b', 1)|\n",
        "|     ('a', 'b')| ('c', 1)|\n",
        "|('a', 'b', 'f')|(None, 1)|\n",
        "|     ('b', 'f')| ('a', 1)|\n",
        "|     ('a', 'f')| ('b', 1)|\n",
        "|     ('a', 'b')| ('f', 1)|\n",
        "|     ('a', 'c')|(None, 1)|\n",
        "|         ('c',)| ('a', 1)|\n",
        "|         ('a',)| ('c', 1)|\n",
        "|('a', 'c', 'f')|(None, 1)|\n",
        "|     ('c', 'f')| ('a', 1)|\n",
        "|     ('a', 'f')| ('c', 1)|\n",
        "|     ('a', 'c')| ('f', 1)|\n",
        "|     ('a', 'f')|(None, 1)|\n",
        "|         ('f',)| ('a', 1)|\n",
        "|         ('a',)| ('f', 1)|\n",
        "|         ('b',)|(None, 4)|\n",
        "|     ('b', 'c')|(None, 3)|\n",
        "|         ('c',)| ('b', 3)|\n",
        "|         ('b',)| ('c', 3)|\n",
        "|('b', 'c', 'f')|(None, 1)|\n",
        "|     ('c', 'f')| ('b', 1)|\n",
        "|     ('b', 'f')| ('c', 1)|\n",
        "|     ('b', 'c')| ('f', 1)|\n",
        "|     ('b', 'f')|(None, 1)|\n",
        "|         ('f',)| ('b', 1)|\n",
        "|         ('b',)| ('f', 1)|\n",
        "|         ('c',)|(None, 3)|\n",
        "|     ('c', 'f')|(None, 1)|\n",
        "|         ('f',)| ('c', 1)|\n",
        "|         ('c',)| ('f', 1)|\n",
        "|         ('f',)|(None, 1)|\n",
        "|('a', 'b', 'd')|(None, 1)|\n",
        "|     ('b', 'd')| ('a', 1)|\n",
        "|     ('a', 'd')| ('b', 1)|\n",
        "|     ('a', 'b')| ('d', 1)|\n",
        "|('a', 'b', 'e')|(None, 1)|\n",
        "|     ('b', 'e')| ('a', 1)|\n",
        "|     ('a', 'e')| ('b', 1)|\n",
        "|     ('a', 'b')| ('e', 1)|\n",
        "|     ('a', 'd')|(None, 1)|\n",
        "|         ('d',)| ('a', 1)|\n",
        "|         ('a',)| ('d', 1)|\n",
        "|('a', 'd', 'e')|(None, 1)|\n",
        "|     ('d', 'e')| ('a', 1)|\n",
        "|     ('a', 'e')| ('d', 1)|\n",
        "|     ('a', 'd')| ('e', 1)|\n",
        "|     ('a', 'e')|(None, 1)|\n",
        "|         ('e',)| ('a', 1)|\n",
        "|         ('a',)| ('e', 1)|\n",
        "|     ('b', 'd')|(None, 1)|\n",
        "|         ('d',)| ('b', 1)|\n",
        "|         ('b',)| ('d', 1)|\n",
        "|('b', 'd', 'e')|(None, 1)|\n",
        "|     ('d', 'e')| ('b', 1)|\n",
        "|     ('b', 'e')| ('d', 1)|\n",
        "|     ('b', 'd')| ('e', 1)|\n",
        "|     ('b', 'e')|(None, 1)|\n",
        "|         ('e',)| ('b', 1)|\n",
        "|         ('b',)| ('e', 1)|\n",
        "|         ('d',)|(None, 1)|\n",
        "|     ('d', 'e')|(None, 1)|\n",
        "|         ('e',)| ('d', 1)|\n",
        "|         ('d',)| ('e', 1)|\n",
        "|         ('e',)|(None, 1)|\n",
        "+---------------+---------+\n",
        "</code>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iAzW7ExI8q3Y",
        "outputId": "03877cb7-b66f-40f6-ee39-4776c0fb17d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from copy import deepcopy\n",
        "def map_to_subpatterns(pattern):\n",
        "    key = pattern[0]\n",
        "    value = pattern[1]\n",
        "\n",
        "    yield (key, tuple((None, value)))\n",
        "    if len(key) != 1:\n",
        "        tmp = deepcopy(key)\n",
        "        for i, elem in enumerate(tmp):\n",
        "            sub = tmp[0:i]+tmp[i+1:]\n",
        "            yield (sub, tuple((elem, value)))\n",
        "\n",
        "subpatterns_rdd = combined_patterns_rdd.flatMap(map_to_subpatterns)\n",
        "\n",
        "# Output as dataframe\n",
        "subpatterns_rdd.map(format_tuples).toDF(['subpatterns', 'rules']).show(150, truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+---------+\n",
            "|subpatterns       |rules    |\n",
            "+------------------+---------+\n",
            "|(u'b', u'c')      |(None, 3)|\n",
            "|(u'c',)           |(u'b', 3)|\n",
            "|(u'b',)           |(u'c', 3)|\n",
            "|(u'b',)           |(None, 4)|\n",
            "|(u'd', u'e')      |(None, 1)|\n",
            "|(u'e',)           |(u'd', 1)|\n",
            "|(u'd',)           |(u'e', 1)|\n",
            "|(u'd',)           |(None, 1)|\n",
            "|(u'a', u'd')      |(None, 1)|\n",
            "|(u'd',)           |(u'a', 1)|\n",
            "|(u'a',)           |(u'd', 1)|\n",
            "|(u'f',)           |(None, 1)|\n",
            "|(u'a', u'b', u'c')|(None, 1)|\n",
            "|(u'b', u'c')      |(u'a', 1)|\n",
            "|(u'a', u'c')      |(u'b', 1)|\n",
            "|(u'a', u'b')      |(u'c', 1)|\n",
            "|(u'a', u'b')      |(None, 2)|\n",
            "|(u'b',)           |(u'a', 2)|\n",
            "|(u'a',)           |(u'b', 2)|\n",
            "|(u'b', u'd')      |(None, 1)|\n",
            "|(u'd',)           |(u'b', 1)|\n",
            "|(u'b',)           |(u'd', 1)|\n",
            "|(u'c', u'f')      |(None, 1)|\n",
            "|(u'f',)           |(u'c', 1)|\n",
            "|(u'c',)           |(u'f', 1)|\n",
            "|(u'b', u'd', u'e')|(None, 1)|\n",
            "|(u'd', u'e')      |(u'b', 1)|\n",
            "|(u'b', u'e')      |(u'd', 1)|\n",
            "|(u'b', u'd')      |(u'e', 1)|\n",
            "|(u'b', u'c', u'f')|(None, 1)|\n",
            "|(u'c', u'f')      |(u'b', 1)|\n",
            "|(u'b', u'f')      |(u'c', 1)|\n",
            "|(u'b', u'c')      |(u'f', 1)|\n",
            "|(u'b', u'e')      |(None, 1)|\n",
            "|(u'e',)           |(u'b', 1)|\n",
            "|(u'b',)           |(u'e', 1)|\n",
            "|(u'a', u'b', u'f')|(None, 1)|\n",
            "|(u'b', u'f')      |(u'a', 1)|\n",
            "|(u'a', u'f')      |(u'b', 1)|\n",
            "|(u'a', u'b')      |(u'f', 1)|\n",
            "|(u'a',)           |(None, 2)|\n",
            "|(u'a', u'b', u'e')|(None, 1)|\n",
            "|(u'b', u'e')      |(u'a', 1)|\n",
            "|(u'a', u'e')      |(u'b', 1)|\n",
            "|(u'a', u'b')      |(u'e', 1)|\n",
            "|(u'c',)           |(None, 3)|\n",
            "|(u'a', u'b', u'd')|(None, 1)|\n",
            "|(u'b', u'd')      |(u'a', 1)|\n",
            "|(u'a', u'd')      |(u'b', 1)|\n",
            "|(u'a', u'b')      |(u'd', 1)|\n",
            "|(u'a', u'f')      |(None, 1)|\n",
            "|(u'f',)           |(u'a', 1)|\n",
            "|(u'a',)           |(u'f', 1)|\n",
            "|(u'e',)           |(None, 1)|\n",
            "|(u'b', u'f')      |(None, 1)|\n",
            "|(u'f',)           |(u'b', 1)|\n",
            "|(u'b',)           |(u'f', 1)|\n",
            "|(u'a', u'd', u'e')|(None, 1)|\n",
            "|(u'd', u'e')      |(u'a', 1)|\n",
            "|(u'a', u'e')      |(u'd', 1)|\n",
            "|(u'a', u'd')      |(u'e', 1)|\n",
            "|(u'a', u'c', u'f')|(None, 1)|\n",
            "|(u'c', u'f')      |(u'a', 1)|\n",
            "|(u'a', u'f')      |(u'c', 1)|\n",
            "|(u'a', u'c')      |(u'f', 1)|\n",
            "|(u'a', u'c')      |(None, 1)|\n",
            "|(u'c',)           |(u'a', 1)|\n",
            "|(u'a',)           |(u'c', 1)|\n",
            "|(u'a', u'e')      |(None, 1)|\n",
            "|(u'e',)           |(u'a', 1)|\n",
            "|(u'a',)           |(u'e', 1)|\n",
            "+------------------+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HtiAJVMi8q3a"
      },
      "source": [
        "### 2.4 Reduce Subpatterns (2.5 points)\n",
        "Once again, a **reduce** function will be required to group all the subpatterns by their KEY. The objective of this reducing procedure is to create a list of all **rules** that appeared in KEY. Hence, the expected output resulting from this reduce function is also a KEY-VALUE element, where the KEY is the subpattern's KEY, and the VALUE is a group containing all the VALUEs of the subpatterns that share the same KEY.\n",
        "\n",
        "For the toy dataset, the expected output is:\n",
        "\n",
        "\n",
        "<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 50em; padding-left:5px\">\n",
        "<code>\n",
        "+---------------+-------------------------------------------------------------+\n",
        "|subpatterns    |combined_rules                                               |\n",
        "+---------------+-------------------------------------------------------------+\n",
        "|('a',)         |[(None, 2), ('b', 2), ('c', 1), ('f', 1), ('d', 1), ('e', 1)]|\n",
        "|('a', 'b')     |[(None, 2), ('c', 1), ('f', 1), ('d', 1), ('e', 1)]          |\n",
        "|('b',)         |[('a', 2), (None, 4), ('c', 3), ('f', 1), ('d', 1), ('e', 1)]|\n",
        "|('a', 'b', 'c')|[(None, 1)]                                                  |\n",
        "|('b', 'c')     |[('a', 1), (None, 3), ('f', 1)]                              |\n",
        "|('a', 'c')     |[('b', 1), (None, 1), ('f', 1)]                              |\n",
        "|('a', 'b', 'f')|[(None, 1)]                                                  |\n",
        "|('b', 'f')     |[('a', 1), ('c', 1), (None, 1)]                              |\n",
        "|('a', 'f')     |[('b', 1), ('c', 1), (None, 1)]                              |\n",
        "|('c',)         |[('a', 1), ('b', 3), (None, 3), ('f', 1)]                    |\n",
        "|('a', 'c', 'f')|[(None, 1)]                                                  |\n",
        "|('c', 'f')     |[('a', 1), ('b', 1), (None, 1)]                              |\n",
        "|('f',)         |[('a', 1), ('b', 1), ('c', 1), (None, 1)]                    |\n",
        "|('b', 'c', 'f')|[(None, 1)]                                                  |\n",
        "|('a', 'b', 'd')|[(None, 1)]                                                  |\n",
        "|('b', 'd')     |[('a', 1), (None, 1), ('e', 1)]                              |\n",
        "|('a', 'd')     |[('b', 1), (None, 1), ('e', 1)]                              |\n",
        "|('a', 'b', 'e')|[(None, 1)]                                                  |\n",
        "|('b', 'e')     |[('a', 1), ('d', 1), (None, 1)]                              |\n",
        "|('a', 'e')     |[('b', 1), ('d', 1), (None, 1)]                              |\n",
        "+---------------+-------------------------------------------------------------+\n",
        "</code>\n",
        "</pre>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2rI4Q9Ok8q3a",
        "outputId": "96c4a25b-183b-49de-ff74-ab16c509babd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "def list_map(pattern):\n",
        "    return (pattern[0], list(pattern[1]))\n",
        "\n",
        "combined_rules = subpatterns_rdd.groupByKey().map(list_map)\n",
        "\n",
        "# Output as dataframe\n",
        "combined_rules.map(format_tuples).toDF(['subpatterns', 'combined_rules']).show(150, truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+------------------------------------------------------------------+\n",
            "|subpatterns       |combined_rules                                                    |\n",
            "+------------------+------------------------------------------------------------------+\n",
            "|(u'b', u'c')      |[(None, 3), (u'a', 1), (u'f', 1)]                                 |\n",
            "|(u'b',)           |[(u'c', 3), (None, 4), (u'a', 2), (u'd', 1), (u'e', 1), (u'f', 1)]|\n",
            "|(u'd', u'e')      |[(None, 1), (u'b', 1), (u'a', 1)]                                 |\n",
            "|(u'd',)           |[(u'e', 1), (None, 1), (u'a', 1), (u'b', 1)]                      |\n",
            "|(u'a', u'd')      |[(None, 1), (u'b', 1), (u'e', 1)]                                 |\n",
            "|(u'f',)           |[(None, 1), (u'c', 1), (u'a', 1), (u'b', 1)]                      |\n",
            "|(u'a', u'b', u'c')|[(None, 1)]                                                       |\n",
            "|(u'a', u'b')      |[(u'c', 1), (None, 2), (u'f', 1), (u'e', 1), (u'd', 1)]           |\n",
            "|(u'b', u'd')      |[(None, 1), (u'e', 1), (u'a', 1)]                                 |\n",
            "|(u'c', u'f')      |[(None, 1), (u'b', 1), (u'a', 1)]                                 |\n",
            "|(u'b', u'd', u'e')|[(None, 1)]                                                       |\n",
            "|(u'b', u'c', u'f')|[(None, 1)]                                                       |\n",
            "|(u'b', u'e')      |[(u'd', 1), (None, 1), (u'a', 1)]                                 |\n",
            "|(u'a', u'b', u'f')|[(None, 1)]                                                       |\n",
            "|(u'a',)           |[(u'd', 1), (u'b', 2), (None, 2), (u'f', 1), (u'c', 1), (u'e', 1)]|\n",
            "|(u'a', u'b', u'e')|[(None, 1)]                                                       |\n",
            "|(u'c',)           |[(u'b', 3), (u'f', 1), (None, 3), (u'a', 1)]                      |\n",
            "|(u'a', u'b', u'd')|[(None, 1)]                                                       |\n",
            "|(u'a', u'f')      |[(u'b', 1), (None, 1), (u'c', 1)]                                 |\n",
            "|(u'e',)           |[(u'd', 1), (u'b', 1), (None, 1), (u'a', 1)]                      |\n",
            "|(u'b', u'f')      |[(u'c', 1), (u'a', 1), (None, 1)]                                 |\n",
            "|(u'a', u'd', u'e')|[(None, 1)]                                                       |\n",
            "|(u'a', u'c', u'f')|[(None, 1)]                                                       |\n",
            "|(u'a', u'c')      |[(u'b', 1), (u'f', 1), (None, 1)]                                 |\n",
            "|(u'a', u'e')      |[(u'b', 1), (u'd', 1), (None, 1)]                                 |\n",
            "+------------------+------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VUSqO2F78q3c"
      },
      "source": [
        "## 2.5. Map to Association Rules (15 points)\n",
        "Finally, the last step of the algorithm is to create the association rules to perform the market basket analysis. The goal of this map function is to calculate the **confidence** level of buying a product, knowing that there is already a set of products in the basket. Thus, the KEY of the subpattern is the set of products placed in the basket and, for each product present in the list of rules, i.e., in the VALUE, the confidence can be calculated as:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\text{number of times the product was bought together with KEY }}{\\text{number of times the KEY appeared}}\n",
        "\\end{align*}\n",
        "\n",
        "For the example given in the Figure workflow, *coffee* was bought 20 times and, in 17 of them, *milk* was bought together. Then, the confidence level of buying *milk* knowing that *coffee* is in the basket is $\\frac{17}{20} = 0.85$, which means that in 85% of the times the coffee was bought, milk was purchased as well.\n",
        "\n",
        "Implement the **map_to_assoc_rules** function that calculates the confidence level for each subpattern.\n",
        "\n",
        "For the toy dataset, the expected output is:\n",
        "<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 57em; padding-left:5px\">\n",
        "<code>\n",
        "+---------------+------------------------------------------------------------------+\n",
        "|patterns       |association_rules                                                 |\n",
        "+---------------+------------------------------------------------------------------+\n",
        "|('a',)         |[('b', 1.0), ('c', 0.5), ('f', 0.5), ('d', 0.5), ('e', 0.5)]      |\n",
        "|('a', 'b')     |[('c', 0.5), ('f', 0.5), ('d', 0.5), ('e', 0.5)]                  |\n",
        "|('b',)         |[('a', 0.5), ('c', 0.75), ('f', 0.25), ('d', 0.25), ('e', 0.25)]  |\n",
        "|('a', 'b', 'c')|[]                                                                |\n",
        "|('b', 'c')     |[('a', 0.3333333333333333), ('f', 0.3333333333333333)]            |\n",
        "|('a', 'c')     |[('b', 1.0), ('f', 1.0)]                                          |\n",
        "|('a', 'b', 'f')|[]                                                                |\n",
        "|('b', 'f')     |[('a', 1.0), ('c', 1.0)]                                          |\n",
        "|('a', 'f')     |[('b', 1.0), ('c', 1.0)]                                          |\n",
        "|('c',)         |[('a', 0.3333333333333333), ('b', 1.0), ('f', 0.3333333333333333)]|\n",
        "|('a', 'c', 'f')|[]                                                                |\n",
        "|('c', 'f')     |[('a', 1.0), ('b', 1.0)]                                          |\n",
        "|('f',)         |[('a', 1.0), ('b', 1.0), ('c', 1.0)]                              |\n",
        "|('b', 'c', 'f')|[]                                                                |\n",
        "|('a', 'b', 'd')|[]                                                                |\n",
        "|('b', 'd')     |[('a', 1.0), ('e', 1.0)]                                          |\n",
        "|('a', 'd')     |[('b', 1.0), ('e', 1.0)]                                          |\n",
        "|('a', 'b', 'e')|[]                                                                |\n",
        "|('b', 'e')     |[('a', 1.0), ('d', 1.0)]                                          |\n",
        "|('a', 'e')     |[('b', 1.0), ('d', 1.0)]                                          |\n",
        "+---------------+------------------------------------------------------------------+\n",
        "</code>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4afeIBQQ8q3d",
        "outputId": "b35b7c69-f962-4387-bf5d-5207b43789a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "def map_to_assoc_rules(rule):\n",
        "    tmp = deepcopy(rule)\n",
        "    key = tmp[0]\n",
        "    values = tmp[1]\n",
        "    confidences = []\n",
        "    denom = values[0][1]\n",
        "    if len(values) == 1:\n",
        "        yield (key, [])\n",
        "    else:\n",
        "        for elem in values[1:]:\n",
        "            confidences.append(tuple((elem[0], float(elem[1])/denom)))\n",
        "        yield (key, confidences)\n",
        "\n",
        "assoc_rules = combined_rules.flatMap(map_to_assoc_rules)\n",
        "\n",
        "# Output as dataframe\n",
        "assoc_rules.map(format_tuples).toDF(['patterns', 'association_rules']).show(150, truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|patterns          |association_rules                                                                                                                           |\n",
            "+------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|(u'b', u'c')      |[(u'a', 0.3333333333333333), (u'f', 0.3333333333333333)]                                                                                    |\n",
            "|(u'b',)           |[(None, 1.3333333333333333), (u'a', 0.6666666666666666), (u'd', 0.3333333333333333), (u'e', 0.3333333333333333), (u'f', 0.3333333333333333)]|\n",
            "|(u'd', u'e')      |[(u'b', 1.0), (u'a', 1.0)]                                                                                                                  |\n",
            "|(u'd',)           |[(None, 1.0), (u'a', 1.0), (u'b', 1.0)]                                                                                                     |\n",
            "|(u'a', u'd')      |[(u'b', 1.0), (u'e', 1.0)]                                                                                                                  |\n",
            "|(u'f',)           |[(u'c', 1.0), (u'a', 1.0), (u'b', 1.0)]                                                                                                     |\n",
            "|(u'a', u'b', u'c')|[]                                                                                                                                          |\n",
            "|(u'a', u'b')      |[(None, 2.0), (u'f', 1.0), (u'e', 1.0), (u'd', 1.0)]                                                                                        |\n",
            "|(u'b', u'd')      |[(u'e', 1.0), (u'a', 1.0)]                                                                                                                  |\n",
            "|(u'c', u'f')      |[(u'b', 1.0), (u'a', 1.0)]                                                                                                                  |\n",
            "|(u'b', u'd', u'e')|[]                                                                                                                                          |\n",
            "|(u'b', u'c', u'f')|[]                                                                                                                                          |\n",
            "|(u'b', u'e')      |[(None, 1.0), (u'a', 1.0)]                                                                                                                  |\n",
            "|(u'a', u'b', u'f')|[]                                                                                                                                          |\n",
            "|(u'a',)           |[(u'b', 2.0), (None, 2.0), (u'f', 1.0), (u'c', 1.0), (u'e', 1.0)]                                                                           |\n",
            "|(u'a', u'b', u'e')|[]                                                                                                                                          |\n",
            "|(u'c',)           |[(u'f', 0.3333333333333333), (None, 1.0), (u'a', 0.3333333333333333)]                                                                       |\n",
            "|(u'a', u'b', u'd')|[]                                                                                                                                          |\n",
            "|(u'a', u'f')      |[(None, 1.0), (u'c', 1.0)]                                                                                                                  |\n",
            "|(u'e',)           |[(u'b', 1.0), (None, 1.0), (u'a', 1.0)]                                                                                                     |\n",
            "|(u'b', u'f')      |[(u'a', 1.0), (None, 1.0)]                                                                                                                  |\n",
            "|(u'a', u'd', u'e')|[]                                                                                                                                          |\n",
            "|(u'a', u'c', u'f')|[]                                                                                                                                          |\n",
            "|(u'a', u'c')      |[(u'f', 1.0), (None, 1.0)]                                                                                                                  |\n",
            "|(u'a', u'e')      |[(u'd', 1.0), (None, 1.0)]                                                                                                                  |\n",
            "+------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zKfTQs2U8q3f"
      },
      "source": [
        "## 3. Instacart dataset\n",
        "\n",
        "With your MBA algorithm ready to be used, now it is time to work on the real dataset. For this part of the TP, download the [instacart](https://www.instacart.com/datasets/grocery-shopping-2017) dataset and read its [description](https://gist.github.com/jeremystan/c3b39d947d9b88b3ccff3147dbcf6c6b) to understand how the dataset is structured. \n",
        "\n",
        "Before applying the developed algorithm on the instacart dataset, you must first filter the transactions to be in the same format defined by your algorithm (one transaction per row). To manipulate the data, we can use Spark's data frame and the SQL module presented in Section 1.\n",
        "\n",
        "The following code cell uses the Spark SQL module to read the orders from the ``order_products__train.csv`` and the detailed information from ``orders.csv`` and ``products.csv`` to construct a data frame that contains a list of all products ever purchased by each user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J7Sel7Mm8q3f",
        "outputId": "9296fb45-ec87-4f39-9d1f-89210dfc2a9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "instacart = \"gs://bucket_tp2_inf8111_kacemoumaymasemah/instacart_2017_05_01\"\n",
        "\n",
        "df_order_prod = spark.read.csv(instacart+'/order_products__train.csv', header=True, sep=',', inferSchema=True)\n",
        "print('order_products__train.csv')\n",
        "df_order_prod.show(5)\n",
        "\n",
        "df_orders = spark.read.csv(instacart+'/orders.csv', header=True, sep=',', inferSchema=True)\n",
        "print('orders.csv')\n",
        "df_orders.show(5)\n",
        "\n",
        "df_products = spark.read.csv(instacart+'/products.csv', header=True, sep=',', inferSchema=True)\n",
        "print('products.csv')\n",
        "df_products.show(5)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "List of products ever purchased by each user\n",
        "\"\"\"\n",
        "# USING SQL\n",
        "df_order_prod.createOrReplaceTempView(\"order_prod\") # creates table 'order_prod'\n",
        "df_orders.createOrReplaceTempView(\"orders\") # creates table 'orders'\n",
        "df_products.createOrReplaceTempView(\"products\") # creates table 'products'\n",
        "spark.sql('SELECT o.user_id, COLLECT_LIST(p.product_name) AS products' \n",
        "               ' FROM orders o '\n",
        "               ' INNER JOIN order_prod op ON op.order_id = o.order_id'\n",
        "               ' INNER JOIN products p    ON op.product_id = p.product_id'\n",
        "               ' GROUP BY user_id ORDER BY o.user_id').show(5, truncate=80)\n",
        "\n",
        "\n",
        "# USING DATAFRAME OPERATIONS\n",
        "# df_orders.join(df_order_prod, df_order_prod.order_id == df_orders.order_id, 'inner')\\\n",
        "# .join(df_products, df_products.product_id == df_order_prod.product_id, 'inner')\\\n",
        "# .groupBy(df_orders.user_id).agg(f.collect_list(df_products.product_name).alias('products'))\\\n",
        "# .orderBy(df_orders.user_id).show(5, truncate=80)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "order_products__train.csv\n",
            "+--------+----------+-----------------+---------+\n",
            "|order_id|product_id|add_to_cart_order|reordered|\n",
            "+--------+----------+-----------------+---------+\n",
            "|       1|     49302|                1|        1|\n",
            "|       1|     11109|                2|        1|\n",
            "|       1|     10246|                3|        0|\n",
            "|       1|     49683|                4|        0|\n",
            "|       1|     43633|                5|        1|\n",
            "+--------+----------+-----------------+---------+\n",
            "only showing top 5 rows\n",
            "\n",
            "orders.csv\n",
            "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
            "|order_id|user_id|eval_set|order_number|order_dow|order_hour_of_day|days_since_prior_order|\n",
            "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
            "| 2539329|      1|   prior|           1|        2|                8|                  null|\n",
            "| 2398795|      1|   prior|           2|        3|                7|                  15.0|\n",
            "|  473747|      1|   prior|           3|        3|               12|                  21.0|\n",
            "| 2254736|      1|   prior|           4|        4|                7|                  29.0|\n",
            "|  431534|      1|   prior|           5|        4|               15|                  28.0|\n",
            "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "products.csv\n",
            "+----------+--------------------+--------+-------------+\n",
            "|product_id|        product_name|aisle_id|department_id|\n",
            "+----------+--------------------+--------+-------------+\n",
            "|         1|Chocolate Sandwic...|      61|           19|\n",
            "|         2|    All-Seasons Salt|     104|           13|\n",
            "|         3|Robust Golden Uns...|      94|            7|\n",
            "|         4|Smart Ones Classi...|      38|            1|\n",
            "|         5|Green Chile Anyti...|       5|           13|\n",
            "+----------+--------------------+--------+-------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------+--------------------------------------------------------------------------------+\n",
            "|user_id|                                                                        products|\n",
            "+-------+--------------------------------------------------------------------------------+\n",
            "|      1|[Soda, Organic String Cheese, 0% Greek Strained Yogurt, XL Pick-A-Size Paper ...|\n",
            "|      2|[Organic Roasted Turkey Breast, Gluten Free Whole Grain Bread, Plantain Chips...|\n",
            "|      5|[Organic Raw Agave Nectar, Organic Large Extra Fancy Fuji Apple, Sharp Chedda...|\n",
            "|      7|[Panama Peach Antioxidant Infusion, Antioxidant Infusions Beverage Malawi Man...|\n",
            "|      8|[Shallot, Organic SprouTofu Silken Tofu, Nutritional Yeast Seasoning, Organic...|\n",
            "+-------+--------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HEPhyKpC8q3h"
      },
      "source": [
        "### 3.1 Business Insights (25 points) \n",
        "\n",
        "Now, you are the data scientist. Considering only the orders of ``order_products__train.csv``, use of Spark SQL module, performing with SQL or data frame, to answer the following questions:\n",
        "\n",
        "1. What are the top 10 products which have the highest probability of being reordered? Consider only products purchased at least 40 times for this task.\n",
        "2. What are the top 3 most purchased products of each department?\n",
        "4. What is the average basket size for each day of the week?\n",
        "- Hint: use a barplot to visualize your results\n",
        "\n",
        "**The output of those questions must contain the products' NAME, not their ID.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yZqlCqg-8FMR",
        "colab": {}
      },
      "source": [
        "top_10 = spark.sql(\"\"\" SELECT p1.product_name, \n",
        "                              total_order, \n",
        "                              nbr_reorder / total_order AS proba_reorder \n",
        "                      FROM (SELECT product_id, \n",
        "                                    COUNT(product_id) AS total_order, \n",
        "                                    SUM(reordered) AS nbr_reorder \n",
        "                      FROM order_prod \n",
        "                      GROUP BY product_id) AS p2\n",
        "                      INNER JOIN products AS p1 ON p1.product_id = p2.product_id \n",
        "                      WHERE total_order >= 40 \n",
        "                      ORDER BY proba_reorder DESC LIMIT 10 \"\"\")\n",
        "\n",
        "top_10.show(15, truncate=80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEfftjf39-4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualization\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "prod_names = top_10.select(\"product_name\").rdd.flatMap(lambda x: x).collect()\n",
        "prob_reord = top_10.select(\"proba_reorder\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "tmp = range(len(prod_names))\n",
        "\n",
        "plt.barh(tmp,prob_reord )\n",
        "plt.yticks(tmp, prod_names)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel(\"Product names\")\n",
        "plt.ylabel(\"Probability of being reordered\")\n",
        "plt.title(\"The top 10 products purchased at least 40 times and which have the highest probability of being reordered\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oBGauYYWRzEI",
        "colab": {}
      },
      "source": [
        "top_3 = spark.sql(\"\"\" SELECT * \n",
        "                      FROM   (SELECT department_id, \n",
        "                                    product_name, \n",
        "                                    total_order, \n",
        "                                    Row_number() \n",
        "                                      OVER ( \n",
        "                                        partition BY department_id \n",
        "                                        ORDER BY total_order DESC ) AS rowno \n",
        "                              FROM   (SELECT p.department_id, \n",
        "                                            op.product_id, \n",
        "                                            p.product_name, \n",
        "                                            Count(op.product_id) AS total_order \n",
        "                                      FROM   order_prod op \n",
        "                                            INNER JOIN products p \n",
        "                                                    ON op.product_id = p.product_id \n",
        "                                      GROUP  BY p.department_id, \n",
        "                                                op.product_id, \n",
        "                                                p.product_name))  \n",
        "                      WHERE  rowno <= 3 \n",
        "                      ORDER  BY int(department_id) ASC, \n",
        "                                total_order DESC \"\"\")\n",
        "\n",
        "top_3.show(150, truncate=80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyjnJHYH-EXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualization\n",
        "\n",
        "prod_names = top_3.select(\"product_name\").rdd.flatMap(lambda x: x).collect()\n",
        "total_order = top_3.select(\"total_order\").rdd.flatMap(lambda x: x).collect()\n",
        "department = top_3.select(\"department_id\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "tmp = range(len(prod_names))\n",
        "fig = plt.figure(figsize=(10,14))\n",
        "labels = [a +' / Dep. #' + str(b) for a, b in zip(prod_names, department)]\n",
        "plt.barh(tmp,total_order)\n",
        "plt.yticks(tmp, labels)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.ylabel(\"Product names\")\n",
        "plt.xlabel(\"Total orders\")\n",
        "plt.title(\"The top 3 most purchased products of each department\")\n",
        "plt.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IX3CHbE4HjYL",
        "colab": {}
      },
      "source": [
        "avg_dow = spark.sql(\"\"\" SELECT order_dow, \n",
        "                               Avg(basket_size) AS avg_basket_size \n",
        "                        FROM   (SELECT order_id, \n",
        "                                      Count(product_id) AS basket_size \n",
        "                                FROM   order_prod \n",
        "                                GROUP  BY order_id) AS orders_baskets \n",
        "                              INNER JOIN orders AS o \n",
        "                                      ON o.order_id = orders_baskets.order_id \n",
        "                        GROUP  BY order_dow \n",
        "                        ORDER  BY order_dow \"\"\")\n",
        "\n",
        "avg_dow.show(150, truncate=80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4cxi2TRFcpYr",
        "colab": {}
      },
      "source": [
        "# Visualization\n",
        "\n",
        "order_dow = avg_dow.select(\"order_dow\").rdd.flatMap(lambda x: x).collect()\n",
        "avg_basket_size  = avg_dow.select(\"avg_basket_size\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "tmp = range(len(order_dow))\n",
        "\n",
        "plt.barh(tmp,avg_basket_size )\n",
        "plt.yticks(tmp, order_dow)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel(\"Average basket size\")\n",
        "plt.ylabel(\"Order day of the week\")\n",
        "plt.title(\"The average basket size for each day of the week\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LMoLK30G8q3m"
      },
      "source": [
        "### 3.2 Run MBA for the training set (15 points)\n",
        "\n",
        "Using the orders from the ``order_products__train.csv``, create a data frame where each row contains the column “transaction” with the list of purchased products, similarly to the toy dataset. In sequence, run the MBA algorithm for this set of transactions. \n",
        "\n",
        "- You must report the time spent to perform this task.\n",
        "- Output must contain the products' name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2uTc29bYXi6O",
        "outputId": "4538399d-29b1-43fb-dbb8-da59a8803882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "Create a query to create and struct the transactions\n",
        "\"\"\"\n",
        "transactions_df = spark.sql(\"\"\"SELECT o.order_id, concat_ws(\";\", collect_set(p.product_name)) AS transaction \n",
        "                          FROM order_prod as o\n",
        "                          INNER JOIN products p ON p.product_id = o.product_id\n",
        "                          GROUP BY o.order_id\"\"\")\n",
        "\n",
        "transactions_df.show(5, truncate=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+----------------------------------------------------------------------------------------------------+\n",
            "|order_id|                                                                                         transaction|\n",
            "+--------+----------------------------------------------------------------------------------------------------+\n",
            "|    1342|Raw Shrimp;Seedless Cucumbers;Versatile Stain Remover;Organic Strawberries;Organic Mandarins;Chic...|\n",
            "|    1591|Cracked Wheat;Strawberry Rhubarb Yoghurt;Organic Bunny Fruit Snacks Berry Patch;Goodness Grapenes...|\n",
            "|    4519|                                  Beet Apple Carrot Lemon Ginger Organic Cold Pressed Juice Beverage|\n",
            "|    4935|                                                                                               Vodka|\n",
            "|    6357|Globe Eggplant;Panko Bread Crumbs;Fresh Mozzarella Ball;Grated Parmesan;Gala Apples;Italian Pasta...|\n",
            "+--------+----------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "CPU times: user 915 µs, sys: 1.1 ms, total: 2.02 ms\n",
            "Wall time: 5.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yGQ2ZJTP8q3q",
        "outputId": "75419805-3907-4e8c-fbcc-aed12f08768c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "Run the MBA algorithm and show the first 5 association rules\n",
        "\"\"\"\n",
        "transactions_rdd = transactions_df.rdd\n",
        "patterns_rdd = transactions_rdd.flatMap(map_to_patterns)\n",
        "\n",
        "# Output as dataframe\n",
        "patterns_rdd.map(format_tuples).toDF(['patterns', 'occurrences']).show(1,truncate=80)\n",
        "\n",
        "combined_patterns_rdd = patterns_rdd.reduceByKey(reduce_patterns)\n",
        "\n",
        "# Output as dataframe\n",
        "combined_patterns_rdd.map(format_tuples).toDF(['patterns', 'combined_occurrences']).show(1,truncate=80)\n",
        "\n",
        "subpatterns_rdd = combined_patterns_rdd.flatMap(map_to_subpatterns)\n",
        "\n",
        "# Output as dataframe\n",
        "subpatterns_rdd.map(format_tuples).toDF(['subpatterns', 'rules']).show(1,truncate=80)\n",
        "\n",
        "combined_rules = subpatterns_rdd.groupByKey().map(list_map)\n",
        "\n",
        "# Output as dataframe\n",
        "combined_rules.map(format_tuples).toDF(['subpatterns', 'combined_rules']).show(1,truncate=80)\n",
        "\n",
        "assoc_rules = combined_rules.flatMap(map_to_assoc_rules)\n",
        "\n",
        "# Output as dataframe\n",
        "assoc_rules.map(format_tuples).toDF(['patterns', 'association_rules']).show(1,truncate=80)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------------+-----------+\n",
            "|                   patterns|occurrences|\n",
            "+---------------------------+-----------+\n",
            "|('Bag of Organic Bananas',)|          1|\n",
            "+---------------------------+-----------+\n",
            "only showing top 1 row\n",
            "\n",
            "+-------------------------------------------------------------------+--------------------+\n",
            "|                                                           patterns|combined_occurrences|\n",
            "+-------------------------------------------------------------------+--------------------+\n",
            "|('Organic Mandarins', 'Organic Strawberries', 'Seedless Cucumbers')|                   4|\n",
            "+-------------------------------------------------------------------+--------------------+\n",
            "only showing top 1 row\n",
            "\n",
            "+-------------------------------------------------------------------+---------+\n",
            "|                                                        subpatterns|    rules|\n",
            "+-------------------------------------------------------------------+---------+\n",
            "|('Organic Mandarins', 'Organic Strawberries', 'Seedless Cucumbers')|(None, 4)|\n",
            "+-------------------------------------------------------------------+---------+\n",
            "only showing top 1 row\n",
            "\n",
            "+-------------------------------------------------------------------+--------------+\n",
            "|                                                        subpatterns|combined_rules|\n",
            "+-------------------------------------------------------------------+--------------+\n",
            "|('Organic Mandarins', 'Organic Strawberries', 'Seedless Cucumbers')|   [(None, 4)]|\n",
            "+-------------------------------------------------------------------+--------------+\n",
            "only showing top 1 row\n",
            "\n",
            "+-------------------------------------------------------------------+-----------------+\n",
            "|                                                           patterns|association_rules|\n",
            "+-------------------------------------------------------------------+-----------------+\n",
            "|('Organic Mandarins', 'Organic Strawberries', 'Seedless Cucumbers')|               []|\n",
            "+-------------------------------------------------------------------+-----------------+\n",
            "only showing top 1 row\n",
            "\n",
            "CPU times: user 486 ms, sys: 113 ms, total: 599 ms\n",
            "Wall time: 1h 4min 15s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qqUhHssJeQvl",
        "outputId": "56ec91c8-fe0b-4c69-f2b5-68092774df03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "source": [
        "assoc_rules.map(format_tuples).toDF(['patterns', 'association_rules']).show(25,truncate=80)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
            "|                                                                        patterns|                                                               association_rules|\n",
            "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
            "|             ('Organic Mandarins', 'Organic Strawberries', 'Seedless Cucumbers')|                                                                              []|\n",
            "|                     ('Banana', 'Cracked Wheat', 'Green Machine Juice Smoothie')|                                                                              []|\n",
            "|                        ('Banana', 'Lemon Yogurt', 'Oven Roasted Turkey Breast')|                                                                              []|\n",
            "|('Buttermilk Waffles', 'Chewy 25% Low Sugar Chocolate Chip Granola', 'Honey G...|                                                                              []|\n",
            "|                  ('Buttermilk Waffles', 'Garlic', 'Oven Roasted Turkey Breast')|                                                                              []|\n",
            "|('Buttermilk Waffles', 'Granny Smith Apples', 'Lower Sugar Instant Oatmeal  V...|                                                                              []|\n",
            "|('Chewy 25% Low Sugar Chocolate Chip Granola', 'Goldfish Pretzel Baked Snack ...|                                                                              []|\n",
            "|('Cinnamon Multigrain Cereal', 'Green Machine Juice Smoothie', 'Original Patt...|                                                                              []|\n",
            "|                          ('Coconut Dreams Cookies', 'Cracked Wheat', 'Spinach')|                                                                              []|\n",
            "|       ('Coconut Dreams Cookies', 'Cracked Wheat', 'Strawberry Banana Smoothie')|                                                                              []|\n",
            "|   ('Coconut Dreams Cookies', 'Garlic', 'Goldfish Pretzel Baked Snack Crackers')|                                                                              []|\n",
            "|                             ('Coconut Dreams Cookies', 'Nutty Bars', 'Spinach')|                                                                              []|\n",
            "|('Coconut Dreams Cookies', 'Organic Greek Whole Milk Blended Vanilla Bean Yog...|                                                                              []|\n",
            "|                       ('Cracked Wheat', 'Lemon Yogurt', 'Pure Vanilla Extract')|                                                                              []|\n",
            "|('Goldfish Pretzel Baked Snack Crackers', 'Granny Smith Apples', 'Medium Scar...|                                                                              []|\n",
            "|('Goldfish Pretzel Baked Snack Crackers', 'Original Whole Grain Chips', 'Oven...|                                                                              []|\n",
            "|                 ('Granny Smith Apples', 'Lemon Yogurt', 'Pure Vanilla Extract')|                                                                              []|\n",
            "|('Green Machine Juice Smoothie', 'Lower Sugar Instant Oatmeal  Variety', 'Str...|                                                                              []|\n",
            "|                   ('Honey Graham Snacks', 'Nutty Bars', 'Uncured Genoa Salami')|                                                                              []|\n",
            "|('Lemon Yogurt', 'Natural Vanilla Ice Cream', 'Organic Greek Whole Milk Blend...|                                                                              []|\n",
            "|('Lemon Yogurt', 'Organic Greek Whole Milk Blended Vanilla Bean Yogurt', 'Str...|                                                                              []|\n",
            "|('Lower Sugar Instant Oatmeal  Variety', 'Strawberry Banana Smoothie', 'Uncur...|                                                                              []|\n",
            "|('Natural Vanilla Ice Cream', 'Organic Bunny Fruit Snacks Berry Patch', 'Stra...|                                                                              []|\n",
            "|                                         ('Coconut Flour', 'Organic Spring Mix')|[('Organic Basil', 0.5), ('Organic Tomato Sauce', 0.5), ('Light Brown Sugar',...|\n",
            "|                                                ('Coconut Flour', 'Roma Tomato')|[('Organic Tapioca Flour', 1.0), ('Organic Baby Spinach', 1.0), ('Limes', 1.0...|\n",
            "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
            "only showing top 25 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WyNBcK5f8q3s"
      },
      "source": [
        "# 3.3 Run MBA for the whole dataset (15 points)\n",
        "\n",
        "As you probably noticed, even for a not so large data set (the training file has only 131K orders), the MBA algorithm is computationally expensive. For that reason, this time, we will repeat the process, but now using the Google Cloud Platform (GCP) to create a large computer cluster. All the instructions for creating a computing cluster with spark and how to submit a job will be explained in both sessions of the laboratory. In any case, you should read the instructions given in the ``Instruction_GCP.pdf``.\n",
        "\n",
        "This time, we will work with the ``order_products__prior.csv`` file, which contains more than 3M orders.\n",
        "\n",
        "**EXPECTED OUTPUT**\n",
        "\n",
        "After you ran the MBA for the larger collection of orders, randomly select ONE product purchased in ``order_products__prior`` and print the association rules (product name and association value) of this product, i.e., when the product is alone in the basket. The output should be formatted in a table, where each row containing the information of one associated product.\n",
        "\n",
        "- Print both ID and Name of the random selected product.\n",
        "- Report the execution time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4538399d-29b1-43fb-dbb8-da59a8803882",
        "id": "Os3QqFzpzFF6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "Create a query to create and struct the transactions\n",
        "\"\"\"\n",
        "transactions_df = spark.sql(\"\"\"SELECT o.order_id, concat_ws(\";\", collect_set(p.product_name)) AS transaction \n",
        "                          FROM order_prod as o\n",
        "                          INNER JOIN products p ON p.product_id = o.product_id\n",
        "                          GROUP BY o.order_id\"\"\")\n",
        "\n",
        "transactions_df.show(5, truncate=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+----------------------------------------------------------------------------------------------------+\n",
            "|order_id|                                                                                         transaction|\n",
            "+--------+----------------------------------------------------------------------------------------------------+\n",
            "|    1342|Raw Shrimp;Seedless Cucumbers;Versatile Stain Remover;Organic Strawberries;Organic Mandarins;Chic...|\n",
            "|    1591|Cracked Wheat;Strawberry Rhubarb Yoghurt;Organic Bunny Fruit Snacks Berry Patch;Goodness Grapenes...|\n",
            "|    4519|                                  Beet Apple Carrot Lemon Ginger Organic Cold Pressed Juice Beverage|\n",
            "|    4935|                                                                                               Vodka|\n",
            "|    6357|Globe Eggplant;Panko Bread Crumbs;Fresh Mozzarella Ball;Grated Parmesan;Gala Apples;Italian Pasta...|\n",
            "+--------+----------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "CPU times: user 0 ns, sys: 2.3 ms, total: 2.3 ms\n",
            "Wall time: 1.47 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "75419805-3907-4e8c-fbcc-aed12f08768c",
        "id": "ZEabxacJzFF8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "Run the MBA algorithm and show the first 5 association rules\n",
        "\"\"\"\n",
        "transactions_rdd = transactions_df.rdd\n",
        "patterns_rdd = transactions_rdd.flatMap(map_to_patterns)\n",
        "\n",
        "# Output as dataframe\n",
        "patterns_rdd.map(format_tuples).toDF(['patterns', 'occurrences']).show(1,truncate=80)\n",
        "\n",
        "combined_patterns_rdd = patterns_rdd.reduceByKey(reduce_patterns)\n",
        "\n",
        "# Output as dataframe\n",
        "combined_patterns_rdd.map(format_tuples).toDF(['patterns', 'combined_occurrences']).show(1,truncate=80)\n",
        "\n",
        "subpatterns_rdd = combined_patterns_rdd.flatMap(map_to_subpatterns)\n",
        "\n",
        "# Output as dataframe\n",
        "subpatterns_rdd.map(format_tuples).toDF(['subpatterns', 'rules']).show(1,truncate=80)\n",
        "\n",
        "combined_rules = subpatterns_rdd.groupByKey().map(list_map)\n",
        "\n",
        "# Output as dataframe\n",
        "combined_rules.map(format_tuples).toDF(['subpatterns', 'combined_rules']).show(1,truncate=80)\n",
        "\n",
        "assoc_rules = combined_rules.flatMap(map_to_assoc_rules)\n",
        "\n",
        "# Output as dataframe\n",
        "assoc_rules.map(format_tuples).toDF(['patterns', 'association_rules']).show(1,truncate=80)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------+-----------+\n",
            "|                    patterns|occurrences|\n",
            "+----------------------------+-----------+\n",
            "|(u'Bag of Organic Bananas',)|          1|\n",
            "+----------------------------+-----------+\n",
            "only showing top 1 row\n",
            "\n",
            "+--------------------------------------------------------------------------------+--------------------+\n",
            "|                                                                        patterns|combined_occurrences|\n",
            "+--------------------------------------------------------------------------------+--------------------+\n",
            "|(u'Barbecue Potato Chips', u\"Kid Z Bar Organic S'mores Energy Bars\", u'Organi...|                   1|\n",
            "+--------------------------------------------------------------------------------+--------------------+\n",
            "only showing top 1 row\n",
            "\n",
            "+--------------------------------------------------------------------------------+---------+\n",
            "|                                                                     subpatterns|    rules|\n",
            "+--------------------------------------------------------------------------------+---------+\n",
            "|(u'Barbecue Potato Chips', u\"Kid Z Bar Organic S'mores Energy Bars\", u'Organi...|(None, 1)|\n",
            "+--------------------------------------------------------------------------------+---------+\n",
            "only showing top 1 row\n",
            "\n",
            "+-------------------------------------------------------------+--------------+\n",
            "|                                                  subpatterns|combined_rules|\n",
            "+-------------------------------------------------------------+--------------+\n",
            "|(u'Bok Choy', u'Grade A Goat Milk', u'Organic Baby Kale Mix')|   [(None, 1)]|\n",
            "+-------------------------------------------------------------+--------------+\n",
            "only showing top 1 row\n",
            "\n",
            "+-------------------------------------------------------------------+-----------------+\n",
            "|                                                           patterns|association_rules|\n",
            "+-------------------------------------------------------------------+-----------------+\n",
            "|(u'Milk and Cookies Ice Cream', u'Organic Lemon', u'Sliced Mangos')|               []|\n",
            "+-------------------------------------------------------------------+-----------------+\n",
            "only showing top 1 row\n",
            "\n",
            "CPU times: user 171 ms, sys: 63.7 ms, total: 235 ms\n",
            "Wall time: 9min 35s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "56ec91c8-fe0b-4c69-f2b5-68092774df03",
        "id": "9N2lbTulzFF-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "source": [
        "assoc_rules.map(format_tuples).toDF(['patterns', 'association_rules']).show(50,truncate=80)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
            "|                                                                        patterns|                                                               association_rules|\n",
            "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
            "|             (u'Milk and Cookies Ice Cream', u'Organic Lemon', u'Sliced Mangos')|                                                                              []|\n",
            "|        (u'Broccoli Wokly', u'Calm Chamomile Herbal Tea', u'Refresh Herbal Tea')|                                                                              []|\n",
            "|(u'Bag of Organic Bananas', u'Cheese & Herb Pizza', u'Organic Chocolate Chip ...|                                                                              []|\n",
            "|(u'Chunk Light Tuna In Water', u'Original Wheat Thins', u'Santa Fe-Style Rice...|                                                                              []|\n",
            "|(u'Pop-Tarts Frosted Chocolate Fudge Toaster Pastries', u'Sausage, Egg, & Che...|                                                                              []|\n",
            "|(u'3 lb Clementines', u'Cherubs Tomatoes', u'Roasted Red Pepper & Basil Quino...|                                                                              []|\n",
            "|(u'Boneless Skinless Chicken Breast Fillets', u'Organic Cheese Frozen Pizza',...|                                                                              []|\n",
            "|(u'Boneless Skinless Chicken Breasts', u'Cage Free Brown Eggs-Large, Grade A'...|                                                                              []|\n",
            "|(u'Dipps Chocolatey Covered Chocolate Chip Granola Bars', u'Neapolitan Ice Cr...|                                                                              []|\n",
            "|                 (u'Organic Ketchup', u'Sugar Snap Peas', u'Variety Snack Pack')|                                                                              []|\n",
            "|(u'Cauliflower Crumbles Chopped Cauliflower', u'Febreze Fresh Clean Garbage B...|                                                                              []|\n",
            "|(u'100% White Grape Juice', u'Original Life Cereal', u'Original Thin Sausage ...|                                                                              []|\n",
            "|(u'Large Lemon', u'Organic Avocado', u'Whole Wheat Bunnies Baked Snack Cracke...|                                                                              []|\n",
            "|(u'Organic  Whole Milk', u'Organic Lowfat Mango Kefir', u'Unsweetened Whole M...|                                                                              []|\n",
            "|(u'Fiber & Protein Organic Pears, Raspberries, Butternut Squash & Carrots Sna...|                                                                              []|\n",
            "|(u'Fluoride-Free Antiplaque & Whitening Spearmint Toothpaste', u'Organic Mexi...|                                                                              []|\n",
            "|                            (u'Bananas', u'Beef Broth', u'Whole Cut Up Chicken')|                                                                              []|\n",
            "|(u'Organic Blueberries', u'Organic Rainbow Carrots', u'Sliced Soppressata Sal...|                                                                              []|\n",
            "|(u'Bag of Organic Bananas', u'Organic Hass Avocado', u'Organix Butcher & Bush...|                                                                              []|\n",
            "|(u'Grovestand Lots Of Pulp Pure 100% Florida Orange Juice', u'Kale Greens', u...|                                                                              []|\n",
            "|(u'Organic 85% Cacao Dark Chocolate Bar', u'Organic Honey Mustard', u'Peanut ...|                                                                              []|\n",
            "|(u'2nd Foods Sweet Potato & Turkey With Whole Grains Baby Food', u'Vegetable ...|                                                                              []|\n",
            "|(u'Gluten Free Broccoli & Cheese Baked Nuggets', u'Organic Broccoli Florets',...|                                                                              []|\n",
            "|(u'Acorn Squash', u'Almond Breeze Original Almond Milk', u'Organic Grade A Fr...|                                                                              []|\n",
            "|(u'Di Giorno Half & Half Pizza Cheese And Pepperoni', u'Frozen Breaded Bagged...|                                                                              []|\n",
            "|                            (u'Hass Avocado Bag', u'Organic Small Bunch Celery')|[(u'Organic Tomato Sauce', 0.5), (u'Chocolate Ice Cream', 0.5), (u'Yellow Oni...|\n",
            "|(u'Cool Moisture Body Wash', u'Cucumber Kirby', u'No Pulp Calcium & Vitamin D...|                                                                              []|\n",
            "|                            (u'Beef Fajitas', u'Beer', u'Light Caesar Dressing')|                                                                              []|\n",
            "|                       (u'Medium Salsa Organica', u'Thin Crust Pepperoni Pizza')|[(u'Gluten Free Uncured Frozen Pepperoni Pizza', 1.0), (u'Organic Extra Large...|\n",
            "|  (u'Olive Brushetta', u'Organic Large Grade AA Brown Eggs', u'White Nectarine')|                                                                              []|\n",
            "|(u'Dairy Pure Fresh Heavy Whipping Cream', u'Zucchini Banana & Amaranth Organ...|[(None, 1.0), (u'Large Lemon', 1.0), (u'Chocolate Peanut Butter Cup Gelato', ...|\n",
            "|(u'Orange Bell Pepper', u'Organic Old Fashioned Rolled Oats', u'Small Hass Av...|                                                                              []|\n",
            "|(u'Dr. Zevia Zero Calorie Soda', u'Italian Deli Slices', u'Organic Whole Stra...|                                                                              []|\n",
            "|           (u'Clean Individual Wipes', u'Natural Shredded Monterey Jack Cheese')|[(u'BBQ Chicken Crispy Thin Crust Pizza', 1.0), (u'All Natural Boneless Skinl...|\n",
            "|   (u'Bag of Organic Bananas', u'Organic Frozen Peas', u'Sparkling Lemon Water')|                                                                              []|\n",
            "|                       (u'Cheerios Cereal', u'Green Peas', u'Organic Red Onion')|                                                                              []|\n",
            "|      (u'Olive Hummus', u'Spinach Peas & Pear Stage 2 Baby Food', u'White Corn')|                                                                              []|\n",
            "|(u'Feta Cheese Crumbles', u'Pure Sparkling Water', u'Sparkling Water Grapefru...|                                                                              []|\n",
            "|(u'Caramel Almond and Sea Salt Nut Bar', u'Crunchy Almond Butter', u'Organic ...|                                                                              []|\n",
            "|            (u'Classic Mild Cheddar Macaroni & Cheese', u'Organic Green Onions')|[(u'40 Watt Appliance Bulb', 1.0), (u'Grape Tomatoes', 1.0), (u'Grillers Prim...|\n",
            "|(u'2-Ply 100% Recycled Bathroom Tissue', u'Organic Cilantro', u'Uncured Genoa...|                                                                              []|\n",
            "|(u'Frozen Broccoli Florets', u'Organic Dijon Mustard', u'Organic Strawberry F...|                                                                              []|\n",
            "|(u'Organic 85% Cacao Dark Chocolate Bar', u'Organic Strawberry & Mango Dried ...|                                                                              []|\n",
            "|(u'Organic 50/50 Blend Salad', u'Original Coffee Creamer', u'Rising Crust Pep...|                                                                              []|\n",
            "|(u'Oats & Honey Gluten Free Granola', u'Persimmon Sharon Fruit Cv', u'Vitamin...|                                                                              []|\n",
            "|(u'Extra Sharp White Cheddar', u'Organic Dill', u'Organic Italian Parsley Bun...|                                                                              []|\n",
            "|(u'Angel Hair Nests Pasta', u'Organic Southwest Powermeal Bowl', u'Smok Cured...|                                                                              []|\n",
            "|                             (u'Original Veggie Straws', u'Overnight Maxi Pads')|[(u'2-Ply 100% Recycled Bathroom Tissue', 1.0), (u'\"9\"\" Apple Pie\"', 1.0), (u...|\n",
            "|(u'Jalapeno Cilantro Salsa', u'Light Sour Cream', u'Sliced Havarti Dill Cheese')|                                                                              []|\n",
            "|                       (u'Apple Juice', u'Balsamic Vinegar', u'Bunched Carrots')|                                                                              []|\n",
            "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
            "only showing top 50 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJWioWPXzFGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}