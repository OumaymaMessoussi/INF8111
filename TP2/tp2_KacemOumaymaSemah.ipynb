{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tp2_KacemOumaymaSemah.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r02mfwtj8q14"
      },
      "source": [
        "# TP2 - Market Basket Analysis \n",
        "INF8111 - Fouille de données, Summer 2020\n",
        "### Team Components\n",
        "    - Kacem Khaled\n",
        "    - Oumayma Messoussi\n",
        "    - Semah Aissaoui\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hr5BXQys8q16"
      },
      "source": [
        "## Date et directives de remise\n",
        "Vous remettrez ce fichier nommé TP2\\_NomDuMembre1\\_NomDuMembre2\\_NomDuMembre3.ipynb dans la boîte de remise sur moodle. \n",
        "\n",
        "Tout devra être remis avant le **7 juin à 23h55**.\n",
        "\n",
        "## Market Basket Analysis\n",
        "\n",
        "Market Basket Analysis (MBA) is a data mining analytics technique to uncover associations between products or product grouping. By exploring interesting patterns from an extensive collection of data, MBA aims to understand/reveal customer purchase behaviors based upon the theory that if you purchased a certain set of products, then you are more (or less) likely to buy another group of products. In other words, MBA allows retailers to identify the relationship between the items that customers buy, revealing patterns of items often purchased together.\n",
        "\n",
        "A widely used approach to explore these patterns is by constructing ***association rules*** such as\n",
        "- **if** bought *ITEM_1* **then** will buy *ITEM_2* with **confidence** *X*.\n",
        "\n",
        "These associations do not have to be 1-to-1 rules. They can involve many items. For example, a person in a supermarket may add eggs to his/her cart, then an MBA application may suggest that the person will also buy some bread and/or flour: \n",
        "\n",
        "+ **if** bought *EGGS* **then** will buy [*BREAD* with confidence *0.2*; *FLOUR* with confidence 0.05].\n",
        "\n",
        "However, if the person now decides to add flour to his/her cart, the new association rule could be as showing below, suggesting ingredients to make a cake.\n",
        "\n",
        "+ **if** bought [*EGGS, FLOUR*] **then** will buy [*SUGGAR* with confidence 0.45; BAKING POWDER with confidence 0.12; *BREAD* with confidence *0.03*].\n",
        "\n",
        "There are many real scenarios where MBA plays a central role in data analysis, such as supermarket transactions, online orders or credit card history. Marketers may use these association rules to arrange correlated products closer to each other on store shelves or make online suggestions so that customers buy more items. Some questions that an MBA can usually help retailers to answer are:\n",
        "\n",
        "- What items are often purchased together?\n",
        "- Given a basket, what items should be suggested?\n",
        "- How should items be placed together on the shelves?\n",
        "\n",
        "### Objective\n",
        "\n",
        "Your goal in this TP is to develop an MBA algorithm for revealing patterns by creating association rules in a big dataset with more than three million supermarket transactions. However, mining association rules for large datasets is a very computationally intensive problem, which makes it almost impractical to perform it without a distributed system. Hence, to run your algorithm, you will have access to a distributed cloud computing cluster with hundreds of cores. \n",
        "\n",
        "To this end, a **MapReduce** algorithm will be implemented upon the [Apache Spark](http://spark.apache.org) framework, a fast cluster computing system. In a nutshell, Spark is an open source framework designed with a *scale-out* methodology which makes it a very powerful tool for programmers or application developers to perform a massive volume of computations and data processing in distributed environments. Spark provides high-level APIs that make it easy to build parallel apps without needing to worry about how your code and data are parallelized/distributed thought the computing cluster. Spark does it all for you.\n",
        "\n",
        "The implementation will follow the Market Basket Analysis algorithm presented by Jongwook Woo and Yuhang Xu (2012). The image **workflow.svg** Illustrates the algorithm's workflow, and is to be used for consultation throughout this TP. The blue boxes are the ones where you must implement a method to perform a map or reduce function, and the gray boxes represent their expected output. **All these operations are explained in detail in the following sections.** \n",
        "\n",
        "\n",
        "## 1. Setting up Spark\n",
        "\n",
        "Spark runs on both Windows and UNIX-like systems (e.g., Linux, Mac OS). It's easy to run locally on one machine — all you need is to have Java installed on your system PATH, or the JAVA_HOME environment variable pointing to a Java installation. It is mandatory that you have the **JDK v8** installed in your system, as Spark currently only support this version. If you haven't, go to [Java's web page](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html) to download and install a Java Virtual Machine. Remember to set the environment variable JAVA_HOME to use JDK v8 if your installation does not do it automatically for you. \n",
        "\n",
        "The interface between Python and Spark is done through **PySpark**, which can be installed by running `pip install pyspark` or set up following the sequence below:\n",
        "\n",
        "1. First, go to http://spark.apache.org/downloads \n",
        "2. Select the newest Spark release and the Pre-built for Apache Hadoop 2.7 package \n",
        "3. Click for download **spark-2.4.5-bin-hadoop2.7.tgz** and unzip it in any folder of your preference. \n",
        "4. Next, export the following variables to link PYSPARK (Spark's python interface) to your python distribution in your `~/.bash_profile` file.\n",
        "\n",
        "``\n",
        "export SPARK_HOME=/path/to/spark-2.4.5-bin-hadoop2.7\n",
        "export PYTHONPATH=\"$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$SPARK_HOME/python/lib/pyspark.zip:$PYTHONPATH\"\n",
        "export PYSPARK_PYTHON=/path/to/your/python3\n",
        "``\n",
        "\n",
        "5. Run `source ~./bash_profile` to effectuate the changes and restart this jupyter notebook session.\n",
        "\n",
        "#### Alternative for using Google Collab\n",
        "\n",
        "If you are planning on using Google Colaboratory platform, run the following code cell to set up Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xqBDLHXy8q17",
        "outputId": "19b79c0a-f527-4855-d853-3037cc66bd2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "import os\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "!pip install pyspark"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n",
            "\u001b[K     |████████████████████████████████| 217.8MB 59kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 44.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257927 sha256=414cc27c56a645f610647697ea7551038b8005af50cba017e0b22a4232a9b1ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y4ItP9Ga8q27"
      },
      "source": [
        "### 1.1 Products Counting Example \n",
        "\n",
        "To test your installation and start to get familiarized with Spark, we will follow an example that counts how many times the products of a toy dataset were purchased.\n",
        "\n",
        "The main entry point to start programming with Spark is the [RDD API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD), an excellent Spark abstraction to work with the MapReduce framework.  RDD is a collection of elements partitioned across the nodes of the cluster that can operate in parallel. In other words, RDD is how Spark keeps your data ready to perform some function (e.g., a map or reduce function) in parallel. **Do not worry if this still sounds confusing, it will be clear once you start implementing**. However, it is part of this TP to study/consult the [Spark python API](https://spark.apache.org/docs/latest/api/python/) and learn how to use it. Some useful functions that the RDD API offers are:\n",
        "\n",
        "1. **map**: return a new RDD by applying a function to each element of this RDD.\n",
        "2. **flatMap**: return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. **Should be used when each entry will yield more than one mapped element**\n",
        "3. **reduce**: reduces the elements of this RDD using the specified commutative and associative binary operator.\n",
        "4. **reduceByKey**: merge the values for each key using an associative and commutative reduce function\n",
        "5. **groupByKey**: group the values for each key in the RDD into a single sequence\n",
        "6. **collect**: return a list that contains all of the elements in this RDD. **Should not be used when working with a lot of data**\n",
        "7. **sample**: return a sampled subset of this RDD\n",
        "8. **count**: return the number of elements in this RDD.\n",
        "9. **filter**: return a new RDD containing only the elements that satisfy a predicate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJTEm3HfYvZ-",
        "colab_type": "code",
        "outputId": "4f2bef33-f212-47d4-bad4-ee20c2cced61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNcEgLh_B9f2",
        "colab_type": "code",
        "outputId": "5f005ee2-5511-4b11-f224-ee5d9d83c8ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "! ls \"/content/gdrive/My Drive/Colab Notebooks/INF8111-TP2/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "instacart_2017_05_01  toy.csv\t tp2_KacemOumaymaSemah.ipynb\n",
            "Instructions_GCP.pdf  tp2.ipynb  workflow.svg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_ozEKZvx8q28",
        "outputId": "0c4c4a8c-8d8c-429d-a29a-f568511ed19c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def map_to_product(row):\n",
        "    \"\"\"\n",
        "    Map each transaction into a set of KEY-VALUE elements.\n",
        "    The KEY is the word (product) itself and the VALUE is its number of apparitions.\n",
        "    \"\"\"\n",
        "    products = row.transaction.split(';') # split products from the column transaction\n",
        "    for p in products:\n",
        "        yield (p, 1)\n",
        "\n",
        "def reduce_product_by_key(value1, value2):\n",
        "    \"Reduce the mapped objects to unique words by merging (summing ) their values\"\n",
        "    return value1+value2\n",
        "\n",
        "# Initializates a object of SparkSession class, main entry point to Spark's funcionalites\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "        \n",
        "# Read a toy dataset\n",
        "toy = spark.read.csv('/content/gdrive/My Drive/Colab Notebooks/INF8111-TP2/toy.csv', header=True)\n",
        "print(\"Toy dataset\")\n",
        "toy.show()\n",
        "\n",
        "# Obtain a RDD object to call a map function\n",
        "toy_rdd = toy.rdd\n",
        "print(\"Toy dataframe as a RDD object (list of Row objects):\\n\\t\", toy_rdd.collect())\n",
        "\n",
        "# Map function to identify all products\n",
        "toy_rdd = toy_rdd.flatMap(map_to_product)\n",
        "print(\"\\nMapped products:\\n\\t\", toy_rdd.collect())\n",
        "\n",
        "# Reduce function to merge values of elements that share the same KEY\n",
        "toy_rdd = toy_rdd.reduceByKey(reduce_product_by_key)\n",
        "print(\"\\nReduced (merged) products:\\n\\t\", toy_rdd.collect())\n",
        "\n",
        "print(\"\\nVisualizing as a dataframe:\")\n",
        "toy_rdd.toDF([\"product\", \"count_product\"]).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Toy dataset\n",
            "+--------+-----------+\n",
            "|order_id|transaction|\n",
            "+--------+-----------+\n",
            "|       1|    a;b;c;f|\n",
            "|       2|    d;b;a;e|\n",
            "|       3|        c;b|\n",
            "|       4|        b;c|\n",
            "+--------+-----------+\n",
            "\n",
            "Toy dataframe as a RDD object (list of Row objects):\n",
            "\t [Row(order_id='1', transaction='a;b;c;f'), Row(order_id='2', transaction='d;b;a;e'), Row(order_id='3', transaction='c;b'), Row(order_id='4', transaction='b;c')]\n",
            "\n",
            "Mapped products:\n",
            "\t [('a', 1), ('b', 1), ('c', 1), ('f', 1), ('d', 1), ('b', 1), ('a', 1), ('e', 1), ('c', 1), ('b', 1), ('b', 1), ('c', 1)]\n",
            "\n",
            "Reduced (merged) products:\n",
            "\t [('a', 2), ('b', 4), ('c', 3), ('f', 1), ('d', 1), ('e', 1)]\n",
            "\n",
            "Visualizing as a dataframe:\n",
            "+-------+-------------+\n",
            "|product|count_product|\n",
            "+-------+-------------+\n",
            "|      a|            2|\n",
            "|      b|            4|\n",
            "|      c|            3|\n",
            "|      f|            1|\n",
            "|      d|            1|\n",
            "|      e|            1|\n",
            "+-------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UqH2nKRI8q3N"
      },
      "source": [
        "### 1.2 Working with Spark's Dataframe\n",
        "\n",
        "In the example above, we briefly used a Spark's Dataframe class, but only to obtain an RDD object with ```toy.rdd``` and to print the data as a structured table with the ```show()``` function. However, [Dataframe](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#) is a crucial part of the current Spark release and is built upon the RDD API. It is a distributed collection of rows under named columns, the same as a table in a relational database. Spark's Dataframe works similarly as [Pandas'](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html). In fact, we can export (obtain) a Spark's dataframe to (from) a pandas' data frame with the function ```toPandas()``` (```spark.createDataFrame```).\n",
        "\n",
        "A central functionality of the data frame is to profit from the [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html#sql), a module that allows SQL queries over structured data. For example, the same 'product counting example' could have been implemented as a sequence of SQL operations over the data:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h68P1Pkr8q3N",
        "outputId": "47004d02-836a-4d12-9e9c-772ed9f82f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "import pyspark.sql.functions as f\n",
        "\n",
        "# Creates a new column, products, with all products appering in each transaction\n",
        "print('New column \\'products\\': exploding the transaction\\'s products to a new row')\n",
        "df_toy = toy.withColumn('products', f.explode(f.split(toy.transaction, ';')))\n",
        "df_toy.show()\n",
        "\n",
        "# Performs a select query and group rows by the product name, aggreagating by counting\n",
        "print('Couting unique products:')\n",
        "df_toy.select(df_toy.products)\\\n",
        "      .groupBy(df_toy.products)\\\n",
        "      .agg(f.count('products').alias('count_product'))\\\n",
        "      .sort('count_product', ascending=False)\\\n",
        "      .show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New column 'products': exploding the transaction's products to a new row\n",
            "+--------+-----------+--------+\n",
            "|order_id|transaction|products|\n",
            "+--------+-----------+--------+\n",
            "|       1|    a;b;c;f|       a|\n",
            "|       1|    a;b;c;f|       b|\n",
            "|       1|    a;b;c;f|       c|\n",
            "|       1|    a;b;c;f|       f|\n",
            "|       2|    d;b;a;e|       d|\n",
            "|       2|    d;b;a;e|       b|\n",
            "|       2|    d;b;a;e|       a|\n",
            "|       2|    d;b;a;e|       e|\n",
            "|       3|        c;b|       c|\n",
            "|       3|        c;b|       b|\n",
            "|       4|        b;c|       b|\n",
            "|       4|        b;c|       c|\n",
            "+--------+-----------+--------+\n",
            "\n",
            "Couting unique products:\n",
            "+--------+-------------+\n",
            "|products|count_product|\n",
            "+--------+-------------+\n",
            "|       b|            4|\n",
            "|       c|            3|\n",
            "|       a|            2|\n",
            "|       f|            1|\n",
            "|       e|            1|\n",
            "|       d|            1|\n",
            "+--------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F_IU_rf18q3S"
      },
      "source": [
        "Also, the same SQL operations performed above could have been done with a traditional SQL language query as showing below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c68l__Eb8q3S",
        "outputId": "c02be637-171b-4bc8-8e83-8df92574cece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "# Creates a relational table TOY in the Spark session\n",
        "df_toy.createOrReplaceTempView(\"TOY\")\n",
        "\n",
        "spark.sql(\"SELECT t.products, COUNT(t.products) AS product_count\"\n",
        "          \" FROM TOY t\"\n",
        "          \" GROUP BY t.products\"\n",
        "          \" ORDER BY product_count DESC\").show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------------+\n",
            "|products|product_count|\n",
            "+--------+-------------+\n",
            "|       b|            4|\n",
            "|       c|            3|\n",
            "|       a|            2|\n",
            "|       f|            1|\n",
            "|       e|            1|\n",
            "|       d|            1|\n",
            "+--------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8Ck1hRFC8q3U"
      },
      "source": [
        "These SQL concepts are being mentioned here because they will be useful to us during the TP, mainly in Section 3, to manipulate the supermarket data, which is structured in tables. Thus, if you are not familiar with SQL, it is recommended that you follow a [tutorial](https://www.w3schools.com/sql/) to understand the basics.\n",
        "\n",
        "## 2. MBA Algorithm \n",
        "The following sections explain how you should develop each step of the MapReduce algorithm for our supermarket application. Figure workflow.png illustrates each step of the algorithm.\n",
        "\n",
        "### 2.1 Map to Patterns (10 points)\n",
        "For a given set of transactions (i.e., the rows of our toy dataset), each transaction must be **mapped** into a set of *purchase patterns* found within the transaction. Formally, these patterns are subsets of products that represent a group of items bought together. \n",
        "\n",
        "For the MapReduce framework, each pattern must be created as a *KEY-VALUE* element, where the KEY can take the form of a singleton, a pair or a trio of products that are present in the transaction. More precisely, for each transaction, the mapping function must generate all possible **UNIQUE** subsets of size **ONE, TWO or THREE**.  The VALUE associated with each KEY is the number of times that the KEY appeared in the transaction (if we assume that no product appears more than once in the transaction, this value is always equal to one). \n",
        "\n",
        "Now, implement the **map_to_patterns** function that receives a transaction (a row from the data frame) and returns the patterns found in the transaction. The mapped elements are a tuple (KEY, VALUE), where KEY is also a tuple of product names. It is crucial to notice that, since each entry (transaction) of the map function will **yield** more than one KEY-VALUE element, a *flatMap* must be invoked for this step.\n",
        "\n",
        "For the toy dataset, the expected output is similar to:\n",
        "\n",
        "\n",
        "<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 20em; padding-left:1px\">\n",
        "<code>\n",
        "+---------------+-----------+\n",
        "|       patterns|occurrences|\n",
        "+---------------+-----------+\n",
        "|         ('a',)|          1|\n",
        "|     ('a', 'b')|          1|\n",
        "|('a', 'b', 'c')|          1|\n",
        "|('a', 'b', 'f')|          1|\n",
        "|     ('a', 'c')|          1|\n",
        "|('a', 'c', 'f')|          1|\n",
        "|     ('a', 'f')|          1|\n",
        "|         ('b',)|          1|\n",
        "|     ('b', 'c')|          1|\n",
        "|('b', 'c', 'f')|          1|\n",
        "|     ('b', 'f')|          1|\n",
        "|         ('c',)|          1|\n",
        "|     ('c', 'f')|          1|\n",
        "|         ('f',)|          1|\n",
        "|         ('a',)|          1|\n",
        "|     ('a', 'b')|          1|\n",
        "|('a', 'b', 'd')|          1|\n",
        "|('a', 'b', 'e')|          1|\n",
        "|     ('a', 'd')|          1|\n",
        "|('a', 'd', 'e')|          1|\n",
        "|     ('a', 'e')|          1|\n",
        "|         ('b',)|          1|\n",
        "|     ('b', 'd')|          1|\n",
        "|('b', 'd', 'e')|          1|\n",
        "|     ('b', 'e')|          1|\n",
        "|         ('d',)|          1|\n",
        "|     ('d', 'e')|          1|\n",
        "|         ('e',)|          1|\n",
        "|         ('b',)|          1|\n",
        "|     ('b', 'c')|          1|\n",
        "|         ('c',)|          1|\n",
        "|         ('b',)|          1|\n",
        "|     ('b', 'c')|          1|\n",
        "|         ('c',)|          1|\n",
        "+---------------+-----------+\n",
        "</code>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XgcZFavR8q3U",
        "outputId": "370edd56-b5eb-4afa-b38e-fd5d9319a5a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        }
      },
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def format_tuples(pattern):\n",
        "    \"\"\"\n",
        "    Used for visualizition.\n",
        "    Transforms tuples to a string since Dataframe does not support column of tuples with different sizes\n",
        "    (a,b,c) -> '(a,b,c)'\n",
        "    \"\"\"\n",
        "    return (str(pattern[0]), str(pattern[1]))\n",
        "\n",
        "def map_to_patterns(row):\n",
        "    \n",
        "    to_yield = []\n",
        "    products = row.transaction.split(';')\n",
        "    for i in range(1, 4):\n",
        "        to_yield.extend(list(combinations(sorted(products), i)))\n",
        "    \n",
        "    for pattern in to_yield:\n",
        "        yield (pattern, 1)\n",
        "    \n",
        "\n",
        "toy_rdd = toy.rdd\n",
        "patterns_rdd = toy_rdd.flatMap(map_to_patterns)\n",
        "\n",
        "# Output as dataframe\n",
        "patterns_rdd.map(format_tuples).toDF(['patterns', 'occurrences']).show(150,truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+-----------+\n",
            "|patterns       |occurrences|\n",
            "+---------------+-----------+\n",
            "|('a',)         |1          |\n",
            "|('b',)         |1          |\n",
            "|('c',)         |1          |\n",
            "|('f',)         |1          |\n",
            "|('a', 'b')     |1          |\n",
            "|('a', 'c')     |1          |\n",
            "|('a', 'f')     |1          |\n",
            "|('b', 'c')     |1          |\n",
            "|('b', 'f')     |1          |\n",
            "|('c', 'f')     |1          |\n",
            "|('a', 'b', 'c')|1          |\n",
            "|('a', 'b', 'f')|1          |\n",
            "|('a', 'c', 'f')|1          |\n",
            "|('b', 'c', 'f')|1          |\n",
            "|('a',)         |1          |\n",
            "|('b',)         |1          |\n",
            "|('d',)         |1          |\n",
            "|('e',)         |1          |\n",
            "|('a', 'b')     |1          |\n",
            "|('a', 'd')     |1          |\n",
            "|('a', 'e')     |1          |\n",
            "|('b', 'd')     |1          |\n",
            "|('b', 'e')     |1          |\n",
            "|('d', 'e')     |1          |\n",
            "|('a', 'b', 'd')|1          |\n",
            "|('a', 'b', 'e')|1          |\n",
            "|('a', 'd', 'e')|1          |\n",
            "|('b', 'd', 'e')|1          |\n",
            "|('b',)         |1          |\n",
            "|('c',)         |1          |\n",
            "|('b', 'c')     |1          |\n",
            "|('b',)         |1          |\n",
            "|('c',)         |1          |\n",
            "|('b', 'c')     |1          |\n",
            "+---------------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "djN0Eb-68q3W"
      },
      "source": [
        "### 2.2 Reduce patterns (2.5 points)\n",
        "Once different CPUs processed the transactions, a **reduce** function must take place to combine identical KEYS (the subset of products) and compute the total number of its occurrences in the entire dataset. In other words, this reduce procedure must sum the *VALUE* of each identical KEY.\n",
        "\n",
        "Create a **reduce_patterns** function below that must sum the VALUE of each pattern.\n",
        "For the toy dataset, the expected output is:\n",
        "<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 28em; padding-left:5px\">\n",
        "<code>\n",
        "+---------------+--------------------+\n",
        "|       patterns|combined_occurrences|\n",
        "+---------------+--------------------+\n",
        "|         ('a',)|                   2|\n",
        "|     ('a', 'b')|                   2|\n",
        "|('a', 'b', 'c')|                   1|\n",
        "|('a', 'b', 'f')|                   1|\n",
        "|     ('a', 'c')|                   1|\n",
        "|('a', 'c', 'f')|                   1|\n",
        "|     ('a', 'f')|                   1|\n",
        "|         ('b',)|                   4|\n",
        "|     ('b', 'c')|                   3|\n",
        "|('b', 'c', 'f')|                   1|\n",
        "|     ('b', 'f')|                   1|\n",
        "|         ('c',)|                   3|\n",
        "|     ('c', 'f')|                   1|\n",
        "|         ('f',)|                   1|\n",
        "|('a', 'b', 'd')|                   1|\n",
        "|('a', 'b', 'e')|                   1|\n",
        "|     ('a', 'd')|                   1|\n",
        "|('a', 'd', 'e')|                   1|\n",
        "|     ('a', 'e')|                   1|\n",
        "|     ('b', 'd')|                   1|\n",
        "|('b', 'd', 'e')|                   1|\n",
        "|     ('b', 'e')|                   1|\n",
        "|         ('d',)|                   1|\n",
        "|     ('d', 'e')|                   1|\n",
        "|         ('e',)|                   1|\n",
        "+---------------+--------------------+\n",
        "</code>\n",
        "</pre>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VHA6boqv8q3W",
        "outputId": "98e39dd8-0499-4e90-9e84-c31066f28192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "def reduce_patterns(value1, value2):\n",
        "    return value1+value2\n",
        "\n",
        "combined_patterns_rdd = patterns_rdd.reduceByKey(reduce_patterns)\n",
        "\n",
        "# Output as dataframe\n",
        "combined_patterns_rdd.map(format_tuples).toDF(['patterns', 'combined_occurrences']).show(150,truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+--------------------+\n",
            "|patterns       |combined_occurrences|\n",
            "+---------------+--------------------+\n",
            "|('a',)         |2                   |\n",
            "|('b',)         |4                   |\n",
            "|('c',)         |3                   |\n",
            "|('f',)         |1                   |\n",
            "|('a', 'b')     |2                   |\n",
            "|('a', 'c')     |1                   |\n",
            "|('a', 'f')     |1                   |\n",
            "|('b', 'c')     |3                   |\n",
            "|('b', 'f')     |1                   |\n",
            "|('c', 'f')     |1                   |\n",
            "|('a', 'b', 'c')|1                   |\n",
            "|('a', 'b', 'f')|1                   |\n",
            "|('a', 'c', 'f')|1                   |\n",
            "|('b', 'c', 'f')|1                   |\n",
            "|('d',)         |1                   |\n",
            "|('e',)         |1                   |\n",
            "|('a', 'd')     |1                   |\n",
            "|('a', 'e')     |1                   |\n",
            "|('b', 'd')     |1                   |\n",
            "|('b', 'e')     |1                   |\n",
            "|('d', 'e')     |1                   |\n",
            "|('a', 'b', 'd')|1                   |\n",
            "|('a', 'b', 'e')|1                   |\n",
            "|('a', 'd', 'e')|1                   |\n",
            "|('b', 'd', 'e')|1                   |\n",
            "+---------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PvHh4-RR8q3Y"
      },
      "source": [
        "### 2.3 Map to subpatterns (15 points)\n",
        "Next, another **map** function should be applied to generate subpatterns. Once again, the subpatterns are KEY-VALUE elements, where the KEY is a subset of products as well. However, creating the subpattern's KEY is a different procedure. This time, the idea is to break down the list of products of each pattern (pattern KEY), remove one product at a time, and yield the resulting list as the new subpattern KEY. \n",
        "\n",
        "For example, for a given pattern $P$ with three products, $p_1, p_2 $ and $p_3$, three new subpatterns KEYs are going to be created: (i) remove $p_1$ and yield ($p_2, p_3$); (ii) remove $p_2$ and yield ($p_1,p_3$); and (iii) remove $p_3$ and yield ($p_1,p_2$). \n",
        "\n",
        "Additionally, the subpattern's VALUE structure will also be different. Instead of just single integer value as we had in the patterns, this time a *tuple* should be created for the subpattern VALUE. This tuple contains the product that was removed when yielding the KEY and the number of times the pattern appeared. For example above, the values should be ($p_1,v$), ($p_2,v$) and ($p_3,v$), respectively, where $v$ is the VALUE of the pattern. \n",
        "\n",
        "The idea behind subpatterns is to create **rules** such as: when the products of KEY were bought, the item present in the VALUE was also bought *v* times. Furthermore, each pattern should also yield a subpattern where the KEY is the same list of products of the pattern, but the VALUE is a tuple with a null product (None) and the number of times the pattern appeared. This element will be useful to keep track of how many times such a pattern was found and later will be used to compute the confidence value when generating the association rules. \n",
        "\n",
        "Now, implement the  **map_to_subpatterns** function that receives a pattern and yields all found subpatterns. Once again, each entry (pattern) will generate more than one KEY-VALUE element, then a flatMap function must be called.\n",
        "\n",
        "For the toy dataset, the expected output is:\n",
        "\n",
        "<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 20em; padding-left:5px\">\n",
        "<code>\n",
        "+---------------+---------+\n",
        "|    subpatterns|    rules|\n",
        "+---------------+---------+\n",
        "|         ('a',)|(None, 2)|\n",
        "|     ('a', 'b')|(None, 2)|\n",
        "|         ('b',)| ('a', 2)|\n",
        "|         ('a',)| ('b', 2)|\n",
        "|('a', 'b', 'c')|(None, 1)|\n",
        "|     ('b', 'c')| ('a', 1)|\n",
        "|     ('a', 'c')| ('b', 1)|\n",
        "|     ('a', 'b')| ('c', 1)|\n",
        "|('a', 'b', 'f')|(None, 1)|\n",
        "|     ('b', 'f')| ('a', 1)|\n",
        "|     ('a', 'f')| ('b', 1)|\n",
        "|     ('a', 'b')| ('f', 1)|\n",
        "|     ('a', 'c')|(None, 1)|\n",
        "|         ('c',)| ('a', 1)|\n",
        "|         ('a',)| ('c', 1)|\n",
        "|('a', 'c', 'f')|(None, 1)|\n",
        "|     ('c', 'f')| ('a', 1)|\n",
        "|     ('a', 'f')| ('c', 1)|\n",
        "|     ('a', 'c')| ('f', 1)|\n",
        "|     ('a', 'f')|(None, 1)|\n",
        "|         ('f',)| ('a', 1)|\n",
        "|         ('a',)| ('f', 1)|\n",
        "|         ('b',)|(None, 4)|\n",
        "|     ('b', 'c')|(None, 3)|\n",
        "|         ('c',)| ('b', 3)|\n",
        "|         ('b',)| ('c', 3)|\n",
        "|('b', 'c', 'f')|(None, 1)|\n",
        "|     ('c', 'f')| ('b', 1)|\n",
        "|     ('b', 'f')| ('c', 1)|\n",
        "|     ('b', 'c')| ('f', 1)|\n",
        "|     ('b', 'f')|(None, 1)|\n",
        "|         ('f',)| ('b', 1)|\n",
        "|         ('b',)| ('f', 1)|\n",
        "|         ('c',)|(None, 3)|\n",
        "|     ('c', 'f')|(None, 1)|\n",
        "|         ('f',)| ('c', 1)|\n",
        "|         ('c',)| ('f', 1)|\n",
        "|         ('f',)|(None, 1)|\n",
        "|('a', 'b', 'd')|(None, 1)|\n",
        "|     ('b', 'd')| ('a', 1)|\n",
        "|     ('a', 'd')| ('b', 1)|\n",
        "|     ('a', 'b')| ('d', 1)|\n",
        "|('a', 'b', 'e')|(None, 1)|\n",
        "|     ('b', 'e')| ('a', 1)|\n",
        "|     ('a', 'e')| ('b', 1)|\n",
        "|     ('a', 'b')| ('e', 1)|\n",
        "|     ('a', 'd')|(None, 1)|\n",
        "|         ('d',)| ('a', 1)|\n",
        "|         ('a',)| ('d', 1)|\n",
        "|('a', 'd', 'e')|(None, 1)|\n",
        "|     ('d', 'e')| ('a', 1)|\n",
        "|     ('a', 'e')| ('d', 1)|\n",
        "|     ('a', 'd')| ('e', 1)|\n",
        "|     ('a', 'e')|(None, 1)|\n",
        "|         ('e',)| ('a', 1)|\n",
        "|         ('a',)| ('e', 1)|\n",
        "|     ('b', 'd')|(None, 1)|\n",
        "|         ('d',)| ('b', 1)|\n",
        "|         ('b',)| ('d', 1)|\n",
        "|('b', 'd', 'e')|(None, 1)|\n",
        "|     ('d', 'e')| ('b', 1)|\n",
        "|     ('b', 'e')| ('d', 1)|\n",
        "|     ('b', 'd')| ('e', 1)|\n",
        "|     ('b', 'e')|(None, 1)|\n",
        "|         ('e',)| ('b', 1)|\n",
        "|         ('b',)| ('e', 1)|\n",
        "|         ('d',)|(None, 1)|\n",
        "|     ('d', 'e')|(None, 1)|\n",
        "|         ('e',)| ('d', 1)|\n",
        "|         ('d',)| ('e', 1)|\n",
        "|         ('e',)|(None, 1)|\n",
        "+---------------+---------+\n",
        "</code>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iAzW7ExI8q3Y",
        "outputId": "9f607c91-b611-4630-93c0-8b7ae81816c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from copy import deepcopy\n",
        "def map_to_subpatterns(pattern):\n",
        "    key = pattern[0]\n",
        "    value = pattern[1]\n",
        "\n",
        "    yield (key, tuple((None, value)))\n",
        "    if len(key) != 1:\n",
        "        tmp = deepcopy(key)\n",
        "        for i, elem in enumerate(tmp):\n",
        "            sub = tmp[0:i]+tmp[i+1:]\n",
        "            yield (sub, tuple((elem, value)))\n",
        "\n",
        "subpatterns_rdd = combined_patterns_rdd.flatMap(map_to_subpatterns)\n",
        "\n",
        "# Output as dataframe\n",
        "subpatterns_rdd.map(format_tuples).toDF(['subpatterns', 'rules']).show(150, truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+---------+\n",
            "|subpatterns    |rules    |\n",
            "+---------------+---------+\n",
            "|('a',)         |(None, 2)|\n",
            "|('b',)         |(None, 4)|\n",
            "|('c',)         |(None, 3)|\n",
            "|('f',)         |(None, 1)|\n",
            "|('a', 'b')     |(None, 2)|\n",
            "|('b',)         |('a', 2) |\n",
            "|('a',)         |('b', 2) |\n",
            "|('a', 'c')     |(None, 1)|\n",
            "|('c',)         |('a', 1) |\n",
            "|('a',)         |('c', 1) |\n",
            "|('a', 'f')     |(None, 1)|\n",
            "|('f',)         |('a', 1) |\n",
            "|('a',)         |('f', 1) |\n",
            "|('b', 'c')     |(None, 3)|\n",
            "|('c',)         |('b', 3) |\n",
            "|('b',)         |('c', 3) |\n",
            "|('b', 'f')     |(None, 1)|\n",
            "|('f',)         |('b', 1) |\n",
            "|('b',)         |('f', 1) |\n",
            "|('c', 'f')     |(None, 1)|\n",
            "|('f',)         |('c', 1) |\n",
            "|('c',)         |('f', 1) |\n",
            "|('a', 'b', 'c')|(None, 1)|\n",
            "|('b', 'c')     |('a', 1) |\n",
            "|('a', 'c')     |('b', 1) |\n",
            "|('a', 'b')     |('c', 1) |\n",
            "|('a', 'b', 'f')|(None, 1)|\n",
            "|('b', 'f')     |('a', 1) |\n",
            "|('a', 'f')     |('b', 1) |\n",
            "|('a', 'b')     |('f', 1) |\n",
            "|('a', 'c', 'f')|(None, 1)|\n",
            "|('c', 'f')     |('a', 1) |\n",
            "|('a', 'f')     |('c', 1) |\n",
            "|('a', 'c')     |('f', 1) |\n",
            "|('b', 'c', 'f')|(None, 1)|\n",
            "|('c', 'f')     |('b', 1) |\n",
            "|('b', 'f')     |('c', 1) |\n",
            "|('b', 'c')     |('f', 1) |\n",
            "|('d',)         |(None, 1)|\n",
            "|('e',)         |(None, 1)|\n",
            "|('a', 'd')     |(None, 1)|\n",
            "|('d',)         |('a', 1) |\n",
            "|('a',)         |('d', 1) |\n",
            "|('a', 'e')     |(None, 1)|\n",
            "|('e',)         |('a', 1) |\n",
            "|('a',)         |('e', 1) |\n",
            "|('b', 'd')     |(None, 1)|\n",
            "|('d',)         |('b', 1) |\n",
            "|('b',)         |('d', 1) |\n",
            "|('b', 'e')     |(None, 1)|\n",
            "|('e',)         |('b', 1) |\n",
            "|('b',)         |('e', 1) |\n",
            "|('d', 'e')     |(None, 1)|\n",
            "|('e',)         |('d', 1) |\n",
            "|('d',)         |('e', 1) |\n",
            "|('a', 'b', 'd')|(None, 1)|\n",
            "|('b', 'd')     |('a', 1) |\n",
            "|('a', 'd')     |('b', 1) |\n",
            "|('a', 'b')     |('d', 1) |\n",
            "|('a', 'b', 'e')|(None, 1)|\n",
            "|('b', 'e')     |('a', 1) |\n",
            "|('a', 'e')     |('b', 1) |\n",
            "|('a', 'b')     |('e', 1) |\n",
            "|('a', 'd', 'e')|(None, 1)|\n",
            "|('d', 'e')     |('a', 1) |\n",
            "|('a', 'e')     |('d', 1) |\n",
            "|('a', 'd')     |('e', 1) |\n",
            "|('b', 'd', 'e')|(None, 1)|\n",
            "|('d', 'e')     |('b', 1) |\n",
            "|('b', 'e')     |('d', 1) |\n",
            "|('b', 'd')     |('e', 1) |\n",
            "+---------------+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HtiAJVMi8q3a"
      },
      "source": [
        "### 2.4 Reduce Subpatterns (2.5 points)\n",
        "Once again, a **reduce** function will be required to group all the subpatterns by their KEY. The objective of this reducing procedure is to create a list of all **rules** that appeared in KEY. Hence, the expected output resulting from this reduce function is also a KEY-VALUE element, where the KEY is the subpattern's KEY, and the VALUE is a group containing all the VALUEs of the subpatterns that share the same KEY.\n",
        "\n",
        "For the toy dataset, the expected output is:\n",
        "\n",
        "\n",
        "<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 50em; padding-left:5px\">\n",
        "<code>\n",
        "+---------------+-------------------------------------------------------------+\n",
        "|subpatterns    |combined_rules                                               |\n",
        "+---------------+-------------------------------------------------------------+\n",
        "|('a',)         |[(None, 2), ('b', 2), ('c', 1), ('f', 1), ('d', 1), ('e', 1)]|\n",
        "|('a', 'b')     |[(None, 2), ('c', 1), ('f', 1), ('d', 1), ('e', 1)]          |\n",
        "|('b',)         |[('a', 2), (None, 4), ('c', 3), ('f', 1), ('d', 1), ('e', 1)]|\n",
        "|('a', 'b', 'c')|[(None, 1)]                                                  |\n",
        "|('b', 'c')     |[('a', 1), (None, 3), ('f', 1)]                              |\n",
        "|('a', 'c')     |[('b', 1), (None, 1), ('f', 1)]                              |\n",
        "|('a', 'b', 'f')|[(None, 1)]                                                  |\n",
        "|('b', 'f')     |[('a', 1), ('c', 1), (None, 1)]                              |\n",
        "|('a', 'f')     |[('b', 1), ('c', 1), (None, 1)]                              |\n",
        "|('c',)         |[('a', 1), ('b', 3), (None, 3), ('f', 1)]                    |\n",
        "|('a', 'c', 'f')|[(None, 1)]                                                  |\n",
        "|('c', 'f')     |[('a', 1), ('b', 1), (None, 1)]                              |\n",
        "|('f',)         |[('a', 1), ('b', 1), ('c', 1), (None, 1)]                    |\n",
        "|('b', 'c', 'f')|[(None, 1)]                                                  |\n",
        "|('a', 'b', 'd')|[(None, 1)]                                                  |\n",
        "|('b', 'd')     |[('a', 1), (None, 1), ('e', 1)]                              |\n",
        "|('a', 'd')     |[('b', 1), (None, 1), ('e', 1)]                              |\n",
        "|('a', 'b', 'e')|[(None, 1)]                                                  |\n",
        "|('b', 'e')     |[('a', 1), ('d', 1), (None, 1)]                              |\n",
        "|('a', 'e')     |[('b', 1), ('d', 1), (None, 1)]                              |\n",
        "+---------------+-------------------------------------------------------------+\n",
        "</code>\n",
        "</pre>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2rI4Q9Ok8q3a",
        "outputId": "b64b127a-6788-4404-c154-1dafc7a15090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "def list_map(pattern):\n",
        "    return (pattern[0], list(pattern[1]))\n",
        "\n",
        "combined_rules = subpatterns_rdd.groupByKey().map(list_map) # Try with .map((x,y) => (x,list(y)))\n",
        "\n",
        "# Output as dataframe\n",
        "combined_rules.map(format_tuples).toDF(['subpatterns', 'combined_rules']).show(150, truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+-------------------------------------------------------------+\n",
            "|subpatterns    |combined_rules                                               |\n",
            "+---------------+-------------------------------------------------------------+\n",
            "|('a',)         |[(None, 2), ('b', 2), ('c', 1), ('f', 1), ('d', 1), ('e', 1)]|\n",
            "|('b',)         |[(None, 4), ('a', 2), ('c', 3), ('f', 1), ('d', 1), ('e', 1)]|\n",
            "|('c',)         |[(None, 3), ('a', 1), ('b', 3), ('f', 1)]                    |\n",
            "|('f',)         |[(None, 1), ('a', 1), ('b', 1), ('c', 1)]                    |\n",
            "|('a', 'b')     |[(None, 2), ('c', 1), ('f', 1), ('d', 1), ('e', 1)]          |\n",
            "|('a', 'c')     |[(None, 1), ('b', 1), ('f', 1)]                              |\n",
            "|('a', 'f')     |[(None, 1), ('b', 1), ('c', 1)]                              |\n",
            "|('b', 'c')     |[(None, 3), ('a', 1), ('f', 1)]                              |\n",
            "|('b', 'f')     |[(None, 1), ('a', 1), ('c', 1)]                              |\n",
            "|('c', 'f')     |[(None, 1), ('a', 1), ('b', 1)]                              |\n",
            "|('a', 'b', 'c')|[(None, 1)]                                                  |\n",
            "|('a', 'b', 'f')|[(None, 1)]                                                  |\n",
            "|('a', 'c', 'f')|[(None, 1)]                                                  |\n",
            "|('b', 'c', 'f')|[(None, 1)]                                                  |\n",
            "|('d',)         |[(None, 1), ('a', 1), ('b', 1), ('e', 1)]                    |\n",
            "|('e',)         |[(None, 1), ('a', 1), ('b', 1), ('d', 1)]                    |\n",
            "|('a', 'd')     |[(None, 1), ('b', 1), ('e', 1)]                              |\n",
            "|('a', 'e')     |[(None, 1), ('b', 1), ('d', 1)]                              |\n",
            "|('b', 'd')     |[(None, 1), ('a', 1), ('e', 1)]                              |\n",
            "|('b', 'e')     |[(None, 1), ('a', 1), ('d', 1)]                              |\n",
            "|('d', 'e')     |[(None, 1), ('a', 1), ('b', 1)]                              |\n",
            "|('a', 'b', 'd')|[(None, 1)]                                                  |\n",
            "|('a', 'b', 'e')|[(None, 1)]                                                  |\n",
            "|('a', 'd', 'e')|[(None, 1)]                                                  |\n",
            "|('b', 'd', 'e')|[(None, 1)]                                                  |\n",
            "+---------------+-------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VUSqO2F78q3c"
      },
      "source": [
        "## 2.5. Map to Association Rules (15 points)\n",
        "Finally, the last step of the algorithm is to create the association rules to perform the market basket analysis. The goal of this map function is to calculate the **confidence** level of buying a product, knowing that there is already a set of products in the basket. Thus, the KEY of the subpattern is the set of products placed in the basket and, for each product present in the list of rules, i.e., in the VALUE, the confidence can be calculated as:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\text{number of times the product was bought together with KEY }}{\\text{number of times the KEY appeared}}\n",
        "\\end{align*}\n",
        "\n",
        "For the example given in the Figure workflow, *coffee* was bought 20 times and, in 17 of them, *milk* was bought together. Then, the confidence level of buying *milk* knowing that *coffee* is in the basket is $\\frac{17}{20} = 0.85$, which means that in 85% of the times the coffee was bought, milk was purchased as well.\n",
        "\n",
        "Implement the **map_to_assoc_rules** function that calculates the confidence level for each subpattern.\n",
        "\n",
        "For the toy dataset, the expected output is:\n",
        "<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 57em; padding-left:5px\">\n",
        "<code>\n",
        "+---------------+------------------------------------------------------------------+\n",
        "|patterns       |association_rules                                                 |\n",
        "+---------------+------------------------------------------------------------------+\n",
        "|('a',)         |[('b', 1.0), ('c', 0.5), ('f', 0.5), ('d', 0.5), ('e', 0.5)]      |\n",
        "|('a', 'b')     |[('c', 0.5), ('f', 0.5), ('d', 0.5), ('e', 0.5)]                  |\n",
        "|('b',)         |[('a', 0.5), ('c', 0.75), ('f', 0.25), ('d', 0.25), ('e', 0.25)]  |\n",
        "|('a', 'b', 'c')|[]                                                                |\n",
        "|('b', 'c')     |[('a', 0.3333333333333333), ('f', 0.3333333333333333)]            |\n",
        "|('a', 'c')     |[('b', 1.0), ('f', 1.0)]                                          |\n",
        "|('a', 'b', 'f')|[]                                                                |\n",
        "|('b', 'f')     |[('a', 1.0), ('c', 1.0)]                                          |\n",
        "|('a', 'f')     |[('b', 1.0), ('c', 1.0)]                                          |\n",
        "|('c',)         |[('a', 0.3333333333333333), ('b', 1.0), ('f', 0.3333333333333333)]|\n",
        "|('a', 'c', 'f')|[]                                                                |\n",
        "|('c', 'f')     |[('a', 1.0), ('b', 1.0)]                                          |\n",
        "|('f',)         |[('a', 1.0), ('b', 1.0), ('c', 1.0)]                              |\n",
        "|('b', 'c', 'f')|[]                                                                |\n",
        "|('a', 'b', 'd')|[]                                                                |\n",
        "|('b', 'd')     |[('a', 1.0), ('e', 1.0)]                                          |\n",
        "|('a', 'd')     |[('b', 1.0), ('e', 1.0)]                                          |\n",
        "|('a', 'b', 'e')|[]                                                                |\n",
        "|('b', 'e')     |[('a', 1.0), ('d', 1.0)]                                          |\n",
        "|('a', 'e')     |[('b', 1.0), ('d', 1.0)]                                          |\n",
        "+---------------+------------------------------------------------------------------+\n",
        "</code>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4afeIBQQ8q3d",
        "outputId": "ac3a80d2-a98b-43c7-d061-14a2aa3521f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "def map_to_assoc_rules(rule):\n",
        "    tmp = deepcopy(rule)\n",
        "    key = tmp[0]\n",
        "    values = tmp[1]\n",
        "    confidences = []\n",
        "    denom = values[0][1]\n",
        "    if len(values) == 1:\n",
        "        yield (key, [])\n",
        "    else:\n",
        "        for elem in values[1:]:\n",
        "            confidences.append(tuple((elem[0], elem[1]/denom)))\n",
        "        yield (key, confidences)\n",
        "\n",
        "assoc_rules = combined_rules.flatMap(map_to_assoc_rules)\n",
        "\n",
        "# Output as dataframe\n",
        "assoc_rules.map(format_tuples).toDF(['patterns', 'association_rules']).show(150, truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+------------------------------------------------------------------+\n",
            "|patterns       |association_rules                                                 |\n",
            "+---------------+------------------------------------------------------------------+\n",
            "|('a',)         |[('b', 1.0), ('c', 0.5), ('f', 0.5), ('d', 0.5), ('e', 0.5)]      |\n",
            "|('b',)         |[('a', 0.5), ('c', 0.75), ('f', 0.25), ('d', 0.25), ('e', 0.25)]  |\n",
            "|('c',)         |[('a', 0.3333333333333333), ('b', 1.0), ('f', 0.3333333333333333)]|\n",
            "|('f',)         |[('a', 1.0), ('b', 1.0), ('c', 1.0)]                              |\n",
            "|('a', 'b')     |[('c', 0.5), ('f', 0.5), ('d', 0.5), ('e', 0.5)]                  |\n",
            "|('a', 'c')     |[('b', 1.0), ('f', 1.0)]                                          |\n",
            "|('a', 'f')     |[('b', 1.0), ('c', 1.0)]                                          |\n",
            "|('b', 'c')     |[('a', 0.3333333333333333), ('f', 0.3333333333333333)]            |\n",
            "|('b', 'f')     |[('a', 1.0), ('c', 1.0)]                                          |\n",
            "|('c', 'f')     |[('a', 1.0), ('b', 1.0)]                                          |\n",
            "|('a', 'b', 'c')|[]                                                                |\n",
            "|('a', 'b', 'f')|[]                                                                |\n",
            "|('a', 'c', 'f')|[]                                                                |\n",
            "|('b', 'c', 'f')|[]                                                                |\n",
            "|('d',)         |[('a', 1.0), ('b', 1.0), ('e', 1.0)]                              |\n",
            "|('e',)         |[('a', 1.0), ('b', 1.0), ('d', 1.0)]                              |\n",
            "|('a', 'd')     |[('b', 1.0), ('e', 1.0)]                                          |\n",
            "|('a', 'e')     |[('b', 1.0), ('d', 1.0)]                                          |\n",
            "|('b', 'd')     |[('a', 1.0), ('e', 1.0)]                                          |\n",
            "|('b', 'e')     |[('a', 1.0), ('d', 1.0)]                                          |\n",
            "|('d', 'e')     |[('a', 1.0), ('b', 1.0)]                                          |\n",
            "|('a', 'b', 'd')|[]                                                                |\n",
            "|('a', 'b', 'e')|[]                                                                |\n",
            "|('a', 'd', 'e')|[]                                                                |\n",
            "|('b', 'd', 'e')|[]                                                                |\n",
            "+---------------+------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zKfTQs2U8q3f"
      },
      "source": [
        "## 3. Instacart dataset\n",
        "\n",
        "With your MBA algorithm ready to be used, now it is time to work on the real dataset. For this part of the TP, download the [instacart](https://www.instacart.com/datasets/grocery-shopping-2017) dataset and read its [description](https://gist.github.com/jeremystan/c3b39d947d9b88b3ccff3147dbcf6c6b) to understand how the dataset is structured. \n",
        "\n",
        "Before applying the developed algorithm on the instacart dataset, you must first filter the transactions to be in the same format defined by your algorithm (one transaction per row). To manipulate the data, we can use Spark's data frame and the SQL module presented in Section 1.\n",
        "\n",
        "The following code cell uses the Spark SQL module to read the orders from the ``order_products__train.csv`` and the detailed information from ``orders.csv`` and ``products.csv`` to construct a data frame that contains a list of all products ever purchased by each user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDV_eegD4N4k",
        "colab_type": "code",
        "outputId": "1eec8250-27ce-4cc4-8b7c-8573d5c2eeb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "! ls \"/content/gdrive/My Drive/Colab Notebooks/INF8111-TP2/instacart_2017_05_01\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aisles.csv\t order_products__prior.csv  orders.csv\n",
            "departments.csv  order_products__train.csv  products.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J7Sel7Mm8q3f",
        "outputId": "94f862ec-afd9-4be2-fbf3-facb582d2b38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "instacart = \"/content/gdrive/My Drive/Colab Notebooks/INF8111-TP2/instacart_2017_05_01\"\n",
        "\n",
        "df_order_prod = spark.read.csv(instacart+'/order_products__train.csv', header=True, sep=',', inferSchema=True)\n",
        "print('order_products__train.csv')\n",
        "df_order_prod.show(5)\n",
        "\n",
        "df_orders = spark.read.csv(instacart+'/orders.csv', header=True, sep=',', inferSchema=True)\n",
        "print('orders.csv')\n",
        "df_orders.show(5)\n",
        "\n",
        "df_products = spark.read.csv(instacart+'/products.csv', header=True, sep=',', inferSchema=True)\n",
        "print('products.csv')\n",
        "df_products.show(5)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "List of products ever purchased by each user\n",
        "\"\"\"\n",
        "# USING SQL\n",
        "df_order_prod.createOrReplaceTempView(\"order_prod\") # creates table 'order_prod'\n",
        "df_orders.createOrReplaceTempView(\"orders\") # creates table 'orders'\n",
        "df_products.createOrReplaceTempView(\"products\") # creates table 'products'\n",
        "spark.sql('SELECT o.user_id, COLLECT_LIST(p.product_name) AS products' \n",
        "               ' FROM orders o '\n",
        "               ' INNER JOIN order_prod op ON op.order_id = o.order_id'\n",
        "               ' INNER JOIN products p    ON op.product_id = p.product_id'\n",
        "               ' GROUP BY user_id ORDER BY o.user_id').show(5, truncate=80)\n",
        "\n",
        "\n",
        "# USING DATAFRAME OPERATIONS\n",
        "# df_orders.join(df_order_prod, df_order_prod.order_id == df_orders.order_id, 'inner')\\\n",
        "# .join(df_products, df_products.product_id == df_order_prod.product_id, 'inner')\\\n",
        "# .groupBy(df_orders.user_id).agg(f.collect_list(df_products.product_name).alias('products'))\\\n",
        "# .orderBy(df_orders.user_id).show(5, truncate=80)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "order_products__train.csv\n",
            "+--------+----------+-----------------+---------+\n",
            "|order_id|product_id|add_to_cart_order|reordered|\n",
            "+--------+----------+-----------------+---------+\n",
            "|       1|     49302|                1|        1|\n",
            "|       1|     11109|                2|        1|\n",
            "|       1|     10246|                3|        0|\n",
            "|       1|     49683|                4|        0|\n",
            "|       1|     43633|                5|        1|\n",
            "+--------+----------+-----------------+---------+\n",
            "only showing top 5 rows\n",
            "\n",
            "orders.csv\n",
            "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
            "|order_id|user_id|eval_set|order_number|order_dow|order_hour_of_day|days_since_prior_order|\n",
            "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
            "| 2539329|      1|   prior|           1|        2|                8|                  null|\n",
            "| 2398795|      1|   prior|           2|        3|                7|                  15.0|\n",
            "|  473747|      1|   prior|           3|        3|               12|                  21.0|\n",
            "| 2254736|      1|   prior|           4|        4|                7|                  29.0|\n",
            "|  431534|      1|   prior|           5|        4|               15|                  28.0|\n",
            "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "products.csv\n",
            "+----------+--------------------+--------+-------------+\n",
            "|product_id|        product_name|aisle_id|department_id|\n",
            "+----------+--------------------+--------+-------------+\n",
            "|         1|Chocolate Sandwic...|      61|           19|\n",
            "|         2|    All-Seasons Salt|     104|           13|\n",
            "|         3|Robust Golden Uns...|      94|            7|\n",
            "|         4|Smart Ones Classi...|      38|            1|\n",
            "|         5|Green Chile Anyti...|       5|           13|\n",
            "+----------+--------------------+--------+-------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------+--------------------------------------------------------------------------------+\n",
            "|user_id|                                                                        products|\n",
            "+-------+--------------------------------------------------------------------------------+\n",
            "|      1|[Soda, Organic String Cheese, 0% Greek Strained Yogurt, XL Pick-A-Size Paper ...|\n",
            "|      2|[Organic Roasted Turkey Breast, Gluten Free Whole Grain Bread, Plantain Chips...|\n",
            "|      5|[Organic Raw Agave Nectar, Organic Large Extra Fancy Fuji Apple, Sharp Chedda...|\n",
            "|      7|[Panama Peach Antioxidant Infusion, Antioxidant Infusions Beverage Malawi Man...|\n",
            "|      8|[Shallot, Organic SprouTofu Silken Tofu, Nutritional Yeast Seasoning, Organic...|\n",
            "+-------+--------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HEPhyKpC8q3h"
      },
      "source": [
        "### 3.1 Business Insights (25 points) \n",
        "\n",
        "Now, you are the data scientist. Considering only the orders of ``order_products__train.csv``, use of Spark SQL module, performing with SQL or data frame, to answer the following questions:\n",
        "\n",
        "1. What are the top 10 products which have the highest probability of being reordered? Consider only products purchased at least 40 times for this task.\n",
        "2. What are the top 3 most purchased products of each department?\n",
        "4. What is the average basket size for each day of the week?\n",
        "- Hint: use a barplot to visualize your results\n",
        "\n",
        "**The output of those questions must contain the products' NAME, not their ID.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZqlCqg-8FMR",
        "colab_type": "code",
        "outputId": "6b0d972a-a386-406f-b28f-5ea20767c6a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "top_10 = spark.sql(\"\"\" SELECT p1.product_name, total_order, nbr_reorder / total_order AS proba_reorder \n",
        "           FROM (SELECT * FROM (SELECT product_id, COUNT(product_id) AS total_order, SUM(reordered) AS nbr_reorder \n",
        "           FROM order_prod \n",
        "           GROUP BY product_id) \n",
        "           WHERE total_order >= 40) as p2 \n",
        "           INNER JOIN products AS p1 ON p1.product_id = p2.product_id \n",
        "           ORDER BY proba_reorder DESC LIMIT 10 \"\"\")\n",
        "\n",
        "top_10.show(15, truncate=80)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------------------------+-----------+------------------+\n",
            "|                         product_name|total_order|     proba_reorder|\n",
            "+-------------------------------------+-----------+------------------+\n",
            "|                 2% Lactose Free Milk|         92|0.9347826086956522|\n",
            "|                 Organic Low Fat Milk|        368|0.9130434782608695|\n",
            "|            100% Florida Orange Juice|         59|0.8983050847457628|\n",
            "|Original Sparkling Seltzer Water Cans|         45|0.8888888888888888|\n",
            "|              Organic Spelt Tortillas|         81|0.8888888888888888|\n",
            "|                               Banana|      18726|0.8841717398269785|\n",
            "|                   Petit Suisse Fruit|        120|0.8833333333333333|\n",
            "|               Organic Lowfat 1% Milk|        483|0.8819875776397516|\n",
            "|  Organic Lactose Free 1% Lowfat Milk|        269|0.8810408921933085|\n",
            "|                       1% Lowfat Milk|        461|0.8785249457700651|\n",
            "+-------------------------------------+-----------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cxi2TRFcpYr",
        "colab_type": "code",
        "outputId": "bc235ec6-c2ae-45cf-91f9-c9d6fad8f279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "top_10.createOrReplaceTempView(\"top_10\")\n",
        "top_10 = top_10.collect()\n",
        "prod_names = [x.product_name for x in top_10]\n",
        "probabilities = [y.proba_reorder for y in top_10]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "tmp = range(len(prod_names))\n",
        "\n",
        "plt.bar(tmp, probabilities)\n",
        "plt.xticks(tmp, prod_names)\n",
        "plt.xlabel(\"product name\")\n",
        "plt.ylabel(\"probability\")\n",
        "plt.title(\"top 10 reorder\")\n",
        "plt.show()\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBGauYYWRzEI",
        "colab_type": "code",
        "outputId": "f78e68f8-fe32-4b69-a3ce-a01148a6b1f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "top_3 = spark.sql(\"\"\" SELECT * FROM (\n",
        "                      SELECT department_id, prod_name, total_order, Rank() OVER (PARTITION BY department_id ORDER BY total_order DESC) AS Rank FROM ( \n",
        "                             SELECT department_id, p1.product_name as prod_name, total_order FROM ( \n",
        "                                    SELECT product_id, COUNT(product_id) AS total_order\n",
        "                                            FROM order_prod\n",
        "                                            GROUP BY product_id\n",
        "                                            ORDER BY total_order DESC\n",
        "                                    ) AS p2\n",
        "                                    INNER JOIN products AS p1 ON p1.product_id = p2.product_id\n",
        "                              )\n",
        "                      ) as candidates \n",
        "                      WHERE candidates.Rank <= 3 ORDER BY candidates.department_id ASC \"\"\").show(150, truncate=80)\n",
        "\n",
        "top_3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+-----------------------------------------------------+-----------+----+\n",
            "|department_id|                                            prod_name|total_order|Rank|\n",
            "+-------------+-----------------------------------------------------+-----------+----+\n",
            "|            1|                                          Blueberries|       2323|   1|\n",
            "|            1|                             Organic Broccoli Florets|       1361|   2|\n",
            "|            1|                           Organic Whole Strawberries|       1213|   3|\n",
            "|           10|                                          Dried Mango|        446|   1|\n",
            "|           10|                                  Organic Rolled Oats|        259|   2|\n",
            "|           10|                           Organic Black Mission Figs|        125|   3|\n",
            "|           11|                                   Lavender Hand Soap|        258|   1|\n",
            "|           11|                              Lemon Verbena Hand Soap|        191|   3|\n",
            "|           11|                                         Cotton Swabs|        258|   1|\n",
            "|           12|                    Boneless Skinless Chicken Breasts|       2088|   1|\n",
            "|           12|                                 Ground Turkey Breast|        958|   2|\n",
            "|           12|                     Boneless Skinless Chicken Breast|        943|   3|\n",
            "|           13|                               Extra Virgin Olive Oil|       2068|   1|\n",
            "|           13|                                 Creamy Peanut Butter|        991|   2|\n",
            "|           13|                                 Creamy Almond Butter|        850|   3|\n",
            "|           14|                    Organic Old Fashioned Rolled Oats|        747|   2|\n",
            "|           14|                                   Honey Nut Cheerios|       1218|   1|\n",
            "|           14|                                   Raisin Bran Cereal|        600|   3|\n",
            "|           15|                            No Salt Added Black Beans|       1250|   2|\n",
            "|           15|                                  Organic Black Beans|       1576|   1|\n",
            "|           15|                               Organic Garbanzo Beans|       1141|   3|\n",
            "|           16|                                  Organic Half & Half|       2646|   2|\n",
            "|           16|                                          Half & Half|       2424|   3|\n",
            "|           16|                                   Organic Whole Milk|       4908|   1|\n",
            "|           17|                           100% Recycled Paper Towels|       1183|   1|\n",
            "|           17|                         Sustainably Soft Bath Tissue|        821|   2|\n",
            "|           17|                                        Aluminum Foil|        407|   3|\n",
            "|           18|     Baby Food Stage 2 Blueberry Pear & Purple Carrot|        310|   1|\n",
            "|           18|                Gluten Free SpongeBob Spinach Littles|        259|   3|\n",
            "|           18|                Spinach Peas & Pear Stage 2 Baby Food|        268|   2|\n",
            "|           19|                 Lightly Salted Baked Snap Pea Crisps|        991|   1|\n",
            "|           19|  Pretzel Crisps Original Deli Style Pretzel Crackers|        753|   2|\n",
            "|           19|                                  Sea Salt Pita Chips|        707|   3|\n",
            "|            2|                                Roasted Almond Butter|        174|   1|\n",
            "|            2|              Light CocoWhip! Coconut Whipped Topping|         86|   2|\n",
            "|            2|                             Roasted Unsalted Almonds|         62|   3|\n",
            "|           20|                                      Original Hummus|       2858|   1|\n",
            "|           20|                                 Uncured Genoa Salami|       1788|   2|\n",
            "|           20|                              Organic Extra Firm Tofu|       1186|   3|\n",
            "|           21|                            Organic Riced Cauliflower|        823|   1|\n",
            "|           21|                          Peanut Butter Ice Cream Cup|        261|   2|\n",
            "|           21|                                 Organic Celery Bunch|        200|   3|\n",
            "|            3|                                      Sourdough Bread|        738|   3|\n",
            "|            3|                               100% Whole Wheat Bread|       2298|   1|\n",
            "|            3|                   Organic Bread with 21 Whole Grains|        938|   2|\n",
            "|            4|                                               Banana|      18726|   1|\n",
            "|            4|                               Bag of Organic Bananas|      15480|   2|\n",
            "|            4|                                 Organic Strawberries|      10894|   3|\n",
            "|            5|                                                 Beer|        224|   3|\n",
            "|            5|                                      Sauvignon Blanc|        295|   1|\n",
            "|            5|                                   Cabernet Sauvignon|        237|   2|\n",
            "|            6|                                       Taco Seasoning|        405|   1|\n",
            "|            6|              Organic Sea Salt Roasted Seaweed Snacks|        345|   2|\n",
            "|            6|            New Mexico Taco Skillet Sauce For Chicken|        224|   3|\n",
            "|            7|                                 Lime Sparkling Water|       1966|   3|\n",
            "|            7|                                         Spring Water|       2225|   2|\n",
            "|            7|                           Sparkling Water Grapefruit|       3359|   1|\n",
            "|            8|Double Duty Advanced Odor Control Clumping Cat Litter|         84|   1|\n",
            "|            8|                          24/7 Performance Cat Litter|         76|   2|\n",
            "|            8|                            Instant Action Cat Litter|         73|   3|\n",
            "|            9|                     Organic Tomato Basil Pasta Sauce|        772|   1|\n",
            "|            9|                                       Marinara Sauce|        754|   2|\n",
            "|            9|                                          Basil Pesto|        699|   3|\n",
            "+-------------+-----------------------------------------------------+-----------+----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX3CHbE4HjYL",
        "colab_type": "code",
        "outputId": "95698208-80e5-49d7-92ae-a85dea540fe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "avg_dow = spark.sql(\"\"\" SELECT day, AVG(n_prod) AS average_basket_size FROM (\n",
        "                            SELECT order_dow AS day, num_products AS n_prod FROM (\n",
        "                                SELECT order_id, COUNT(order_id) AS num_products  \n",
        "                                  FROM order_prod  \n",
        "                                  GROUP BY order_id ) AS p1\n",
        "                                INNER JOIN orders as p2 ON p1.order_id = p2.order_id )\n",
        "                        GROUP BY day \n",
        "                        ORDER BY day ASC \"\"\").show(150, truncate=80)\n",
        "\n",
        "avg_dow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------------------+\n",
            "|day|average_basket_size|\n",
            "+---+-------------------+\n",
            "|  0| 11.797778991443655|\n",
            "|  1| 10.470618137454249|\n",
            "|  2|   9.96103976673491|\n",
            "|  3|   9.84133358832154|\n",
            "|  4|  9.742527727301209|\n",
            "|  5| 10.163736642537057|\n",
            "|  6| 10.966562615734617|\n",
            "+---+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LMoLK30G8q3m"
      },
      "source": [
        "### 3.2 Run MBA for the training set (15 points)\n",
        "\n",
        "Using the orders from the ``order_products__train.csv``, create a data frame where each row contains the column “transaction” with the list of purchased products, similarly to the toy dataset. In sequence, run the MBA algorithm for this set of transactions. \n",
        "\n",
        "- You must report the time spent to perform this task.\n",
        "- Output must contain the products' name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "saItRyfZ8q3o",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "TODO: create a query to create and sctruct the transactions\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yGQ2ZJTP8q3q",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "TODO: run the MBA algorithm and show the first 5 association rules\n",
        "\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WyNBcK5f8q3s"
      },
      "source": [
        "# 3.3 Run MBA for the whole dataset (15 points)\n",
        "\n",
        "As you probably noticed, even for a not so large data set (the training file has only 131K orders), the MBA algorithm is computationally expensive. For that reason, this time, we will repeat the process, but now using the Google Cloud Platform (GCP) to create a large computer cluster. All the instructions for creating a computing cluster with spark and how to submit a job will be explained in both sessions of the laboratory. In any case, you should read the instructions given in the ``Instruction_GCP.pdf``.\n",
        "\n",
        "This time, we will work with the ``order_products__prior.csv`` file, which contains more than 3M orders.\n",
        "\n",
        "**EXPECTED OUTPUT**\n",
        "\n",
        "After you ran the MBA for the larger collection of orders, randomly select ONE product purchased in ``order_products__prior`` and print the association rules (product name and association value) of this product, i.e., when the product is alone in the basket. The output should be formatted in a table, where each row containing the information of one associated product.\n",
        "\n",
        "- Print both ID and Name of the random selected product.\n",
        "- Report the execution time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GOjubwiJ8q3s",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "TODO: create a query to create and sctruct the transactions from the order_products__prior.csv file\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Ow-cH1e8q3t",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "TODO: run the MBA algorithm and print the requested output\n",
        "\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H9zX9iAA8q3v",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}