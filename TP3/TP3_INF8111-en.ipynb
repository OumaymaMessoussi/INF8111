{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5fHWJhwOXGzh"
   },
   "source": [
    "# INF8111 - Fouille de données \n",
    "## Summer 2020 - TP3 - Fouille de réseaux sociaux\n",
    "### Team member\n",
    "    - Member 1\n",
    "    - Member 2\n",
    "    - Member 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmGvqtSVgfXi"
   },
   "source": [
    "## Directives de remise\n",
    "The work will be carried out with the same team as for the previous TPs.\n",
    "You must put back in the submission box on moodle:\n",
    "\n",
    "1. this file renamed TP3\\_NomDuMembre1\\_NomDuMembre2\\_NomDuMembre3.ipynb\n",
    "\n",
    "**N.B**: Make sure that all results are there when you open your notebook.\n",
    "\n",
    "Everything must be submitted before **20 juin 2020 à 23h55**. Any late work will be penalized with a value of 10% per day of delay.\n",
    "\n",
    "## Barème\n",
    "Partie 1: 12 points\n",
    "\n",
    "Partie 2: 8 points\n",
    "\n",
    "For a total of 20 points on 20 points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1et8f3nXGzk"
   },
   "source": [
    "## Social networks\n",
    "Social networks are a major component of the humain life. Each person belongs throughout their life to different communities. With the aggregation of information on various online social media platforms, data analysts were interested in exploiting its data. It is a relatively new field that is growing with impacts on several aspects such as advertising and recommendation systems.\n",
    "\n",
    "\n",
    "### Goal\n",
    "The purpose of this lab is to give you an overview of social network analysis.\n",
    "\n",
    "In the first part, you will implement an algorithm for detecting communities in a social network called LPAm+. This algorithm was proposed by [X. Liu and T. Murata in 2010](https://www.sciencedirect.com/science/article/pii/S0378437109010152).\n",
    "\n",
    "In the second part, you will find the people with the most influence in their social network.\n",
    "\n",
    "For both parties, we provide you with all the csv containing the social networks to be analyzed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "517JwzlPXGzp"
   },
   "source": [
    "# 1. LPAm+ (12 points)\n",
    "\n",
    "\n",
    "## Community detection\n",
    "Community detection in a social network is a frequent manipulation when analyzing a network. A clustering method is used to bring people together in communities according to the links between them.\n",
    "\n",
    "\n",
    "## LPAm+\n",
    "In this part, you will implement the LPAm+ algorithm to detect the communities among the characters of Games of Thrones. You must use the nodes and edges csv for this.\n",
    "\n",
    "This algorithm consists in propagating the labels in the network according to an evaluation rule optimizing the modularity of the network. When the algorithm reaches a local optimum, it checks whether it can combine two communities to increase the modularity of the network. The algorithm always chooses the most advantageous combination. If a combination is found, the propagation of the labels is redone. The algorithm continues until it is no longer able to increase modularity. You can read the article mentioned above for more details but you don't need to as you will be guided throughout the TP.\n",
    "\n",
    "\n",
    "You can read more about the package [here](https://networkx.github.io/documentation/stable/tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also use anacondas to install the package instead\n",
    "!pip install --user numpy\n",
    "!pip install --user pandas\n",
    "!pip install --user matplotlib\n",
    "!pip install --user networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "\n",
    "class LPAmPlus:\n",
    "    \"\"\"\n",
    "    Contructeur\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph):\n",
    "        \"\"\"\n",
    "        graph gives the graph on which the algorithm will be applied;\n",
    "        \"\"\"\n",
    "        self.graph = graph\n",
    "\n",
    "        \"\"\"\n",
    "        labels gives all the communities present in the network\n",
    "        \"\"\"\n",
    "        self.labels = None\n",
    "\n",
    "        \"\"\"\n",
    "        Assign a label to each node\n",
    "        \"\"\"\n",
    "        #TODO\n",
    "\n",
    "    \"\"\"\n",
    "    Term to optimize when replacing labels\n",
    "    \"\"\"\n",
    "\n",
    "    def label_evaluation(self, current_node, new_label):\n",
    "        #TODO\n",
    "        return 0\n",
    "\n",
    "    \"\"\"\n",
    "    Function to choose the new label for a node\n",
    "    \"\"\"\n",
    "\n",
    "    def update_label(self, current_node):\n",
    "        return False\n",
    "\n",
    "    \"\"\"\n",
    "    Function that calculates the current modularity of the network\n",
    "    \"\"\"\n",
    "\n",
    "    def modularity(self):\n",
    "        #TODO\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Function that applies the LPAm algorithm on the network\n",
    "    \"\"\"\n",
    "\n",
    "    def LPAm(self):\n",
    "        #TODO\n",
    "   \n",
    "    \"\"\"\n",
    "    Function that find which communities to combine and combine them\n",
    "    \"\"\"\n",
    "    def merge_communities(self):\n",
    "        #TODO\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Function that applies the LPAm+ algorithm on the network\n",
    "    \"\"\"\n",
    "\n",
    "    def find_communities(self):\n",
    "        #TODO\n",
    "\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yv83c1sWXGzq"
   },
   "source": [
    "### 1.1 Dataset (1 point)\n",
    "\n",
    "We have provided you with the csv for all the seasons of Games of Thrones. You must now represent each of those networks in code using two csv for each season: the one for the nodes and the one for the edges.\n",
    "\n",
    "\n",
    "#### Implémentation\n",
    "1. Implement the function *`load_unweighted_network`*. This function returns a undirected and unweighted graph.\n",
    "\n",
    "Use the function `test_load` to verify your implementation of the function. This test use a toy dataset. You should obtain a result similar to this:\n",
    "![title](data/picture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def load_unweighted_network(node_csv, edge_csv):\n",
    "    #TODO\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 607,
     "status": "ok",
     "timestamp": 1572273158769,
     "user": {
      "displayName": "Kim Thuyen Ton",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBg1xy2-8526x-b1HLCbT16SZlMsjmtKfyoHRgghw=s64",
      "userId": "10674693358177584608"
     },
     "user_tz": 240
    },
    "id": "HEwBsNVsjvfs",
    "outputId": "aedf1752-21aa-49b8-e31b-8892f3f5e2e6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def test_load():\n",
    "    network = load_unweighted_network(\"data/toy-nodes.csv\", \"data/toy-edges.csv\")\n",
    "    nx.draw_networkx(network,font_color='white')\n",
    "    plt.show()\n",
    "\n",
    "test_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6UTIxaGIXGzz"
   },
   "source": [
    "### 1.2  Modularity (1 point)\n",
    "\n",
    "The modularity $Q$ of the network is an important measure for the algorithm. The algorithm uses it to determine if it reached a local optimum or not. $$ Q=\\frac{1}{2m}\\sum_{u,v=1}^n B_{uv}\\delta(l_u,l_v)$$ \n",
    "\n",
    "- m: number of edges\n",
    "- l: node's label\n",
    "- u, v: nodes in the graph\n",
    "- B: modularity matrix where each element is $A_{uv} - P_{uv}$\n",
    "- $A_{uv}$: is 1 if there is an edge between u and v else 0\n",
    "- $P_{uv}$: probability that there is an edge between u and v following the null model $$P_{uv}=\\frac{degree(u)*degree(x)}{2m}$$\n",
    "- $\\delta(l_u,l_v)$: Kronecker's delta, is 1 if labels are the same else 0\n",
    "\n",
    "The modularity can also be defined like this: $$Q=\\sum_{t=1}^{N_c}\\left(\\frac{I_t}{m}-\\left(\\frac{D_t}{2m}\\right)^2\\right)$$\n",
    "\n",
    "- m: number of edges\n",
    "- Nc: the number of community in the graph\n",
    "- t: a community in the graph\n",
    "- $I_t$: the number of arc in the community t meaning all arcs that have both nodes in the community t\n",
    "- $D_t$: the sum of degree of all the nodes in the community t\n",
    "\n",
    "#### Implementation\n",
    "1. Implement the function `modularity` in the class LPAmPlus. This function returns the modularity of the network. You can use the function `linalg.modularity_matrix` from networkx to calculate B. You can implement whichever definition for the modularity. **N.B:** You can add data to nodes with Networkx to store information about the node.\n",
    "\n",
    "Use the function `test_modularity` to test your implementation. You should have a modularity of 0.413."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_modularity():\n",
    "    social_network = load_unweighted_network(\"data/toy-nodes.csv\", \"data/toy-edges.csv\")\n",
    "    lpam = LPAmPlus(social_network)\n",
    "    lpam.labels = [0, 1]\n",
    "    for i in [0,1,2,3,4,5,6,7,8,9]:\n",
    "        lpam.graph.nodes[i]['label'] = 0\n",
    "    for i in [10,11,12,13,14,15]:\n",
    "        lpam.graph.nodes[i]['label'] = 1\n",
    "    print(\"Modularity: {}\".format(lpam.modularity()))\n",
    "\n",
    "test_modularity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwxvqYj4XGzu"
   },
   "source": [
    "### 1.3 Updating rule for the labels (2 points)\n",
    "\n",
    "As mentioned above, the algorithm is strongly based on its optimization of modularity. You are now asked to implement the term to optimize. The new label $l_x^{new}$ corresponds to the label for which the sum gives the greatest value.\n",
    "$$l_x^{new}=\\arg\\max\\sum_{u=1}^n B_{ux}\\delta(l_u,l)$$\n",
    "\n",
    "- n: number of nodes\n",
    "- m: number of edges\n",
    "- l: node's label\n",
    "- x: current node being evaluated\n",
    "- u: another node in the network (starts at 1, because we exclude the node x)\n",
    "- B: modularity matrix where each element is $A_{ux} - P_{ux}$\n",
    "- $A_{ux}$: is 1 if there is an edge between u and x else 0\n",
    "- $P_{ux}$: the probability that there is an edge between u and x  following the null model  $$P_{ux}=\\frac{degree(u)*degree(x)}{2m}$$\n",
    "- $\\delta(l_u,l)$: Kronecker's delta, is 1 if labels are the same else 0\n",
    "\n",
    "\n",
    "#### Implementation\n",
    "1. Implement the function `label_evaluation`. This function returns the value for the term to optimize. You can use the function `linalg.modularity_matrix` from networkx to calculate B. It is normal if there is a similarity with the modularity depending on the definition you took.\n",
    "2. Implement the function `update_label`. This function chooses the new label for the current node. If there is more than one label with the max value, the function chooses randomly one amoung those. Don't forget to remove the unused labels from the `labels` attribute. **N.B:**  You can add data to nodes with Networkx to store information about the node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g5_N31uPXGz2"
   },
   "source": [
    "### 1.4 LPAm (3 points)\n",
    "\n",
    "You can now implement the LPAm algorithm. This algorithm is the predecessor of LPAm +. He begins by giving a unique label to each node. It then explores all the nodes and changes their label according to the evaluation function that you implemented earlier. The algorithm continues until it can no longer improve the modularity of the network.\n",
    "\n",
    "#### Implementation\n",
    "1. Initialise the `labels` attribute from LPAmPlus and add the those labels to the nodes in the graph in the function `__init__`.\n",
    "\n",
    "2. Implement the LPAm algorithm in the function`LPAm`. Make sure that all your label's changes improve the modularity.\n",
    "\n",
    "Use the function `test_lpam` to verify your implementation. You should have a modularity of 0.399 with 4 communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lpam():\n",
    "    social_network = load_unweighted_network(\"data/toy-nodes.csv\", \"data/toy-edges.csv\")\n",
    "    lpam = LPAmPlus(social_network)\n",
    "    lpam.LPAm()\n",
    "    print(\"Modularity: {}\\nCommunities: {}\".format(lpam.modularity(), lpam.labels))\n",
    "\n",
    "test_lpam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cj5Ghd5jXGz6"
   },
   "source": [
    "### 1.5 LPAm+ (2 point)\n",
    "\n",
    "You can now fully implement LPAm+. When LPAm falls into a local optimum, LPAm+ tries to combine two communities to increase modularity. LPAm+ chooses the combination that most increases modularity and redo the label's propagation until the next local optimum. The algorithm continues until it can no longer increase modularity.\n",
    "\n",
    "#### Implementation\n",
    "1. Implement the function  `merge_communities`. This function check if combining communities improve the modularity and combine the best choice. It returns True if a combinaison was made else False.\n",
    "2. Implement the LPAM+ algorithm in the function `find_communities`.\n",
    "\n",
    "Use the function `test_lpam_plus` to verify your implementation. You should end with a modularity of 0.413 and 2 communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lpam_plus():\n",
    "    social_network = load_unweighted_network(\"data/toy-nodes.csv\", \"data/toy-edges.csv\")\n",
    "    lpam = LPAmPlus(social_network)\n",
    "    lpam.find_communities()\n",
    "    print(\"Modularity: {}\\nCommunities: {}\".format(lpam.modularity(), lpam.labels))\n",
    "\n",
    "test_lpam_plus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vODCJbRaXGz-"
   },
   "source": [
    "### 1.6 GOT dataset (4 points)\n",
    "\n",
    "Run your algorithm over the Games of Thrones data from each season and compare what you get and the real communities. \n",
    "\n",
    "Start by calculating the ARI (adjusted Rand index) of your results. $$ ARI=\\frac{TP+TN}{TP+TN+FN+TN} = \\frac{TP+TN}{\\binom{n}{2}}$$\n",
    "\n",
    "- n: number of nodes\n",
    "- TP: True positive the number of pairs of elements that are in the same community in your results and in the ground truth\n",
    "- TN: True negative the number of pairs of elements that are in different communities in your results and in the ground truth\n",
    "- FP: False positive the number of pairs of elements which are in the same community in your results but which are in different communities in the ground truth\n",
    "- FN: False negative the number of pairs of elements which are in different communities in your results but which are in the same community in the ground truth\n",
    "\n",
    "Does the algorithm perform well on all seasons or for some only? Why did the algorithm perform like that? You can do the manipulations you want to better present your results and better support your statements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1-2UipMRXG0R"
   },
   "outputs": [],
   "source": [
    "# Put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Write your analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r3FmllqgXG0d"
   },
   "source": [
    "# 2. Influent characters in GOT (8 points)\n",
    "\n",
    "##  Social network analysis\n",
    "\n",
    "Another interesting analysis to do with a social network is to find the influential people in the network, ie the people around whom the people in the network gather.\n",
    "\n",
    "There are measures which make it possible to know these people: the centrality measures. To help you during the implementation of those measurements, a second toy dataset is provided to you. It looks like this: ![title](data/picture2.png)\n",
    "\n",
    "## GOT datasets\n",
    "The Games of Thrones series is known to kill its important characters. We ask you to verify this statement. For this part, you must use all the csv given with the TP (nodes, edges and deaths). We want you to find the most influential characters from each season and compare them with the list of dead characters during the season.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uJxSGCnOXG0e"
   },
   "source": [
    "## 2.1 Degree centrality (1 point)\n",
    "\n",
    "A first simple measure to find the importance of a node in a network is the degree centrality. It is calculated $$C_{D}(i) = \\frac{degree(i)}{n-1}$$\n",
    "\n",
    "- i: a node in the network\n",
    "- n: the number of nodes\n",
    "- degree: the number of edges attached to the node\n",
    "\n",
    "#### Implementation\n",
    "1. Implement the function `calculate_degree_centrality`. This function calculates degree centrality for all nodes in the network and adds this measurement to each node.\n",
    "\n",
    "Use the function `test_degree_centrality` to verify your implementation. The best node should be node_1 with 0.4375."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_degree_centrality(social_network):\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_degree_centrality():\n",
    "    social_network = load_unweighted_network(\"data/toy-nodes2.csv\", \"data/toy-edges2.csv\")\n",
    "    calculate_degree_centrality(social_network)\n",
    "    dict_centrality = nx.get_node_attributes(social_network, 'degree_centrality')\n",
    "    best_node = max(dict_centrality, key=dict_centrality.get)\n",
    "    print(\"Highest degree centrality node: {} with {}\".format(best_node, dict_centrality[best_node]))\n",
    "test_degree_centrality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uJxSGCnOXG0e"
   },
   "source": [
    "## 2.2 Proximity centrality (1 point)\n",
    "\n",
    "Another simple measure for finding the importance of a node in a network is proximity centrality. It is calculated $$C_{P}(i) = \\frac{1}{AvDist(i)}$$\n",
    "\n",
    "- i: a node in the network\n",
    "- AvDist: the average of all shortest distances to reach each vertex from vertex i\n",
    "\n",
    "#### Implementation\n",
    "1. Implement the function `calculate_closeness_centrality`. This function calculates proximity centrality for all nodes in the network and adds this measurement to each node. Consider each edge as a distance of 1.\n",
    "\n",
    "**NB**: Use the fucntion `shortest_path()` from Networkx to find the shortest path between two nodes.\n",
    "\n",
    "Use the function `test_closeness_centrality` to verify your implementation. The best node should be node_3 with 0.41."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_closeness_centrality(social_network):\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_closeness_centrality():\n",
    "    social_network = load_unweighted_network(\"data/toy-nodes2.csv\", \"data/toy-edges2.csv\")\n",
    "    calculate_closeness_centrality(social_network)\n",
    "    dict_centrality = nx.get_node_attributes(social_network, 'closeness_centrality')\n",
    "    best_node = max(dict_centrality, key=dict_centrality.get)\n",
    "    print(\"Highest closeness centrality node: {} with {}\".format(best_node, dict_centrality[best_node]))\n",
    "\n",
    "test_closeness_centrality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uJxSGCnOXG0e"
   },
   "source": [
    "## 2.3 Betweeness centrality (2 points)\n",
    "\n",
    "A final simple measure to find the importance of a node in a network is the betweeness centrality. It is calculated $$C_{I}(i) = \\frac{\\sum_{j<k}f_{jk}(i)}{\\binom{n}{2}}$$\n",
    "\n",
    "- n: the number of nodes in the network\n",
    "- i: a node in the network\n",
    "- j,k: two nodes in the network excluding i\n",
    "- $f_{jk}(i)$: the  number of shortest paths from vertex j to vertex k (> j) passing through node i\n",
    "\n",
    "#### Implementation\n",
    "1. Implement the function `calculate_betweenness_centrality`.This function calculates the betweenness centrality for all the nodes of the network and adds this measurement to each node.\n",
    "\n",
    "Use the function `test_betweennes_centrality` to verify your implementation. The best node should be the node_4 with 0.57."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_betweenness_centrality(social_network):\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_betweenness_centrality():\n",
    "    social_network = load_unweighted_network(\"data/toy-nodes2.csv\", \"data/toy-edges2.csv\")\n",
    "    calculate_betweenness_centrality(social_network)\n",
    "    dict_centrality = nx.get_node_attributes(social_network, 'betweenness_centrality')\n",
    "    best_node = max(dict_centrality, key=dict_centrality.get)\n",
    "    print(\"Highest betweenness centrality node: {} with {}\".format(best_node, dict_centrality[best_node]))\n",
    "\n",
    "test_betweenness_centrality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "996_M0-sXG0w"
   },
   "source": [
    "## 2.4 Analysis of your results (4 points)\n",
    "\n",
    "Run the three functions on the networks of each season and present the top 10 for each metric. For each season, compare the top 10 metrics with the season's death list. Is the top 10 enough to find the significant deaths of each season? What measure seems to better predict the dead? Is the reputation of Games of Thrones for killing its important characters founded?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2U3Xs73-rDr"
   },
   "source": [
    "### Analysis\n",
    "\n",
    "Write your analysis here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TP3_INF8215.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
